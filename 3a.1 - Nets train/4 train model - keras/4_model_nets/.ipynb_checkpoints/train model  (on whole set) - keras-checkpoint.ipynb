{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keras \n",
    "# Classifier: LSTM \n",
    "# Classification type: multi-class (404 classes)\n",
    "# Output nodes: #of classes with softmax\n",
    "\n",
    "# word2vec model: \n",
    "# word2vect_class_specififc__vec64_win1__dict_sample_5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing process_string.py \n",
      "from /Users/altay.amanbay/Desktop/new node booster/experiments/3a.1 - Nets train/4 train model - keras/2_common_aux_script ...\n",
      "\n",
      "3.5.2 |Anaconda 4.2.0 (x86_64)| (default, Jul  2 2016, 17:52:12) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from keras.datasets import reuters\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer, one_hot, text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializations\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "\n",
    "from keras.utils.visualize_util import plot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os    \n",
    "os.environ['THEANO_FLAGS'] = \"device=gpu\" \n",
    "import theano\n",
    "#theano.config.device = 'gpu0'\n",
    "theano.config.floatX = 'float32'\n",
    "\n",
    "\n",
    "# import custom code\n",
    "import os\n",
    "import sys\n",
    "pardir = os.path.abspath(os.path.join(os.getcwd(), '../'))\n",
    "script_path = pardir + \"/2_common_aux_script\"\n",
    "print('Importing process_string.py \\nfrom ' + script_path + \" ...\\n\")\n",
    "sys.path.append(script_path)\n",
    "from process_string import process_string\n",
    "sys.path.remove(script_path)\n",
    "\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def NGramGenerator_wordwise_interval(phrase, min_ngram, max_ngram):\n",
    "    all_ngram_lists = []\n",
    "\n",
    "    #printable_ = 'abcdefghijklmnopqrstuvwxyz0123456789 '\n",
    "    #s_split = \"\".join((char if char in printable_ else \"\") for char in phrase).split()\n",
    "    phrase_processed = process_string(phrase)\n",
    "    s_split = phrase_processed.split()\n",
    "    \n",
    "    for n in range(max_ngram, min_ngram - 1, -1):\n",
    "        n_gram = [s_split[i:i+n] for i in range(len(s_split)-n+1)]\n",
    "        all_ngram_lists.extend(n_gram)\n",
    "        \n",
    "    all_ngrams = []\n",
    "    for n_gram in all_ngram_lists:\n",
    "        all_ngrams.extend([' '.join(n_gram)])\n",
    "    \n",
    "    return all_ngrams\n",
    "\n",
    "def get_word2index(texts_ls_):\n",
    "    word2index_ = {}\n",
    "\n",
    "    c = 1\n",
    "    for text_str in texts_ls_:\n",
    "        text_tokens_ls = text_str.lower().split()\n",
    "        for token in text_tokens_ls:\n",
    "            if(token not in word2index_):\n",
    "                word2index_[token] = c\n",
    "                c = c + 1\n",
    "                \n",
    "    return word2index_\n",
    "\n",
    "def train_df_preprocess(top_words_, texts_ls_, max_pad_length_):\n",
    "    # texts_ls_: list of texts strings\n",
    "    \n",
    "    tok = Tokenizer(top_words_)\n",
    "    tok.fit_on_texts(texts_ls_)\n",
    "\n",
    "    words = []\n",
    "    for iter in range(top_words):\n",
    "        words += [key for key,value in tok.word_index.items() if value==iter+1]\n",
    "\n",
    "    #Class for vectorizing texts, or/and turning texts into sequences \n",
    "    #(=list of word indexes, where the word of rank i in the dataset (starting at 1) has index i).\n",
    "    texts_vec_ls = tok.texts_to_sequences(texts_ls_)#turns text to sequence, stating which word comes in what place\n",
    "    texts_vec_mtx = sequence.pad_sequences(texts_vec_ls, maxlen=max_pad_length_)#pad sequence, essentially padding it with 0's at the end\n",
    "    \n",
    "    return texts_vec_mtx\n",
    "\n",
    "def text_2_vec(text_str, word2index_):\n",
    "    # text_str: text string\n",
    "    \n",
    "    text_tokens_ls = text_str.lower().split()\n",
    "    \n",
    "    text_vec = []\n",
    "    for token in text_tokens_ls:\n",
    "        if token in word2index_:\n",
    "            text_vec.append(word2index_[token])\n",
    "        else:\n",
    "            text_vec.append(0)\n",
    "            \n",
    "    return text_vec\n",
    "\n",
    "def train_df_preprocess_2(texts_ls_, word2index_, max_pad_length_):\n",
    "    # texts_ls_: list of texts strings\n",
    "    \n",
    "    texts_vec_ls = []\n",
    "    for text_ in texts_ls_:\n",
    "        #print(text_)\n",
    "        #print(type(text_))\n",
    "        text_vec = text_2_vec(text_, word2index_)\n",
    "        texts_vec_ls.append(text_vec)\n",
    "    \n",
    "    texts_vec_ary = sequence.pad_sequences(texts_vec_ls, maxlen=max_pad_length_)\n",
    "    \n",
    "    return texts_vec_ary\n",
    "\n",
    "def texts_to_sequences_custom(texts_ls, word_index_):\n",
    "    texts_seq = []\n",
    "    \n",
    "    for text in texts_ls:\n",
    "        #text_split = text.lower().split()\n",
    "        text_split = NGramGenerator_wordwise_interval(text,1,1)\n",
    "        seq = []\n",
    "        for token in text_split:\n",
    "            if(token in word_index_):\n",
    "                seq.append(word_index_[token])\n",
    "            else:\n",
    "                seq.append(0)\n",
    "                \n",
    "        texts_seq.append(seq)\n",
    "#         for k,v in word_index_.items():\n",
    "#             if(v == 395):\n",
    "#                 print(k,v)\n",
    "    return texts_seq\n",
    "\n",
    "\n",
    "def get_model_file_aux(model_file_aux_name):\n",
    "    with open(model_file_aux_name, 'rb') as pickle_file:\n",
    "        model_file_aux = pickle.load(pickle_file)\n",
    "    return model_file_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples data shape: (956776, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_mod1</th>\n",
       "      <th>category_id_mod1</th>\n",
       "      <th>category_full_path_mod1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!iT Jeans Maternity Skinny Jeans Dark Wash M</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1822 Denim 'Butter' Maternity Skinny Jeans Rin...</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25 J Brand Maternity Skinny Jean nirvana blue</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26 J Brand Maternity Skinny Jean nirvana blue</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26 James Jeans Maternity Skinny External Mater...</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    description_mod1  category_id_mod1  \\\n",
       "0       !iT Jeans Maternity Skinny Jeans Dark Wash M               100   \n",
       "1  1822 Denim 'Butter' Maternity Skinny Jeans Rin...               100   \n",
       "2      25 J Brand Maternity Skinny Jean nirvana blue               100   \n",
       "3      26 J Brand Maternity Skinny Jean nirvana blue               100   \n",
       "4  26 James Jeans Maternity Skinny External Mater...               100   \n",
       "\n",
       "                       category_full_path_mod1  \n",
       "0  Apparel & Accessories > Apparel > Maternity  \n",
       "1  Apparel & Accessories > Apparel > Maternity  \n",
       "2  Apparel & Accessories > Apparel > Maternity  \n",
       "3  Apparel & Accessories > Apparel > Maternity  \n",
       "4  Apparel & Accessories > Apparel > Maternity  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read sampled descriptionary\n",
    "\n",
    "path = pardir+'/1_data/'\n",
    "file_name = 'sampled_descriptionary_sample_size_5000.csv'\n",
    "samples_df = pd.read_csv(path + file_name)\n",
    "\n",
    "# Rename columns\n",
    "samples_df.rename(columns={'description': 'description_mod1', \n",
    "                           'category_id': 'category_id_mod1',\n",
    "                           'category_path': 'category_full_path_mod1'}, inplace=True)\n",
    "\n",
    "# Drop 'screwdrivers' from descriptionary\n",
    "#samples_df = samples_df.loc[samples_df.category_id_mod1 != 927,:]\n",
    "\n",
    "# Drop index column\n",
    "samples_df.drop(labels=['index'], axis=1, inplace=True)\n",
    "\n",
    "print('samples data shape:',samples_df.shape)\n",
    "samples_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape (deduplicate): (938810, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_mod1</th>\n",
       "      <th>category_id_mod1</th>\n",
       "      <th>category_full_path_mod1</th>\n",
       "      <th>target_le</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it jeans maternity skinny jeans dark wash m</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1822 denim butter maternity skinny jeans rinse...</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    description_mod1  category_id_mod1  \\\n",
       "0        it jeans maternity skinny jeans dark wash m               100   \n",
       "1  1822 denim butter maternity skinny jeans rinse...               100   \n",
       "\n",
       "                       category_full_path_mod1  target_le  \n",
       "0  Apparel & Accessories > Apparel > Maternity         27  \n",
       "1  Apparel & Accessories > Apparel > Maternity         27  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concat original train set and sampled descriptionary\n",
    "#train_df = pd.concat([train_df, samples_df], axis=0)\n",
    "train_df = samples_df\n",
    "#train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# description into chars\n",
    "train_df['description_mod1'] = train_df['description_mod1'].apply(lambda x: process_string(x))\n",
    "\n",
    "# deduplicate\n",
    "train_df.drop_duplicates(subset=['description_mod1'], inplace = True, keep=False)\n",
    "print('train data shape (deduplicate):',train_df.shape)\n",
    "    \n",
    "# Encode target feature\n",
    "le = LabelEncoder()\n",
    "le.fit(train_df['category_full_path_mod1'])\n",
    "train_df['target_le'] = le.transform(train_df['category_full_path_mod1'])\n",
    "\n",
    "\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938810\n",
      "(938810, 398)\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test\n",
    "#X = train_df.loc[:,['description_mod1']]\n",
    "X_ls = np.array(list(train_df['description_mod1']))\n",
    "y_ary = np.array(list(train_df['target_le']))\n",
    "y_ary_cat = np_utils.to_categorical(train_df['target_le'])\n",
    "# X_ls = train_df[['description_mod1']]\n",
    "# y_ary = train_df[['target_le']]\n",
    "\n",
    "#print(type(X_ls))\n",
    "#print(type(y_ary))\n",
    "#print(type(y_ary_cat))\n",
    "\n",
    "#X_train_ls, X_test_ls, y_train_ary, y_test_ary = train_test_split(X_ls, y_ary, test_size = 0.3)\n",
    "#X_train_ls, X_test_ls, y_train_ary, y_test_ary = train_test_split(X_ls, y_ary_cat, test_size = 0.3)\n",
    "\n",
    "print(len(X_ls))\n",
    "print(y_ary_cat.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index size: 234950\n",
      "train_texts_vec_mtx shape: (938810, 30)\n"
     ]
    }
   ],
   "source": [
    "# Convert train set into sequences for nets\n",
    "\n",
    "top_words = None\n",
    "max_description_length = 30\n",
    "\n",
    "tok = Tokenizer(nb_words = top_words)\n",
    "tok.fit_on_texts(X_ls)\n",
    "word_index = tok.word_index\n",
    "print('word_index size:',len(word_index))\n",
    "\n",
    "#train_texts_vec_ls = tok.texts_to_sequences(X_train_ls)\n",
    "train_texts_vec_ls = texts_to_sequences_custom(X_ls, word_index)\n",
    "train_texts_vec_mtx = sequence.pad_sequences(train_texts_vec_ls, maxlen = max_description_length)\n",
    "\n",
    "print('train_texts_vec_mtx shape:',train_texts_vec_mtx.shape)\n",
    "list(tok.word_index)[0:5]\n",
    "\n",
    "# Delete objects\n",
    "X_ls = None\n",
    "y_ary = None\n",
    "tok = None\n",
    "train_texts_vec_ls = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert test set into sequences for nets\n",
    "\n",
    "#test_texts_vec_ls = tok.texts_to_sequences(X_test_ls)\n",
    "#test_texts_vec_ls = texts_to_sequences_custom(X_test_ls, word_index)\n",
    "#test_texts_vec_mtx = sequence.pad_sequences(test_texts_vec_ls, maxlen = max_description_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   0    0    0    0    0    0    0    0    0    0    0    0    0    0    0\n",
      "    0    0    0    0    0 6780 4241  251   68   34   58   49   58   49  267]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "234950"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test check\n",
    "i = 100\n",
    "print(train_texts_vec_mtx[i])\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded 235349 word vectors.\n",
      "embedding_vecor_length: 64\n",
      "\n",
      "embedding matrix shape: (234951, 64)\n",
      "[ 0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.\n",
      "  0.  0.  0.  0.  0.  0.  0.  0.  0.  0.]\n",
      "[ 0.17534356 -0.08578434  0.18103811  0.02477756 -0.07118656 -0.15512228\n",
      "  0.05136703  0.2165056  -0.06552318  0.09003545 -0.20524196  0.00697994\n",
      " -0.16218811  0.07659905 -0.02378268  0.04618275  0.09628467  0.00470943\n",
      "  0.11890937 -0.09564091  0.30388096  0.08125744 -0.0361764   0.0483168\n",
      "  0.19096969 -0.18906899 -0.04920458  0.010562   -0.17351735  0.16046672\n",
      " -0.06544918 -0.00549091  0.01579626 -0.04615073  0.20403807  0.23351906\n",
      " -0.1007614  -0.01959764 -0.10367423  0.03201051 -0.01045111  0.30996674\n",
      " -0.05559701 -0.0629313  -0.09992196  0.01259949  0.08974306  0.12655872\n",
      " -0.07601544 -0.09347545 -0.11799838 -0.06275873  0.16728345 -0.16929968\n",
      " -0.02340443  0.09140575 -0.12097249 -0.08594122  0.15284209 -0.0008869\n",
      "  0.00310367  0.0343681   0.28310841 -0.08586995]\n"
     ]
    }
   ],
   "source": [
    "## Create word embeddings from trained Word2Vec model\n",
    "from gensim.models import word2vec, Phrases\n",
    "\n",
    "# Load model\n",
    "file_path_1 = pardir+\"/3_model_word2vec_vec64_win3__dict_sample_5000/word2vect_vec64_win3__dict_sample_5000\"\n",
    "model = word2vec.Word2Vec.load(file_path_1)\n",
    "\n",
    "#print(model.vocab.keys())\n",
    "#sys.exit()\n",
    "\n",
    "# word vector embeddings from model into dictionary\n",
    "word2vec_dict={}\n",
    "for word in model.vocab.keys():\n",
    "    try:\n",
    "        word2vec_dict[word]=model[word]\n",
    "    except:    \n",
    "        pass\n",
    "print('Loaded %s word vectors.' % len(word2vec_dict))\n",
    "    \n",
    "embedding_vecor_length = len(model[word])\n",
    "print('embedding_vecor_length:',embedding_vecor_length)\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_vecor_length))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = word2vec_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print('\\nembedding matrix shape:',embedding_matrix.shape)\n",
    "print(embedding_matrix[0]) # first cell should be all zeros\n",
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Best model result holder\n",
    "best_model_aux = {}\n",
    "best_model_aux['Max length'] = max_description_length\n",
    "best_model_aux['Best score'] = 0\n",
    "best_model_aux['texts_to_sequences'] = texts_to_sequences_custom\n",
    "best_model_aux['word_index'] = word_index\n",
    "best_model_aux['Label encoder'] = le\n",
    "\n",
    "\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optional\n",
    "# Load previous model (if needs to be compared in the following training)\n",
    "#best_model = load_model('category_927_nets_1000_model.h5')\n",
    "#best_model_aux = get_model_file_aux('category_927_nets_1000_model_aux.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes count: 398\n"
     ]
    }
   ],
   "source": [
    "# prediction nodes count\n",
    "nb_classes = train_df['category_full_path_mod1'].unique()\n",
    "print('Classes count:', len(nb_classes))\n",
    "\n",
    "train_df = None\n",
    "samples_df = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 30, 64)        15036864    embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 128)           98816       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 398)           51342       lstm_1[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 15,187,022\n",
      "Trainable params: 150,158\n",
      "Non-trainable params: 15,036,864\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch iter #1\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1265s - loss: 1.1055 - acc: 0.7114  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.792852653892\n",
      "\n",
      "Epoch iter #2\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 989s - loss: 0.6829 - acc: 0.8064   \n",
      "Captured improved model\n",
      "Valid accuracy:  0.822426263036\n",
      "\n",
      "Epoch iter #3\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 946s - loss: 0.6138 - acc: 0.8240   \n",
      "Captured improved model\n",
      "Valid accuracy:  0.83392805786\n",
      "\n",
      "Epoch iter #4\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 982s - loss: 0.5742 - acc: 0.8343   \n",
      "Captured improved model\n",
      "Valid accuracy:  0.841525974373\n",
      "\n",
      "Epoch iter #5\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 997s - loss: 0.5467 - acc: 0.8417   \n",
      "Captured improved model\n",
      "Valid accuracy:  0.847221482515\n",
      "\n",
      "Epoch iter #6\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 959s - loss: 0.5262 - acc: 0.8470   \n",
      "Captured improved model\n",
      "Valid accuracy:  0.853788306473\n",
      "\n",
      "Epoch iter #7\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 927s - loss: 0.5092 - acc: 0.8515   \n",
      "Captured improved model\n",
      "Valid accuracy:  0.856713285969\n",
      "\n",
      "Epoch iter #8\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1083s - loss: 0.4954 - acc: 0.8551  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.862987185906\n",
      "\n",
      "Epoch iter #9\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1836s - loss: 0.4833 - acc: 0.8584  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.865590481567\n",
      "\n",
      "Epoch iter #10\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1706s - loss: 0.4736 - acc: 0.8610  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.866940062419\n",
      "\n",
      "Epoch iter #11\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1681s - loss: 0.4646 - acc: 0.8636  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.869882084766\n",
      "\n",
      "Epoch iter #12\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1685s - loss: 0.4569 - acc: 0.8655  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.871215687945\n",
      "\n",
      "Epoch iter #13\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1682s - loss: 0.4497 - acc: 0.8674  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.874449569136\n",
      "\n",
      "Epoch iter #14\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1689s - loss: 0.4435 - acc: 0.8690  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.876784439876\n",
      "\n",
      "Epoch iter #15\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1679s - loss: 0.4383 - acc: 0.8705  \n",
      "Training took 25616.9 s\n"
     ]
    }
   ],
   "source": [
    "# TRAIN MODEL 1\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "model = Sequential()\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ---- Embedding layer -----------------------------------------------------------------\n",
    "embedding_layer = Embedding(top_words, \n",
    "                            embedding_vecor_length, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length = max_description_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "## LSTM 1\n",
    "## ======================================================================================\n",
    "LSTM_1 = LSTM(128,return_sequences=False)\n",
    "model.add(LSTM_1)\n",
    "\n",
    "## Dense 1\n",
    "## ======================================================================================\n",
    "Dense_1 = Dense(128,activation='sigmoid')\n",
    "#model.add(Dense_1)\n",
    "\n",
    "## Output classes layer\n",
    "## ======================================================================================\n",
    "model.add(Dense(len(nb_classes), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try loss=categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(20):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx, y_ary_cat, nb_epoch=1, batch_size=batch_size_)\n",
    "    \n",
    "    scores = model.evaluate(train_texts_vec_mtx, y_ary_cat, verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))\n",
    "\n",
    "# Accuracy: loss: 0.2879 - acc: 0.9090\n",
    "# Valid accuracy:  0.915107423227"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.87678443987614019"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_aux['Best score']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FUNCTION FOR TRAIN MODEL 2\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializations\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializations.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 30, 64)        15036864    embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 30, 128)       98816       embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistribut (None, 30, 200)       25800       lstm_1[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "attlayer_1 (AttLayer)            (None, 200)           200         timedistributed_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 398)           79998       attlayer_1[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 15,241,678\n",
      "Trainable params: 204,814\n",
      "Non-trainable params: 15,036,864\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch iter #1\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1314s - loss: 1.0071 - acc: 0.7329  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.83166029335\n",
      "\n",
      "Epoch iter #2\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1329s - loss: 0.5154 - acc: 0.8465  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.858745646084\n",
      "\n",
      "Epoch iter #3\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1217s - loss: 0.4404 - acc: 0.8672  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.875484922401\n",
      "\n",
      "Epoch iter #4\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1199s - loss: 0.4003 - acc: 0.8785  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.884913880337\n",
      "\n",
      "Epoch iter #5\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1179s - loss: 0.3734 - acc: 0.8862  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.89090337768\n",
      "\n",
      "Epoch iter #6\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1181s - loss: 0.3532 - acc: 0.8917  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.897052651761\n",
      "\n",
      "Epoch iter #7\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1178s - loss: 0.3374 - acc: 0.8963  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.90096611668\n",
      "\n",
      "Epoch iter #8\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1149s - loss: 0.3249 - acc: 0.8996  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.905237481492\n",
      "\n",
      "Epoch iter #9\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1144s - loss: 0.3142 - acc: 0.9026  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.9069566792\n",
      "\n",
      "Epoch iter #10\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1160s - loss: 0.3051 - acc: 0.9050  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.909822008713\n",
      "\n",
      "Epoch iter #11\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1152s - loss: 0.2970 - acc: 0.9072  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.911577422482\n",
      "\n",
      "Epoch iter #12\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1143s - loss: 0.2899 - acc: 0.9091  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.914466185916\n",
      "\n",
      "Epoch iter #13\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1151s - loss: 0.2838 - acc: 0.9108  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.91495829827\n",
      "\n",
      "Epoch iter #14\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1148s - loss: 0.2774 - acc: 0.9128  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.916454873723\n",
      "\n",
      "Epoch iter #15\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1155s - loss: 0.2722 - acc: 0.9140  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.91873115966\n",
      "\n",
      "Epoch iter #16\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1151s - loss: 0.2671 - acc: 0.9152  \n",
      "Training took 23582.3 s\n"
     ]
    }
   ],
   "source": [
    "# TRAIN MODEL 2\n",
    "# Attention-based\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "model = Sequential()\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ---- Embedding layer -----------------------------------------------------------------\n",
    "embedding_layer = Embedding(top_words, \n",
    "                            embedding_vecor_length, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length = max_description_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "## LSTM 1\n",
    "## ======================================================================================\n",
    "LSTM_1 = LSTM(128,return_sequences=True)\n",
    "model.add(LSTM_1)\n",
    "\n",
    "## Attention 1\n",
    "## ======================================================================================\n",
    "TimeDist_1 = TimeDistributed(Dense(200))  #, input_shape=(max_description_length, embedding_vecor_length)\n",
    "model.add(TimeDist_1)\n",
    "model.add(AttLayer())\n",
    "\n",
    "## Dense 1\n",
    "## ======================================================================================\n",
    "Dense_1 = Dense(128,activation='sigmoid')\n",
    "#model.add(Dense_1)\n",
    "\n",
    "## Output classes layer\n",
    "## ======================================================================================\n",
    "model.add(Dense(len(nb_classes), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try loss=categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(20):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx, y_ary_cat, nb_epoch=1, batch_size=batch_size_)\n",
    "    \n",
    "    scores = model.evaluate(train_texts_vec_mtx, y_ary_cat, verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))\n",
    "\n",
    "# Accuracy: loss: 0.2722 - acc: 0.9140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.91873115965956897"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_aux['Best score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Nets design\n",
    "#from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "plot(model, to_file='/Users/altay.amanbay/Desktop/model.png')\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FUNCTION FOR TRAIN MODEL 3\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "        Attention operation, with a context/query vector, for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "        \"Hierarchical Attention Networks for Document Classification\"\n",
    "        by using a context vector to assist the attention\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(AttentionWithContext())\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializations.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = K.dot(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number ε to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_8 (Embedding)          (None, 30, 64)        15036864    embedding_input_8[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_8 (TimeDistribut (None, 30, 200)       13000       embedding_8[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_11 (LSTM)                   (None, 128)           168448      timedistributed_8[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dense_20 (Dense)                 (None, 398)           51342       lstm_11[0][0]                    \n",
      "====================================================================================================\n",
      "Total params: 15,269,654\n",
      "Trainable params: 232,790\n",
      "Non-trainable params: 15,036,864\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch iter #1\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1149s - loss: 0.8609 - acc: 0.7741  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.873400368552\n",
      "\n",
      "Epoch iter #2\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1070s - loss: 0.4008 - acc: 0.8796  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.892280653168\n",
      "\n",
      "Epoch iter #3\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1170s - loss: 0.3515 - acc: 0.8933  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.900832969397\n",
      "\n",
      "Epoch iter #4\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1138s - loss: 0.3260 - acc: 0.9001  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.906560432888\n",
      "\n",
      "Epoch iter #5\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1245s - loss: 0.3092 - acc: 0.9043  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.912048231271\n",
      "\n",
      "Epoch iter #6\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1245s - loss: 0.2964 - acc: 0.9076  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.913732278096\n",
      "\n",
      "Epoch iter #7\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1259s - loss: 0.2872 - acc: 0.9104  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.918558600781\n",
      "\n",
      "Epoch iter #8\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1256s - loss: 0.2795 - acc: 0.9124  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.918972955124\n",
      "\n",
      "Epoch iter #9\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1235s - loss: 0.2733 - acc: 0.9139  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.920193649407\n",
      "\n",
      "Epoch iter #10\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1284s - loss: 0.2677 - acc: 0.9152  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.923365750258\n",
      "\n",
      "Epoch iter #11\n",
      "Epoch 1/1\n",
      "938810/938810 [==============================] - 1243s - loss: 0.2634 - acc: 0.9165  \n",
      "Training took 16461.6 s\n"
     ]
    }
   ],
   "source": [
    "# TRAIN MODEL 3\n",
    "# Attention-based\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "model = Sequential()\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ---- Embedding layer -----------------------------------------------------------------\n",
    "embedding_layer = Embedding(top_words, \n",
    "                            embedding_vecor_length, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length = max_description_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "## LSTM 1\n",
    "## ======================================================================================\n",
    "LSTM_1 = LSTM(128,return_sequences=True)\n",
    "# model.add(LSTM_1)\n",
    "\n",
    "## Attention 1\n",
    "## ======================================================================================\n",
    "TimeDist_1 = TimeDistributed(Dense(200))  #, input_shape=(max_description_length, embedding_vecor_length)\n",
    "model.add(TimeDist_1)\n",
    "#model.add(AttentionWithContext())\n",
    "\n",
    "LSTM_2 = LSTM(128,return_sequences=False)\n",
    "model.add(LSTM_2)\n",
    "\n",
    "## Dense 1\n",
    "## ======================================================================================\n",
    "Dense_1 = Dense(128,activation='sigmoid')\n",
    "#model.add(Dense_1)\n",
    "\n",
    "## Output classes layer\n",
    "## ======================================================================================\n",
    "model.add(Dense(len(nb_classes), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try loss=categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(20):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx, y_ary_cat, nb_epoch=1, batch_size=batch_size_)\n",
    "    \n",
    "    scores = model.evaluate(train_texts_vec_mtx, y_ary_cat, verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))\n",
    "\n",
    "Accuracy: loss: 0.2677 - acc: 0.9152\n",
    "Valid accuracy:  0.923365750258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_aux['Best score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Nets design\n",
    "#from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "plot(model, to_file='/Users/altay.amanbay/Desktop/model.png')\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAIN MODEL 4\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "model = Sequential()\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ---- Embedding layer -----------------------------------------------------------------\n",
    "embedding_layer = Embedding(top_words, \n",
    "                            embedding_vecor_length, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length = max_description_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "## LSTM 1\n",
    "## ======================================================================================\n",
    "LSTM_1 = LSTM(128,return_sequences=True, activation='softmax')\n",
    "model.add(LSTM_1)\n",
    "\n",
    "## Attention 1\n",
    "## ======================================================================================\n",
    "TimeDist_1 = TimeDistributed(Dense(200))  #, input_shape=(max_description_length, embedding_vecor_length)\n",
    "model.add(TimeDist_1)\n",
    "#model.add(AttentionWithContext())\n",
    "\n",
    "LSTM_2 = LSTM(128,return_sequences=False)\n",
    "model.add(LSTM_2)\n",
    "\n",
    "## Output classes layer\n",
    "## ======================================================================================\n",
    "model.add(Dense(len(nb_classes), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try loss=categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(20):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx, y_ary_cat, nb_epoch=1, batch_size=batch_size_)\n",
    "    \n",
    "    scores = model.evaluate(train_texts_vec_mtx, y_ary_cat, verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))\n",
    "\n",
    "# Fails with acc: 0.0636"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_8 (Embedding)          (None, 30, 64)        15036864    embedding_input_8[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_7 (TimeDistribut (None, 30, 200)       13000       embedding_8[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "lstm_15 (LSTM)                   (None, 30, 128)       168448      timedistributed_7[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "attentionwithcontext_3 (Attentio (None, 128)           16640       lstm_15[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "dense_15 (Dense)                 (None, 398)           51342       attentionwithcontext_3[0][0]     \n",
      "====================================================================================================\n",
      "Total params: 15,286,294\n",
      "Trainable params: 249,430\n",
      "Non-trainable params: 15,036,864\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch iter #1\n",
      "Epoch 1/1\n",
      " 73664/938810 [=>............................] - ETA: 1040s - loss: nan - acc: 0.0268"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-24-4c47ecb87946>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     40\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     41\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch iter #'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 42\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts_vec_mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ary_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     43\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts_vec_mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ary_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1194\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAIN MODEL 5\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "model = Sequential()\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ---- Embedding layer -----------------------------------------------------------------\n",
    "embedding_layer = Embedding(top_words, \n",
    "                            embedding_vecor_length, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length = max_description_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "## LSTM 1\n",
    "## ======================================================================================\n",
    "LSTM_1 = LSTM(128,return_sequences=True)\n",
    "#model.add(LSTM_1)\n",
    "\n",
    "## Attention 1\n",
    "## ======================================================================================\n",
    "TimeDist_1 = TimeDistributed(Dense(200))  #, input_shape=(max_description_length, embedding_vecor_length)\n",
    "model.add(TimeDist_1)\n",
    "#model.add(AttLayer())\n",
    "#model.add(AttentionWithContext())\n",
    "\n",
    "LSTM_2 = LSTM(128,return_sequences=True)\n",
    "model.add(LSTM_2)\n",
    "model.add(AttentionWithContext())\n",
    "## Output classes layer\n",
    "## ======================================================================================\n",
    "model.add(Dense(len(nb_classes), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try loss=categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(20):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx, y_ary_cat, nb_epoch=1, batch_size=batch_size_)\n",
    "    \n",
    "    scores = model.evaluate(train_texts_vec_mtx, y_ary_cat, verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))\n",
    "\n",
    "# Fails with acc: 0.0636"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save model and aux file\n",
    "\n",
    "best_model.save('nets_category_all_unigrams__traindata5000_vectrain5000_model.h5')\n",
    "\n",
    "best_model_aux_name = 'nets_category_all_unigrams__traindata5000_vectrain5000_aux.pkl'\n",
    "with open(best_model_aux_name, 'wb') as pickle_file:\n",
    "    pickle.dump(best_model_aux, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 86.66%\n",
      "Accuracy on test set: 83.86%\n",
      "\n",
      "Evaluation took 390.131 s\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "start = time.time()\n",
    "\n",
    "scores = model.evaluate(train_texts_vec_mtx, y_train_ary, verbose=0)\n",
    "print(\"Accuracy on train set: %.2f%%\" % (scores[1]*100))\n",
    "scores = model.evaluate(test_texts_vec_mtx, y_test_ary, verbose=0)\n",
    "print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "print(\"\\nEvaluation took %g s\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "284256/284293 [============================>.] - ETA: 0spredictions[0]       : 304\n",
      "\n",
      "Prediction took 242.941 s\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "start = time.time()\n",
    "\n",
    "predictions = model.predict_classes(test_texts_vec_mtx)\n",
    "#predictions_rnd = np.round_(predictions, decimals=0, out=None)\n",
    "predictions_probs = model.predict(test_texts_vec_mtx)\n",
    "\n",
    "print('%-20s' % \"predictions[0]\",':', predictions[0])\n",
    "#print('%-20s' % \"predictions_rnd[0]:\",':',predictions_rnd[0])\n",
    "#print('%-20s' % \"predictions_probs[0]\",':', predictions_probs[0])\n",
    "print(\"\\nPrediction took %g s\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tools & Home Improvement > Lighting, Light Bulbs & Ceiling Fans > Other\n",
      "[ 'progress lighting p299281 archiethree light bath vanity antique nickel finish'\n",
      " 'bulbrite 60a15f 60watt incandescent a15 appliance bulb frost'\n",
      " 'greenhouse indooroutdoor chandelier rasped iron finish' ...,\n",
      " 'ge advantage fluorescent'\n",
      " 'feit ctcdm500led 60w equivalent candelabra base torpedo tip chandelier led light soft white'\n",
      " 'alena 97 arched floor lamp by house of hampton'] \n",
      "\n",
      "Tools & Home Improvement > Lighting, Light Bulbs & Ceiling Fans > Light Fixtures & Lamps\n",
      "['cortina nightstand right door'\n",
      " 'camino vintage candelabra twotier chandelier 72'\n",
      " 'golden lighting 2501ba3' ..., 'aspect white 23 open unit'\n",
      " 'harpwell 7light oilrubbed bronze chandelier'\n",
      " 'connie 2light antique black flush mount'] \n",
      "\n",
      "Tools & Home Improvement > Lighting, Light Bulbs & Ceiling Fans > Light Bulbs\n",
      "[ '3 pack led light bulbs lohas b35 7w soft white 3000k e12 candelabra bulb equivalent to 6065 watt incandescent'\n",
      " 'br30 led bulbsluminwiz 9w 3000k 680lm soft white dimmable flood light bulb65w equivalentmedium base e26dimmableul listedenergy star pack of 4'\n",
      " 'philips 12watt 65w 420281 led br30 flood bright white 3000k light bulb'\n",
      " ...,\n",
      " '10 pack par38 led 13 watt 120w equivalent 3000k warm white dimmable indooroutdoor lighting 1050 lumens'\n",
      " '4watt 25w equivalent mr 16 gu 53 led bulb 249 lumens neutral bright 3500k  nondimmable'\n",
      " 'bulbrite industries r20 halogen reflector flood bulb'] \n",
      "\n"
     ]
    }
   ],
   "source": [
    "# Manual check which nodes are not distinguishable for nets\n",
    "class_ = 392\n",
    "print(le.inverse_transform(class_))\n",
    "print(X_test_ls[np_utils.categorical_probas_to_classes(y_test_ary)==class_],'\\n')\n",
    "\n",
    "class_ = 390\n",
    "print(le.inverse_transform(class_))\n",
    "print(X_test_ls[np_utils.categorical_probas_to_classes(y_test_ary)==class_],'\\n')\n",
    "\n",
    "class_ = 389\n",
    "print(le.inverse_transform(class_))\n",
    "print(X_test_ls[np_utils.categorical_probas_to_classes(y_test_ary)==class_],'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>9</th>\n",
       "      <th>10</th>\n",
       "      <th>11</th>\n",
       "      <th>12</th>\n",
       "      <th>...</th>\n",
       "      <th>389</th>\n",
       "      <th>390</th>\n",
       "      <th>392</th>\n",
       "      <th>393</th>\n",
       "      <th>394</th>\n",
       "      <th>395</th>\n",
       "      <th>396</th>\n",
       "      <th>397</th>\n",
       "      <th>403</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>120</td>\n",
       "      <td>1</td>\n",
       "      <td>22</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>18</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>27</td>\n",
       "      <td>0</td>\n",
       "      <td>1226</td>\n",
       "      <td>189</td>\n",
       "      <td>70</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1540</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>190</td>\n",
       "      <td>1146</td>\n",
       "      <td>60</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>146</td>\n",
       "      <td>311</td>\n",
       "      <td>1001</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>321</td>\n",
       "      <td>31</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>367</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>42</td>\n",
       "      <td>1291</td>\n",
       "      <td>146</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>113</td>\n",
       "      <td>496</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>622</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>134</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>139</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>50</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>86</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1476</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1570</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1511</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>14</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1323</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1446</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1519</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1537</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>366</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>370</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>371</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>847</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>646</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1512</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>270</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>619</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>23</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1348</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1475</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>1444</td>\n",
       "      <td>18</td>\n",
       "      <td>47</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1538</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>29</td>\n",
       "      <td>1253</td>\n",
       "      <td>80</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1445</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>328</td>\n",
       "      <td>294</td>\n",
       "      <td>804</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1471</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>519</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>862</td>\n",
       "      <td>1</td>\n",
       "      <td>6</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>885</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "      <td>1411</td>\n",
       "      <td>6</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1451</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>1246</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>383</td>\n",
       "      <td>0</td>\n",
       "      <td>387</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>400</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>10</td>\n",
       "      <td>35</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>164</td>\n",
       "      <td>21</td>\n",
       "      <td>1641</td>\n",
       "      <td>1697</td>\n",
       "      <td>1157</td>\n",
       "      <td>11</td>\n",
       "      <td>370</td>\n",
       "      <td>1474</td>\n",
       "      <td>666</td>\n",
       "      <td>155</td>\n",
       "      <td>...</td>\n",
       "      <td>1832</td>\n",
       "      <td>1706</td>\n",
       "      <td>964</td>\n",
       "      <td>532</td>\n",
       "      <td>864</td>\n",
       "      <td>1424</td>\n",
       "      <td>1262</td>\n",
       "      <td>387</td>\n",
       "      <td>29</td>\n",
       "      <td>284293</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>364 rows × 330 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    1   2     3     4     5   6    9    10   11   12   ...     389  \\\n",
       "True                                                            ...           \n",
       "1          120   1    22     9     0   0    0     1    0    0   ...       0   \n",
       "2            1  18     1     0     4   0    0     0    0    0   ...       0   \n",
       "3           27   0  1226   189    70   0    0     0    0    0   ...       0   \n",
       "4            6   0   190  1146    60   0    1     2    7    0   ...       0   \n",
       "5            4   1   146   311  1001   0    0     0    0    0   ...       0   \n",
       "6            0   0     0     0     0   9    0     0    0    0   ...       0   \n",
       "9            3   0     0     1     0   0  321    31    3    0   ...       0   \n",
       "10           0   0     4     2     0   0   42  1291  146    0   ...       0   \n",
       "11           0   0     1     0     0   0    1   113  496    0   ...       0   \n",
       "12           0   0     0     0     0   0    0     0    0  134   ...       0   \n",
       "13           0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "14           0   0     0     0     0   0    0     0    0    6   ...       0   \n",
       "15           1   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "16           0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "17           0   0     3     1     0   0    0     1    2    0   ...       0   \n",
       "18           0   0     0     2     0   0    0     0    0    0   ...       0   \n",
       "19           1   0     0     1     1   0    0     1    0    0   ...       0   \n",
       "20           0   0     0     0     0   0    0     0    0   14   ...       0   \n",
       "21           0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "22           0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "23           0   0     1     0     0   0    0     0    0    0   ...       0   \n",
       "24           0   0     0     0     0   0    0     0    0    1   ...       0   \n",
       "25           0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "26           0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "27           0   0     0     0     2   0    0     0    0    0   ...       0   \n",
       "28           0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "30           0   0     0     0     0   0    1     2    0    0   ...       0   \n",
       "31           0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "32           0   0     1     0     1   0    0     1    0    0   ...       0   \n",
       "33           0   0     3     2     0   0    0     0    0    0   ...       0   \n",
       "...        ...  ..   ...   ...   ...  ..  ...   ...  ...  ...   ...     ...   \n",
       "366          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "367          0   0     0     1     0   0    0     0    0    0   ...       0   \n",
       "368          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "369          0   0     9     3     0   0    0     0    0    0   ...       1   \n",
       "370          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "371          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "373          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "376          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "378          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "379          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "380          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "381          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "382          0   0     0     0     0   0    0     0    0    0   ...       4   \n",
       "383          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "384          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "385          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "386          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "387          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "388          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "389          0   0     0     0     0   0    0     0    0    0   ...    1444   \n",
       "390          0   0     1     2     0   0    0     0    0    0   ...      29   \n",
       "392          0   0     0     0     0   0    0     0    1    0   ...     328   \n",
       "393          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "394          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "395          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "396          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "397          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "400          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "403          0   0     0     0     0   0    0     0    0    0   ...       0   \n",
       "All        164  21  1641  1697  1157  11  370  1474  666  155   ...    1832   \n",
       "\n",
       "Predicted   390  392  393  394   395   396  397  403     All  \n",
       "True                                                          \n",
       "1             0    0    0    0     0     0    0    0     158  \n",
       "2             0    0    0    0     0     0    0    0      25  \n",
       "3             0    0    0    0     0     0    0    0    1540  \n",
       "4             0    0    0    0     0     0    0    0    1451  \n",
       "5             1    0    0    0     0     0    0    0    1493  \n",
       "6             0    0    0    0     0     0    0    0       9  \n",
       "9             0    0    0    0     0     0    0    0     367  \n",
       "10            0    0    0    0     0     0    0    0    1514  \n",
       "11            0    0    0    0     0     0    0    0     622  \n",
       "12            0    0    0    0     0     0    0    0     139  \n",
       "13            0    0    0    0     0     0    0    0      50  \n",
       "14            0    0    0    0     0     0    0    0      86  \n",
       "15            1    0    0    0     0     0    0    0    1456  \n",
       "16            1    0    0    0     0     0    0    0    1476  \n",
       "17            0    0    0    0     0     0    0    0    1570  \n",
       "18            0    0    0    0     0     0    0    0    1440  \n",
       "19            0    0    0    0     0     0    0    0    1511  \n",
       "20            0    0    0    0     0     0    0    0    1512  \n",
       "21            0    0    0    0     0     0    0    0    1474  \n",
       "22            0    0    0    0     0     0    0    0    1462  \n",
       "23            0    0    0    0     0     0    0    0    1573  \n",
       "24            0    0    0    0     0     0    0    0    1507  \n",
       "25            0    0    0    0     0     0    0    0    1323  \n",
       "26            0    0    0    0     0     0    0    0    1446  \n",
       "27            0    0    0    0     0     0    0    0    1457  \n",
       "28            0    0    0    0     0     0    0    0    1080  \n",
       "30            0    0    0    0     0     0    0    0    1441  \n",
       "31            0    0    0    0     0     0    0    0    1433  \n",
       "32            0    0    0    0     0     0    0    0    1519  \n",
       "33            1    0    0    0     0     0    0    0    1537  \n",
       "...         ...  ...  ...  ...   ...   ...  ...  ...     ...  \n",
       "366           0    0    0    0     0     0    0    0       1  \n",
       "367           0    0    0    0     0     0    0    0       9  \n",
       "368           0    0    0    0     0     0    0    0      10  \n",
       "369           0    1    0    0     0     0    0    0    1545  \n",
       "370           0    0    0    0     0     0    0    0       1  \n",
       "371           0    0    0    0     0     0    0    0       1  \n",
       "373           0    0    0    0     0     0    0    0     847  \n",
       "376           1    0    0    0     0     0    0    0     646  \n",
       "378           0    0    0    0     0     0    0    0    1512  \n",
       "379           1    0    0    0     0     0    0    0     270  \n",
       "380           1    0    0    0     0     0    0    0     619  \n",
       "381           0    0    0    0     0     0    0    0     484  \n",
       "382          23    2    0    0     0     1    0    0    1574  \n",
       "383           6    0    0    0     0     0    0    0    1212  \n",
       "384           0    0    0    0     0     0    0    0       3  \n",
       "385           0    0    0    0     0     0    0    0    1348  \n",
       "386           1    0    0    0     0     0    0    0    1500  \n",
       "387           1    0    0    0     0     1    0    0    1475  \n",
       "388           9    2    0    0     0     0    0    0    1499  \n",
       "389          18   47    1    0     0     0    0    0    1538  \n",
       "390        1253   80    0    0     1     0    0    0    1445  \n",
       "392         294  804    0    0     0     0    0    0    1471  \n",
       "393           0    0  519    0     0     0    0    0     536  \n",
       "394           0    1    0  862     1     6    0    0     885  \n",
       "395           0    0    2    1  1411     6    1    0    1451  \n",
       "396           1    0    0    0     5  1246    1    0    1271  \n",
       "397           0    0    0    0     0     1  383    0     387  \n",
       "400           0    0    0    0     0     0    0    0       1  \n",
       "403           0    0    0    0     0     0    0   10      35  \n",
       "All        1706  964  532  864  1424  1262  387   29  284293  \n",
       "\n",
       "[364 rows x 330 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.crosstab(pd.Series(y_test_ary.ravel()), pd.Series(predictions_rnd.ravel()), rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "pd.crosstab(pd.Series(np_utils.categorical_probas_to_classes(y_test_ary)), pd.Series(predictions), rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0.0</th>\n",
       "      <th>1.0</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>78363</td>\n",
       "      <td>103</td>\n",
       "      <td>78466</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>102</td>\n",
       "      <td>1493</td>\n",
       "      <td>1595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>78465</td>\n",
       "      <td>1596</td>\n",
       "      <td>80061</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted    0.0   1.0    All\n",
       "True                         \n",
       "0          78363   103  78466\n",
       "1            102  1493   1595\n",
       "All        78465  1596  80061"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(pd.Series(np_utils.categorical_probas_to_classes(y_test_ary)), pd.Series(predictions.ravel()), rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model.get_config()\n",
    "def prediction_to_str(clf_prediction, category_id):\n",
    "    if(clf_prediction > 0.5):\n",
    "        return str(category_id)\n",
    "    else:\n",
    "        return 'not ' + str(category_id)\n",
    "\n",
    "def predict(description_str, tok_, clf_, max_length_, category_id_):\n",
    "    #seq_ = tok_.texts_to_sequences([description_str])\n",
    "    seq_ = texts_to_sequences_custom([description_str.lower()], tok_.word_index)\n",
    "    seq_pad = sequence.pad_sequences(seq_, maxlen = max_length_)\n",
    "    #clf_prediction = clf_.predict(seq_pad)\n",
    "    clf_prediction = old_best_model_.predict_classes(seq_pad, verbose=0)\n",
    "    \n",
    "    #print(seq_)\n",
    "    #print(seq_pad)\n",
    "    \n",
    "    # Prediction to string\n",
    "    #clf_prediction_str = prediction_to_str(clf_prediction[0][0], category_id_)\n",
    "    clf_prediction_str = clf_prediction\n",
    "    \n",
    "    return clf_prediction_str\n",
    "    #return clf_prediction[0][0]\n",
    "\n",
    "def predict_2(description_str, tok_, clf_, max_length_, category_id_):\n",
    "    #seq_ = tok_.texts_to_sequences([description_str])\n",
    "    seq_ = texts_to_sequences_custom([description_str.lower()], tok_.word_index)\n",
    "    seq_pad = sequence.pad_sequences(seq_, maxlen = max_length_)\n",
    "    #clf_prediction = clf_.predict(seq_pad)\n",
    "    clf_prediction = model.predict_classes(seq_pad)\n",
    "    \n",
    "    #print(seq_)\n",
    "    #print(seq_pad)\n",
    "    \n",
    "    # Prediction to string\n",
    "    #clf_prediction_str = prediction_to_str(clf_prediction[0][0], category_id_)\n",
    "    clf_prediction = le.inverse_transform(clf_prediction)\n",
    "    \n",
    "    if(clf_prediction == ['Positive']):\n",
    "        return str(category_id_)\n",
    "    else:\n",
    "        return 'not ' + str(category_id_)\n",
    "    \n",
    "    \n",
    "def predict_proba(description_str, tok_, clf_, max_length_):\n",
    "    #seq_ = tok_.texts_to_sequences([description_str])\n",
    "    seq_ = texts_to_sequences_custom([description_str], tok_.word_index)\n",
    "    seq_pad = sequence.pad_sequences(seq_, maxlen = max_length_)\n",
    "    clf_prediction_proba = clf_.predict_proba(seq_pad, verbose=0)\n",
    "    \n",
    "    return clf_prediction_proba[0][0]\n",
    "\n",
    "\n",
    "# id_ = 'table Setr'\n",
    "# p = predict(id_, best_model_aux['Tokenizer'], best_model, best_model_aux['Max length'], best_model_aux['Category ID'])\n",
    "# pp = predict_proba(id_, best_model_aux['Tokenizer'], best_model, best_model_aux['Max length'])\n",
    "# print(p)\n",
    "# print(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# model_file = \"nets_category_all_unigrams__traindata5000_vectrain5000_model.h5\"\n",
    "# aux_file = \"nets_category_all_unigrams__traindata5000_vectrain5000_aux.pkl\"\n",
    "# old_best_model_ = load_model(model_file)\n",
    "# old_best_model_aux_ = get_model_file_aux(aux_file)\n",
    "# old_tok_ = old_best_model_aux_['Tokenizer']\n",
    "#old_word_index_ = old_best_model_aux_['Tokenizer'].word_index\n",
    "old_word_index_ = best_model_aux['word_index']\n",
    "le = best_model_aux['Label encoder']\n",
    "\n",
    "\n",
    "# item_d = \"tekton 2780 10slot screwdriver holder and organizer\"\n",
    "# print('Old model prediction:')\n",
    "# print('item:',item_d)\n",
    "# print('Seq max len:', old_best_model_aux_['Max length'])\n",
    "# print(predict(item_d, old_tok_, old_best_model_, old_best_model_aux_['Max length'], '927'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "938720/938810 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "X_ls = np.array(list(train_df['description_mod1']))\n",
    "seq_ = texts_to_sequences_custom(X_ls, old_word_index_)\n",
    "seq_pad = sequence.pad_sequences(seq_, maxlen = 30)\n",
    "predictions = old_best_model_.predict_classes(seq_pad)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_mod1</th>\n",
       "      <th>category_id_mod1</th>\n",
       "      <th>category_full_path_mod1</th>\n",
       "      <th>target_le</th>\n",
       "      <th>Predictions_le</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>it jeans maternity skinny jeans dark wash m</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1822 denim butter maternity skinny jeans rinse...</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25 j brand maternity skinny jean nirvana blue</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26 j brand maternity skinny jean nirvana blue</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26 james jeans maternity skinny external mater...</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    description_mod1  category_id_mod1  \\\n",
       "0        it jeans maternity skinny jeans dark wash m               100   \n",
       "1  1822 denim butter maternity skinny jeans rinse...               100   \n",
       "2      25 j brand maternity skinny jean nirvana blue               100   \n",
       "3      26 j brand maternity skinny jean nirvana blue               100   \n",
       "4  26 james jeans maternity skinny external mater...               100   \n",
       "\n",
       "                       category_full_path_mod1  target_le  Predictions_le  \\\n",
       "0  Apparel & Accessories > Apparel > Maternity         27              27   \n",
       "1  Apparel & Accessories > Apparel > Maternity         27              27   \n",
       "2  Apparel & Accessories > Apparel > Maternity         27              27   \n",
       "3  Apparel & Accessories > Apparel > Maternity         27              27   \n",
       "4  Apparel & Accessories > Apparel > Maternity         27              27   \n",
       "\n",
       "                                   Predictions  \n",
       "0  Apparel & Accessories > Apparel > Maternity  \n",
       "1  Apparel & Accessories > Apparel > Maternity  \n",
       "2  Apparel & Accessories > Apparel > Maternity  \n",
       "3  Apparel & Accessories > Apparel > Maternity  \n",
       "4  Apparel & Accessories > Apparel > Maternity  "
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_df['Predictions_le'] = list(predictions)\n",
    "train_df['Predictions'] = train_df['Predictions_le'].apply(lambda x: le.inverse_transform(x))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.to_csv('truth_and_predictions.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ====================================================================================================\n",
      "Old model prediction:\n",
      "item: tekton 2655 flare nut wrench set metric 6piece\n",
      "Seq max len: 30\n",
      "not 927\n",
      "4.17323e-08\n",
      "\n",
      "Fresh model prediction:\n",
      "item: tekton 2655 flare nut wrench set metric 6piece\n",
      "Seq max len: 30\n",
      "1/1 [==============================] - 0s\n",
      "not 927\n",
      "\n",
      "1 ====================================================================================================\n",
      "Old model prediction:\n",
      "item: tekton 2780 10slot screwdriver holder and organizer\n",
      "Seq max len: 30\n",
      "not 927\n",
      "0.121584\n",
      "\n",
      "Fresh model prediction:\n",
      "item: tekton 2780 10slot screwdriver holder and organizer\n",
      "Seq max len: 30\n",
      "1/1 [==============================] - 0s\n",
      "927\n",
      "\n",
      "2 ====================================================================================================\n",
      "Old model prediction:\n",
      "item: titan 17237 insulated electrical screwdriver set  7 piece\n",
      "Seq max len: 30\n",
      "927\n",
      "0.998989\n",
      "\n",
      "Fresh model prediction:\n",
      "item: titan 17237 insulated electrical screwdriver set  7 piece\n",
      "Seq max len: 30\n",
      "1/1 [==============================] - 0s\n",
      "927\n",
      "\n",
      "3 ====================================================================================================\n",
      "Old model prediction:\n",
      "item: tool sorter screwdriver organizer red\n",
      "Seq max len: 30\n",
      "not 927\n",
      "0.405264\n",
      "\n",
      "Fresh model prediction:\n",
      "item: tool sorter screwdriver organizer red\n",
      "Seq max len: 30\n",
      "1/1 [==============================] - 0s\n",
      "927\n",
      "\n",
      "4 ====================================================================================================\n",
      "Old model prediction:\n",
      "item: torin sdh15rt magnetic screwdriver holder\n",
      "Seq max len: 30\n",
      "927\n",
      "0.994613\n",
      "\n",
      "Fresh model prediction:\n",
      "item: torin sdh15rt magnetic screwdriver holder\n",
      "Seq max len: 30\n",
      "1/1 [==============================] - 0s\n",
      "927\n",
      "\n",
      "5 ====================================================================================================\n",
      "Old model prediction:\n",
      "item: wera 05020013001 joker combination wrenchset 11 pieces\n",
      "Seq max len: 30\n",
      "not 927\n",
      "0.0210169\n",
      "\n",
      "Fresh model prediction:\n",
      "item: wera 05020013001 joker combination wrenchset 11 pieces\n",
      "Seq max len: 30\n",
      "1/1 [==============================] - 0s\n",
      "not 927\n",
      "\n",
      "6 ====================================================================================================\n",
      "Old model prediction:\n",
      "item: wera kk vde 60i62i68i18 insulated pouch set with interchangeable blades 18piece\n",
      "Seq max len: 30\n",
      "927\n",
      "0.995252\n",
      "\n",
      "Fresh model prediction:\n",
      "item: wera kk vde 60i62i68i18 insulated pouch set with interchangeable blades 18piece\n",
      "Seq max len: 30\n",
      "1/1 [==============================] - 0s\n",
      "927\n",
      "\n",
      "7 ====================================================================================================\n",
      "Old model prediction:\n",
      "item: wiha 28103 magnetic 14 bit holder stubby 57mm pliers screwdriver\n",
      "Seq max len: 30\n",
      "927\n",
      "0.972863\n",
      "\n",
      "Fresh model prediction:\n",
      "item: wiha 28103 magnetic 14 bit holder stubby 57mm pliers screwdriver\n",
      "Seq max len: 30\n",
      "1/1 [==============================] - 0s\n",
      "927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "\n",
    "## load old model\n",
    "model_file = \"category_927_nets__traindata5000_vectrain5000_model.h5\"\n",
    "aux_file = \"category_927_nets__traindata5000_vectrain5000_aux.pkl\"\n",
    "old_best_model_ = load_model(model_file)\n",
    "old_best_model_aux_ = get_model_file_aux(aux_file)\n",
    "old_tok_ = old_best_model_aux_['Tokenizer']\n",
    "old_word_index_ = old_best_model_aux_['Tokenizer'].word_index\n",
    "\n",
    "## use fresh model\n",
    "best_model_ = best_model\n",
    "best_model_aux_ = best_model_aux\n",
    "tok_ = tok\n",
    "word_index_ = word_index\n",
    "\n",
    "item_d = 'NieR: Automata™ DEMO 120161128 (Playable Demo)'\n",
    "\n",
    "# screwdrivers check\n",
    "scrw_items = [\n",
    "\"tekton 2655 flare nut wrench set metric 6piece\"\n",
    ",\"tekton 2780 10slot screwdriver holder and organizer\"\n",
    ",\"titan 17237 insulated electrical screwdriver set  7 piece\"\n",
    ",\"tool sorter screwdriver organizer red\"\n",
    ",\"torin sdh15rt magnetic screwdriver holder\"  #wrong predict\n",
    ",\"wera 05020013001 joker combination wrenchset 11 pieces\"\n",
    ",\"wera kk vde 60i62i68i18 insulated pouch set with interchangeable blades 18piece\" # tricky\n",
    ",\"wiha 28103 magnetic 14 bit holder stubby 57mm pliers screwdriver\" # tricky, wrong predict\n",
    "]\n",
    "\n",
    "for n, i in enumerate(scrw_items):\n",
    "    item_d = i\n",
    "    \n",
    "    print(str(n) + ' ' + '='*100)\n",
    "    \n",
    "    print('Old model prediction:')\n",
    "    print('item:',item_d)\n",
    "    print('Seq max len:', old_best_model_aux_['Max length'])\n",
    "    print(predict(item_d, old_tok_, old_best_model_, old_best_model_aux_['Max length'], '927'))\n",
    "    print(predict_proba(item_d, old_tok_, old_best_model_, old_best_model_aux_['Max length']))\n",
    "\n",
    "\n",
    "    print('\\nFresh model prediction:')\n",
    "    print('item:',item_d)\n",
    "    print('Seq max len:', best_model_aux_['Max length'])\n",
    "    print(predict_2(item_d, tok_, best_model_, best_model_aux_['Max length'], '927'))\n",
    "    #print(predict_proba(item_d, tok_, best_model_, best_model_aux_['Max length']))\n",
    "\n",
    "    print()\n",
    "\n",
    "    #tt = train_df.loc[0:10,['description_mod1']]\n",
    "    #tt['pred'] = tt['description_mod1'].apply(lambda x: predict(x, best_model_aux_['Tokenizer'], best_model, best_model_aux_['Max length'], best_model_aux_['Category ID']))\n",
    "    #tt['prob'] = tt['description_mod1'].apply(lambda x: predict_proba(x, best_model_aux_['Tokenizer'], best_model, best_model_aux_['Max length']))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
