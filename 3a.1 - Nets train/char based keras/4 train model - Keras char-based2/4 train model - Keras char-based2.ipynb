{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keras\n",
    "# Input tokens        : char-based with embeddings\n",
    "# Classification type : binary\n",
    "# Prediction output   : multi-nodes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.5.2 |Anaconda 4.2.0 (x86_64)| (default, Jul  2 2016, 17:52:12) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "import string\n",
    "\n",
    "from keras.datasets import reuters\n",
    "from keras.models import Sequential\n",
    "from keras.layers import LSTM, Dense, Dropout, Activation\n",
    "from keras.layers import Convolution1D, MaxPooling1D, Flatten\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.utils import np_utils\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.preprocessing.text import Tokenizer, one_hot, text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.models import load_model\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "\n",
    "from keras.utils.visualize_util import plot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import sys\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def get_word2index(texts_ls_):\n",
    "    word2index_ = {}\n",
    "\n",
    "    c = 1\n",
    "    for text_str in texts_ls_:\n",
    "        text_tokens_ls = text_str.lower().split()\n",
    "        for token in text_tokens_ls:\n",
    "            if(token not in word2index_):\n",
    "                word2index_[token] = c\n",
    "                c = c + 1\n",
    "                \n",
    "    return word2index_\n",
    "\n",
    "def train_df_preprocess(top_words_, texts_ls_, max_pad_length_):\n",
    "    # texts_ls_: list of texts strings\n",
    "    \n",
    "    tok = Tokenizer(top_words_)\n",
    "    tok.fit_on_texts(texts_ls_)\n",
    "\n",
    "    words = []\n",
    "    for iter in range(top_words):\n",
    "        words += [key for key,value in tok.word_index.items() if value==iter+1]\n",
    "\n",
    "    #Class for vectorizing texts, or/and turning texts into sequences \n",
    "    #(=list of word indexes, where the word of rank i in the dataset (starting at 1) has index i).\n",
    "    texts_vec_ls = tok.texts_to_sequences(texts_ls_)#turns text to sequence, stating which word comes in what place\n",
    "    texts_vec_mtx = sequence.pad_sequences(texts_vec_ls, maxlen=max_pad_length_)#pad sequence, essentially padding it with 0's at the end\n",
    "    \n",
    "    return texts_vec_mtx\n",
    "\n",
    "def text_2_vec(text_str, word2index_):\n",
    "    # text_str: text string\n",
    "    \n",
    "    text_tokens_ls = text_str.lower().split()\n",
    "    \n",
    "    text_vec = []\n",
    "    for token in text_tokens_ls:\n",
    "        if token in word2index_:\n",
    "            text_vec.append(word2index_[token])\n",
    "        else:\n",
    "            text_vec.append(0)\n",
    "            \n",
    "    return text_vec\n",
    "\n",
    "def train_df_preprocess_2(texts_ls_, word2index_, max_pad_length_):\n",
    "    # texts_ls_: list of texts strings\n",
    "    \n",
    "    texts_vec_ls = []\n",
    "    for text_ in texts_ls_:\n",
    "        #print(text_)\n",
    "        #print(type(text_))\n",
    "        text_vec = text_2_vec(text_, word2index_)\n",
    "        texts_vec_ls.append(text_vec)\n",
    "    \n",
    "    texts_vec_ary = sequence.pad_sequences(texts_vec_ls, maxlen=max_pad_length_)\n",
    "    \n",
    "    return texts_vec_ary\n",
    "\n",
    "def texts_to_sequences_custom(texts_ls, word_index_):\n",
    "    texts_seq = []\n",
    "    \n",
    "    for text in texts_ls:\n",
    "        text_split = text.lower().split()\n",
    "        seq = []\n",
    "        for token in text_split:\n",
    "            if(token in word_index_):\n",
    "                seq.append(word_index_[token])\n",
    "            else:\n",
    "                seq.append(0)\n",
    "                \n",
    "        texts_seq.append(seq)\n",
    "#         for k,v in word_index_.items():\n",
    "#             if(v == 395):\n",
    "#                 print(k,v)\n",
    "    return texts_seq\n",
    "\n",
    "\n",
    "def text_chars_to_sequences_custom(text_chars_ls, char_index_):\n",
    "    texts_seq = []\n",
    "    \n",
    "    for text in text_chars_ls:\n",
    "        text_split = text.lower().split()\n",
    "        seq = []\n",
    "        for token in text_split:\n",
    "            if(token in char_index_):\n",
    "                seq.append(char_index_[token])\n",
    "            else:\n",
    "                seq.append(0)\n",
    "                \n",
    "        texts_seq.append(seq)\n",
    "#         for k,v in word_index_.items():\n",
    "#             if(v == 395):\n",
    "#                 print(k,v)\n",
    "    return texts_seq\n",
    "\n",
    "def get_model_file_aux(model_file_aux_name):\n",
    "    with open(model_file_aux_name, 'rb') as pickle_file:\n",
    "        model_file_aux = pickle.load(pickle_file)\n",
    "    return model_file_aux"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (6822, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_mod1</th>\n",
       "      <th>category_id_mod1</th>\n",
       "      <th>category_full_path_mod1</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>RION TECH 5 point 6 point 3 Piece Tool Kit Pen...</td>\n",
       "      <td>927</td>\n",
       "      <td>Tools &amp; Home Improvement &gt; Power &amp; Hand Tools ...</td>\n",
       "      <td>Positive</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Stalwart 25-piece 4.8V Cordless Screwdriver Set</td>\n",
       "      <td>927</td>\n",
       "      <td>Tools &amp; Home Improvement &gt; Power &amp; Hand Tools ...</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    description_mod1  category_id_mod1  \\\n",
       "0  RION TECH 5 point 6 point 3 Piece Tool Kit Pen...               927   \n",
       "1    Stalwart 25-piece 4.8V Cordless Screwdriver Set               927   \n",
       "\n",
       "                             category_full_path_mod1    target  \n",
       "0  Tools & Home Improvement > Power & Hand Tools ...  Positive  \n",
       "1  Tools & Home Improvement > Power & Hand Tools ...     False  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read train set (screwdrivers)\n",
    "pkl_file = '/Users/altay.amanbay/Desktop/new node booster/experiments/3a.1 - Nets train/train_data.pkl'\n",
    "train_df = pd.read_pickle(pkl_file)\n",
    "\n",
    "# Create target feature\n",
    "train_df['target'] = train_df['type'].apply(lambda x: 'False' if x == 'False Positive' else 'Positive')\n",
    "\n",
    "# Drop index column\n",
    "train_df.drop(labels=['type'], axis=1, inplace=True)\n",
    "\n",
    "# Encode target feature\n",
    "#le = LabelEncoder()\n",
    "#le.fit(train_df['target'])\n",
    "#train_df['target_le'] = le.transform(train_df['target'])\n",
    "\n",
    "\n",
    "# cat = 'Tools & Home Improvement > Power & Hand Tools > Hand Tools > Screwdrivers'\n",
    "# positives, negatives = get_positives_negatives(train_df, cat)\n",
    "# X_train = input_text = pd.concat([positives, negatives])\n",
    "# y_train = [1] * len(positives) + [0] * len(negatives)\n",
    "\n",
    "print('train data shape:',train_df.shape)\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples data shape: (956776, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_mod1</th>\n",
       "      <th>category_id_mod1</th>\n",
       "      <th>category_full_path_mod1</th>\n",
       "      <th>target</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>!iT Jeans Maternity Skinny Jeans Dark Wash M</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1822 Denim 'Butter' Maternity Skinny Jeans Rin...</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>25 J Brand Maternity Skinny Jean nirvana blue</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>26 J Brand Maternity Skinny Jean nirvana blue</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>26 James Jeans Maternity Skinny External Mater...</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>False</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    description_mod1  category_id_mod1  \\\n",
       "0       !iT Jeans Maternity Skinny Jeans Dark Wash M               100   \n",
       "1  1822 Denim 'Butter' Maternity Skinny Jeans Rin...               100   \n",
       "2      25 J Brand Maternity Skinny Jean nirvana blue               100   \n",
       "3      26 J Brand Maternity Skinny Jean nirvana blue               100   \n",
       "4  26 James Jeans Maternity Skinny External Mater...               100   \n",
       "\n",
       "                       category_full_path_mod1 target  \n",
       "0  Apparel & Accessories > Apparel > Maternity  False  \n",
       "1  Apparel & Accessories > Apparel > Maternity  False  \n",
       "2  Apparel & Accessories > Apparel > Maternity  False  \n",
       "3  Apparel & Accessories > Apparel > Maternity  False  \n",
       "4  Apparel & Accessories > Apparel > Maternity  False  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read sampled descriptionary\n",
    "\n",
    "path = '/Users/altay.amanbay/Desktop/new node booster/experiments/train data from descriptionary nodes by sampling/3 - Picking samples from each node/sampled descriptionary/'\n",
    "file_name = 'sampled_descriptionary_sample_size_5000.csv'\n",
    "samples_df = pd.read_csv(path + file_name)\n",
    "\n",
    "# Rename columns\n",
    "samples_df.rename(columns={'description': 'description_mod1', \n",
    "                           'category_id': 'category_id_mod1',\n",
    "                           'category_path': 'category_full_path_mod1'}, inplace=True)\n",
    "\n",
    "# Drop 'screwdrivers' from descriptionary\n",
    "#samples_df = samples_df.loc[samples_df.category_id_mod1 != 927,:]\n",
    "\n",
    "# Drop index column\n",
    "samples_df.drop(labels=['index'], axis=1, inplace=True)\n",
    "\n",
    "# Add target column and make all false as all items are not screwdrivers\n",
    "samples_df['target'] = 'False'\n",
    "samples_df.loc[samples_df.category_id_mod1 == 927,['target']] = 'Positive'\n",
    "\n",
    "print('samples data shape:',samples_df.shape)\n",
    "samples_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'i t j e a n s m a t e r n i t y s k i n n y'"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def modif_1(phrase):\n",
    "    phrase_mod =  \" \".join((char if char.isalnum() else \"\") for char in phrase.lower())\n",
    "    phrase_mod = ' '.join(phrase_mod.split())\n",
    "    return phrase_mod\n",
    "\n",
    "def modif_2(phrase, remove_chars=['ï','½','®','™','©','ž','볶','불','양','¢','õ','î','€']):\n",
    "    phrase_mod = \" \".join((char if char in string.printable else \" \") for char in phrase.lower())\n",
    "    phrase_mod =  \" \".join((char if char.isalnum() else \"\") for char in phrase_mod)\n",
    "    phrase_mod = ' '.join(phrase_mod.split())\n",
    "    return phrase_mod\n",
    "\n",
    "modif_1('!iT Jeans Maternity Skinny')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train data shape: (963598, 4)\n",
      "train data shape (deduplicate): (945464, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_mod1</th>\n",
       "      <th>category_id_mod1</th>\n",
       "      <th>category_full_path_mod1</th>\n",
       "      <th>target</th>\n",
       "      <th>target_le</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>r i o n t e c h 5 p o i n t 6 p o i n t 3 p i ...</td>\n",
       "      <td>927</td>\n",
       "      <td>Tools &amp; Home Improvement &gt; Power &amp; Hand Tools ...</td>\n",
       "      <td>Positive</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>s t a l w a r t 2 5 p i e c e 4 8 v c o r d l ...</td>\n",
       "      <td>927</td>\n",
       "      <td>Tools &amp; Home Improvement &gt; Power &amp; Hand Tools ...</td>\n",
       "      <td>False</td>\n",
       "      <td>395</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    description_mod1  category_id_mod1  \\\n",
       "0  r i o n t e c h 5 p o i n t 6 p o i n t 3 p i ...               927   \n",
       "1  s t a l w a r t 2 5 p i e c e 4 8 v c o r d l ...               927   \n",
       "\n",
       "                             category_full_path_mod1    target  target_le  \n",
       "0  Tools & Home Improvement > Power & Hand Tools ...  Positive        395  \n",
       "1  Tools & Home Improvement > Power & Hand Tools ...     False        395  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Concat original train set and sampled descriptionary\n",
    "train_df = pd.concat([train_df, samples_df], axis=0)\n",
    "train_df.reset_index(drop=True, inplace=True)\n",
    "print('train data shape:',train_df.shape)\n",
    "\n",
    "# description into chars\n",
    "train_df['description_mod1'] = train_df['description_mod1'].apply(lambda x: modif_1(x))\n",
    "\n",
    "# deduplicate\n",
    "train_df.drop_duplicates(subset=['description_mod1'], inplace = True)\n",
    "print('train data shape (deduplicate):',train_df.shape)\n",
    "    \n",
    "\n",
    "# Encode target feature\n",
    "le = LabelEncoder()\n",
    "#le.fit(train_df['target'])\n",
    "#train_df['target_le'] = le.transform(train_df['target'])\n",
    "le.fit(train_df['category_full_path_mod1'])\n",
    "train_df['target_le'] = le.transform(train_df['category_full_path_mod1'])\n",
    "\n",
    "\n",
    "\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'numpy.ndarray'>\n",
      "<class 'numpy.ndarray'>\n",
      "661824\n",
      "283640\n",
      "(661824, 404)\n",
      "(283640, 404)\n"
     ]
    }
   ],
   "source": [
    "# Split into train and test\n",
    "#X = train_df.loc[:,['description_mod1']]\n",
    "X_ls = np.array(list(train_df['description_mod1']))\n",
    "#y_ary = np.array(list(train_df['target_le']))\n",
    "y_ary_cat = np_utils.to_categorical(train_df['target_le'])\n",
    "# X_ls = train_df[['description_mod1']]\n",
    "# y_ary = train_df[['target_le']]\n",
    "\n",
    "print(type(X_ls))\n",
    "#print(type(y_ary))\n",
    "print(type(y_ary_cat))\n",
    "\n",
    "#X_train_ls, X_test_ls, y_train_ary, y_test_ary = train_test_split(X_ls, y_ary, test_size = 0.3)\n",
    "X_train_ls, X_test_ls, y_train_ary, y_test_ary = train_test_split(X_ls, y_ary_cat, test_size = 0.3)\n",
    "\n",
    "# print(X_train_df.shape)\n",
    "# print(X_test_df.shape)\n",
    "print(len(X_train_ls))\n",
    "print(len(X_test_ls))\n",
    "print(y_train_ary.shape)\n",
    "#print(y_test_ary.shape)\n",
    "print(y_test_ary.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index size: 36\n",
      "train_texts_vec_mtx shape: (661824, 100)\n"
     ]
    }
   ],
   "source": [
    "# Convert train set into sequences for nets\n",
    "\n",
    "top_words = 200000\n",
    "max_description_length = 100\n",
    "\n",
    "# tok = Tokenizer(nb_words = top_words)\n",
    "# tok.fit_on_texts(X_train_ls)\n",
    "# word_index = tok.word_index\n",
    "# print('word_index size:',len(word_index))\n",
    "\n",
    "\n",
    "ls = '0123456789abcdefghijklmnopqrstuvwxyz'\n",
    "word_index = {}\n",
    "for i, c in enumerate(ls):\n",
    "    word_index[c] = i+1 #ord(c)\n",
    "print('word_index size:',len(word_index))\n",
    "\n",
    "# train_texts_vec_ls = tok.texts_to_sequences(X_train_ls)\n",
    "# train_texts_vec_ls = texts_to_sequences_custom(X_train_ls, word_index)\n",
    "train_texts_vec_ls = text_chars_to_sequences_custom(X_train_ls, word_index)\n",
    "train_texts_vec_mtx = sequence.pad_sequences(train_texts_vec_ls, maxlen = max_description_length)\n",
    "\n",
    "print('train_texts_vec_mtx shape:',train_texts_vec_mtx.shape)\n",
    "#list(tok.word_index)[0:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Convert test set into sequences for nets\n",
    "\n",
    "#test_texts_vec_ls = tok.texts_to_sequences(X_test_ls)\n",
    "#test_texts_vec_ls = texts_to_sequences_custom(X_test_ls, word_index)\n",
    "test_texts_vec_ls = text_chars_to_sequences_custom(X_test_ls, word_index)\n",
    "test_texts_vec_mtx = sequence.pad_sequences(test_texts_vec_ls, maxlen = max_description_length)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word index size: 36\n",
      "embedding matrix shape: (37, 1)\n",
      "[ 0.]\n",
      "[ 0.28436418]\n"
     ]
    }
   ],
   "source": [
    "# Create RANDOM embedding vectors for each word in word index (lower cell code preferable)\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(7)\n",
    "\n",
    "embedding_vecor_length = 1 #32\n",
    "#uniq_token_count = len(tok.word_index)\n",
    "uniq_token_count = len(word_index)\n",
    "print('word index size:', uniq_token_count)\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_vecor_length))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = np.random.uniform(.1, size=(1, embedding_vecor_length))\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "\n",
    "print('embedding matrix shape:',embedding_matrix.shape)\n",
    "print(embedding_matrix[0])\n",
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Best model result holder\n",
    "best_model_aux = {}\n",
    "best_model_aux['Max length'] = max_description_length\n",
    "best_model_aux['Best Score'] = 0\n",
    "#best_model_aux['Category ID'] = 927\n",
    "#best_model_aux['Category name'] = 'Tools & Home Improvement > Power & Hand Tools > Hand Tools > Screwdrivers'\n",
    "#best_model_aux['Tokenizer'] = tok\n",
    "best_model_aux['Word index'] = word_index\n",
    "\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optional\n",
    "# Load previous model (if needs to be compared in the following training)\n",
    "#best_model = load_model('category_927_nets_1000_model.h5')\n",
    "#best_model_aux = get_model_file_aux('category_927_nets_1000_model_aux.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0"
      ]
     },
     "execution_count": 82,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_aux['Best Score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes count: 404\n"
     ]
    }
   ],
   "source": [
    "# prediction nodes count\n",
    "classes_ls = train_df['category_full_path_mod1'].unique()\n",
    "print('Classes count:', len(classes_ls))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_59 (Embedding)         (None, 100, 1)        37          embedding_input_44[0][0]         \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_67 (Convolution1D) (None, 46, 128)       1408        embedding_59[0][0]               \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_68 (Convolution1D) (None, 7, 128)        98432       convolution1d_67[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_28 (Flatten)             (None, 896)           0           convolution1d_68[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "dense_71 (Dense)                 (None, 100)           89700       flatten_28[0][0]                 \n",
      "____________________________________________________________________________________________________\n",
      "dense_72 (Dense)                 (None, 404)           40804       dense_71[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 230,381\n",
      "Trainable params: 230,381\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch iter #1\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 206s - loss: 5.1079 - acc: 0.0355 - val_loss: 4.9697 - val_acc: 0.0515\n",
      "\n",
      "Epoch iter #2\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 210s - loss: 4.8300 - acc: 0.0661 - val_loss: 4.7421 - val_acc: 0.0765\n",
      "\n",
      "Epoch iter #3\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 210s - loss: 4.6161 - acc: 0.0919 - val_loss: 4.5599 - val_acc: 0.1010\n",
      "\n",
      "Epoch iter #4\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 211s - loss: 4.4782 - acc: 0.1112 - val_loss: 4.4554 - val_acc: 0.1155\n",
      "\n",
      "Epoch iter #5\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 213s - loss: 4.3745 - acc: 0.1269 - val_loss: 4.3609 - val_acc: 0.1316\n",
      "\n",
      "Epoch iter #6\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 211s - loss: 4.2725 - acc: 0.1433 - val_loss: 4.2777 - val_acc: 0.1454\n",
      "\n",
      "Epoch iter #7\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 209s - loss: 4.1670 - acc: 0.1596 - val_loss: 4.1837 - val_acc: 0.1597\n",
      "\n",
      "Epoch iter #8\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 210s - loss: 4.0676 - acc: 0.1749 - val_loss: 4.0806 - val_acc: 0.1776\n",
      "\n",
      "Epoch iter #9\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 224s - loss: 3.9642 - acc: 0.1924 - val_loss: 3.9964 - val_acc: 0.1905\n",
      "\n",
      "Epoch iter #10\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 248s - loss: 3.8620 - acc: 0.2095 - val_loss: 3.8968 - val_acc: 0.2090\n",
      "\n",
      "Epoch iter #11\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 245s - loss: 3.7660 - acc: 0.2253 - val_loss: 3.8259 - val_acc: 0.2209\n",
      "\n",
      "Epoch iter #12\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 244s - loss: 3.6790 - acc: 0.2395 - val_loss: 3.7534 - val_acc: 0.2324\n",
      "\n",
      "Epoch iter #13\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 249s - loss: 3.6006 - acc: 0.2534 - val_loss: 3.6845 - val_acc: 0.2435\n",
      "\n",
      "Epoch iter #14\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 251s - loss: 3.5320 - acc: 0.2653 - val_loss: 3.6164 - val_acc: 0.2557\n",
      "\n",
      "Epoch iter #15\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 245s - loss: 3.4713 - acc: 0.2758 - val_loss: 3.5976 - val_acc: 0.2605\n",
      "\n",
      "Epoch iter #16\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 271s - loss: 3.4177 - acc: 0.2850 - val_loss: 3.5269 - val_acc: 0.2723\n",
      "Captured improved model\n",
      "\n",
      "Epoch iter #17\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 257s - loss: 3.3685 - acc: 0.2935 - val_loss: 3.5024 - val_acc: 0.2771\n",
      "Captured improved model\n",
      "\n",
      "Epoch iter #18\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 268s - loss: 3.3231 - acc: 0.3023 - val_loss: 3.4474 - val_acc: 0.2858\n",
      "Captured improved model\n",
      "\n",
      "Epoch iter #19\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 295s - loss: 3.2816 - acc: 0.3094 - val_loss: 3.4360 - val_acc: 0.2911\n",
      "Captured improved model\n",
      "\n",
      "Epoch iter #20\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 269s - loss: 3.2433 - acc: 0.3165 - val_loss: 3.3886 - val_acc: 0.2972\n",
      "Captured improved model\n",
      "\n",
      "Epoch iter #21\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 250s - loss: 3.2089 - acc: 0.3227 - val_loss: 3.3612 - val_acc: 0.3029\n",
      "Captured improved model\n",
      "\n",
      "Epoch iter #22\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 247s - loss: 3.1775 - acc: 0.3281 - val_loss: 3.3439 - val_acc: 0.3059\n",
      "Captured improved model\n",
      "\n",
      "Epoch iter #23\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 235s - loss: 3.1480 - acc: 0.3335 - val_loss: 3.3164 - val_acc: 0.3116\n",
      "Captured improved model\n",
      "\n",
      "Epoch iter #24\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 234s - loss: 3.1210 - acc: 0.3382 - val_loss: 3.3041 - val_acc: 0.3134\n",
      "Captured improved model\n",
      "\n",
      "Epoch iter #25\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 233s - loss: 3.0961 - acc: 0.3430 - val_loss: 3.2790 - val_acc: 0.3196\n",
      "Captured improved model\n",
      "\n",
      "Epoch iter #26\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 250s - loss: 3.0737 - acc: 0.3469 - val_loss: 3.2689 - val_acc: 0.3203\n",
      "Captured improved model\n",
      "\n",
      "Epoch iter #27\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 247s - loss: 3.0509 - acc: 0.3509 - val_loss: 3.2488 - val_acc: 0.3256\n",
      "Captured improved model\n",
      "\n",
      "Epoch iter #28\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 238s - loss: 3.0309 - acc: 0.3542 - val_loss: 3.2346 - val_acc: 0.3279\n",
      "Captured improved model\n",
      "\n",
      "Epoch iter #29\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 238s - loss: 3.0121 - acc: 0.3579 - val_loss: 3.2261 - val_acc: 0.3281\n",
      "Captured improved model\n",
      "\n",
      "Epoch iter #30\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "183040/661824 [=======>......................] - ETA: 150s - loss: 2.9708 - acc: 0.3640"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-103-ccdad52c5991>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     65\u001b[0m               \u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_texts_vec_mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_ary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     66\u001b[0m               \u001b[0;34m,\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 67\u001b[0;31m               \u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     68\u001b[0m              )\n\u001b[1;32m     69\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1194\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1941\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 1943\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   1944\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    963\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mfinal_fetches\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0mfinal_targets\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    964\u001b[0m       results = self._do_run(handle, final_targets, final_fetches,\n\u001b[0;32m--> 965\u001b[0;31m                              feed_dict_string, options, run_metadata)\n\u001b[0m\u001b[1;32m    966\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    967\u001b[0m       \u001b[0mresults\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_run\u001b[0;34m(self, handle, target_list, fetch_list, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m   1013\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mhandle\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1014\u001b[0m       return self._do_call(_run_fn, self._session, feed_dict, fetch_list,\n\u001b[0;32m-> 1015\u001b[0;31m                            target_list, options, run_metadata)\n\u001b[0m\u001b[1;32m   1016\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1017\u001b[0m       return self._do_call(_prun_fn, self._session, handle, feed_dict,\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_do_call\u001b[0;34m(self, fn, *args)\u001b[0m\n\u001b[1;32m   1020\u001b[0m   \u001b[0;32mdef\u001b[0m \u001b[0m_do_call\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1021\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1022\u001b[0;31m       \u001b[0;32mreturn\u001b[0m \u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0margs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1023\u001b[0m     \u001b[0;32mexcept\u001b[0m \u001b[0merrors\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mOpError\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1024\u001b[0m       \u001b[0mmessage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcompat\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mas_text\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0me\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmessage\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run_fn\u001b[0;34m(session, feed_dict, fetch_list, target_list, options, run_metadata)\u001b[0m\n\u001b[1;32m   1002\u001b[0m         return tf_session.TF_Run(session, options,\n\u001b[1;32m   1003\u001b[0m                                  \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtarget_list\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1004\u001b[0;31m                                  status, run_metadata)\n\u001b[0m\u001b[1;32m   1005\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1006\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_prun_fn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msession\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfeed_dict\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfetch_list\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAIN MODEL 2\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 32   # 64\n",
    "hidden_nodes = 100\n",
    "nb_filter_1 = 128\n",
    "nb_filter_2 = 128\n",
    "filter_length_1 = 5\n",
    "filter_length_2 = 5\n",
    "subsample_length_ = 2  #stride\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(input_dim=top_words, \n",
    "                            output_dim=embedding_vecor_length, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length = max_description_length, # e.g. time_steps\n",
    "                            #batch_input_shape=(batch_size_, max_description_length), # use batch_input_shape when stateful=True\n",
    "                            trainable=True)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "conv1D_1 = Convolution1D(nb_filter = nb_filter_1\n",
    "                        ,filter_length = 10\n",
    "                        ,border_mode='valid'  # same, valid\n",
    "                        ,activation='sigmoid'\n",
    "                        ,subsample_length=2\n",
    "                        )\n",
    "model.add(conv1D_1)\n",
    "conv1D_2 = Convolution1D(nb_filter = nb_filter_1\n",
    "                        ,filter_length = 6\n",
    "                        ,border_mode='valid'  # same, valid\n",
    "                        ,activation='sigmoid'\n",
    "                        ,subsample_length=6\n",
    "                        )\n",
    "model.add(conv1D_2)\n",
    "#model.add(MaxPooling1D(pool_length = 2, stride = 2)) #model.output_shape[1]\n",
    "#model.add(Dropout(0.2))\n",
    "\n",
    "# conv1D_3 = Convolution1D(nb_filter = nb_filter_2\n",
    "#                         ,filter_length = 1\n",
    "#                         ,border_mode='valid'  # same, valid\n",
    "#                         ,activation='sigmoid'\n",
    "#                         ,subsample_length=1\n",
    "#                         )\n",
    "# model.add(conv1D_3)\n",
    "# model.add(MaxPooling1D(pool_length = 4, stride = 2)) #model.output_shape[1]\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "model.add(Dense(100, activation='sigmoid')) # sigmoid, relu\n",
    "#model.add(Dropout(0.2))\n",
    "model.add(Dense(len(classes_ls), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try binary_crossentropy,categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(40):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    \n",
    "    #model.fit(X_train_ary, y_train_ary, validation_data=(X_train_ary, y_train_ary), nb_epoch=5, batch_size=64)\n",
    "    model.fit(train_texts_vec_mtx \n",
    "              ,y_train_ary \n",
    "              ,validation_data=(test_texts_vec_mtx, y_test_ary)\n",
    "              ,nb_epoch=1\n",
    "              ,batch_size=batch_size_\n",
    "             )\n",
    "    \n",
    "    scores = model.evaluate(test_texts_vec_mtx, y_test_ary, verbose=0)\n",
    "    if(best_model_aux['Best Score'] < scores[1]):\n",
    "        best_model_aux['Best Score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "dense_16 (Dense)                 (None, 100)           10100       dense_input_2[0][0]              \n",
      "____________________________________________________________________________________________________\n",
      "dense_17 (Dense)                 (None, 100)           10100       dense_16[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "dense_18 (Dense)                 (None, 404)           40804       dense_17[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 61,004\n",
      "Trainable params: 61,004\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch iter #1\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 53s - loss: 4.9849 - acc: 0.0556 - val_loss: 4.7801 - val_acc: 0.0772\n",
      "Captured improved model\n",
      "\n",
      "Epoch iter #2\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "661824/661824 [==============================] - 56s - loss: 4.6911 - acc: 0.0922 - val_loss: 4.6363 - val_acc: 0.1014\n",
      "Captured improved model\n",
      "\n",
      "Epoch iter #3\n",
      "Train on 661824 samples, validate on 283640 samples\n",
      "Epoch 1/1\n",
      "473664/661824 [====================>.........] - ETA: 16s - loss: 4.5887 - acc: 0.1067"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-38-026633602d7d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     38\u001b[0m               \u001b[0;34m,\u001b[0m\u001b[0mvalidation_data\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest_texts_vec_mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test_ary\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     39\u001b[0m               \u001b[0;34m,\u001b[0m\u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 40\u001b[0;31m               \u001b[0;34m,\u001b[0m\u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size_\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     41\u001b[0m              )\n\u001b[1;32m     42\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1194\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/backend/tensorflow_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m   1941\u001b[0m         \u001b[0msession\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_session\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1942\u001b[0m         updated = session.run(self.outputs + [self.updates_op],\n\u001b[0;32m-> 1943\u001b[0;31m                               feed_dict=feed_dict)\n\u001b[0m\u001b[1;32m   1944\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mupdated\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0moutputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1945\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36mrun\u001b[0;34m(self, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    765\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    766\u001b[0m       result = self._run(None, fetches, feed_dict, options_ptr,\n\u001b[0;32m--> 767\u001b[0;31m                          run_metadata_ptr)\n\u001b[0m\u001b[1;32m    768\u001b[0m       \u001b[0;32mif\u001b[0m \u001b[0mrun_metadata\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    769\u001b[0m         \u001b[0mproto_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtf_session\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mTF_GetBuffer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mrun_metadata_ptr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/tensorflow/python/client/session.py\u001b[0m in \u001b[0;36m_run\u001b[0;34m(self, handle, fetches, feed_dict, options, run_metadata)\u001b[0m\n\u001b[1;32m    936\u001b[0m                 ' to a larger type (e.g. int64).')\n\u001b[1;32m    937\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 938\u001b[0;31m           \u001b[0mnp_val\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0masarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msubfeed_val\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msubfeed_dtype\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    940\u001b[0m           \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0msubfeed_t\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_shape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mis_compatible_with\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp_val\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/numpy/core/numeric.py\u001b[0m in \u001b[0;36masarray\u001b[0;34m(a, dtype, order)\u001b[0m\n\u001b[1;32m    480\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    481\u001b[0m     \"\"\"\n\u001b[0;32m--> 482\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mcopy\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mFalse\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0morder\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    483\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    484\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0masanyarray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0ma\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdtype\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0morder\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAIN MODEL 1\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "hidden_nodes = 100\n",
    "\n",
    "model = Sequential()\n",
    "embedding_layer = Embedding(input_dim=top_words, \n",
    "                            output_dim=embedding_vecor_length, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length = max_description_length, # e.g. time_steps\n",
    "                            #batch_input_shape=(batch_size_, max_description_length), # use batch_input_shape when stateful=True\n",
    "                            trainable=True)\n",
    "#model.add(embedding_layer)\n",
    "LSTM_1 = LSTM(hidden_nodes\n",
    "               ,input_shape=(max_description_length, 1)\n",
    "               #,batch_input_shape=(batch_size_,max_description_length,embedding_vecor_length)\n",
    "               #,inner_activation='sigmoid'\n",
    "               ,return_sequences=True\n",
    "               ,stateful=False)\n",
    "#model.add(LSTM_1)\n",
    "model.add(Dense(100, input_shape = (max_description_length,), activation='sigmoid'))\n",
    "model.add(Dense(100))\n",
    "#model.add(LSTM(hidden_nodes, return_sequences=False))\n",
    "model.add(Dense(len(classes_ls), activation='softmax'))  # try activation='sigmoid' or 'softmax'\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try binary_crossentropy,categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(10):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    \n",
    "    #model.fit(X_train_ary, y_train_ary, validation_data=(X_train_ary, y_train_ary), nb_epoch=5, batch_size=64)\n",
    "    model.fit(train_texts_vec_mtx \n",
    "              ,y_train_ary \n",
    "              ,validation_data=(test_texts_vec_mtx, y_test_ary)\n",
    "              ,nb_epoch=1\n",
    "              ,batch_size=batch_size_\n",
    "             )\n",
    "    \n",
    "    scores = model.evaluate(test_texts_vec_mtx, y_test_ary, verbose=0)\n",
    "    if(best_model_aux['Best Score'] < scores[1]):\n",
    "        best_model_aux['Best Score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.0074954167254265968"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_aux['Best Score']\n",
    "#model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"264pt\" viewBox=\"0.00 0.00 205.57 264.00\" width=\"206pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 260)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-260 201.5693,-260 201.5693,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 4783655232 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>4783655232</title>\n",
       "<polygon fill=\"none\" points=\"0,-219.5 0,-255.5 197.5693,-255.5 197.5693,-219.5 0,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"98.7847\" y=\"-233.3\">embedding_input_1: InputLayer</text>\n",
       "</g>\n",
       "<!-- 4403675264 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>4403675264</title>\n",
       "<polygon fill=\"none\" points=\"16.7139,-146.5 16.7139,-182.5 180.8555,-182.5 180.8555,-146.5 16.7139,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"98.7847\" y=\"-160.3\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 4783655232&#45;&gt;4403675264 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>4783655232-&gt;4403675264</title>\n",
       "<path d=\"M98.7847,-219.4551C98.7847,-211.3828 98.7847,-201.6764 98.7847,-192.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"102.2848,-192.5903 98.7847,-182.5904 95.2848,-192.5904 102.2848,-192.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 9959960928 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>9959960928</title>\n",
       "<polygon fill=\"none\" points=\"49.3623,-73.5 49.3623,-109.5 148.207,-109.5 148.207,-73.5 49.3623,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"98.7847\" y=\"-87.3\">lstm_1: LSTM</text>\n",
       "</g>\n",
       "<!-- 4403675264&#45;&gt;9959960928 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>4403675264-&gt;9959960928</title>\n",
       "<path d=\"M98.7847,-146.4551C98.7847,-138.3828 98.7847,-128.6764 98.7847,-119.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"102.2848,-119.5903 98.7847,-109.5904 95.2848,-119.5904 102.2848,-119.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 10258445088 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>10258445088</title>\n",
       "<polygon fill=\"none\" points=\"46.6587,-.5 46.6587,-36.5 150.9106,-36.5 150.9106,-.5 46.6587,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"98.7847\" y=\"-14.3\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 9959960928&#45;&gt;10258445088 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>9959960928-&gt;10258445088</title>\n",
       "<path d=\"M98.7847,-73.4551C98.7847,-65.3828 98.7847,-55.6764 98.7847,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"102.2848,-46.5903 98.7847,-36.5904 95.2848,-46.5904 102.2848,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Plot Nets design\n",
    "#plot(model, to_file='/Users/altay.amanbay/Desktop/model.png')\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save model and aux file\n",
    "\n",
    "best_model.save('category_927_char_nets__traindata5000_vecRand32_model.h5')\n",
    "\n",
    "best_model_aux_name = 'category_' + str(best_model_aux['Category ID']) + '_char_nets__traindata5000_vecRand32_aux.pkl'\n",
    "with open(best_model_aux_name, 'wb') as pickle_file:\n",
    "    pickle.dump(best_model_aux, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy on train set: 0.74%\n",
      "Accuracy on test set: 0.75%\n",
      "\n",
      "Evaluation took 898.337 s\n"
     ]
    }
   ],
   "source": [
    "# Final evaluation of the model\n",
    "start = time.time()\n",
    "\n",
    "scores = model.evaluate(train_texts_vec_mtx, y_train_ary, verbose=0)\n",
    "print(\"Accuracy on train set: %.2f%%\" % (scores[1]*100))\n",
    "scores = model.evaluate(test_texts_vec_mtx, y_test_ary, verbose=0)\n",
    "print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "print(\"\\nEvaluation took %g s\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "283616/283640 [============================>.] - ETA: 0spredictions[0]       : 395\n",
      "predictions_probs[0] : [  1.57149884e-06   5.65032766e-04   1.04814411e-04   5.06013026e-03\n",
      "   5.31121530e-03   4.95075528e-03   1.98241978e-05   7.38064784e-07\n",
      "   1.62571894e-06   1.47916691e-03   5.60404314e-03   2.19421601e-03\n",
      "   4.82816860e-04   1.95416564e-04   2.55046121e-04   5.14175044e-03\n",
      "   5.52940881e-03   5.39144361e-03   5.11699403e-03   5.67135401e-03\n",
      "   5.35661727e-03   4.90648160e-03   5.15679037e-03   5.24536800e-03\n",
      "   4.55971481e-03   4.68741730e-03   5.20593440e-03   4.98983636e-03\n",
      "   3.76462145e-03   8.63722448e-07   5.17276628e-03   5.28980745e-03\n",
      "   5.56021743e-03   5.44886524e-03   5.58351073e-03   5.23483707e-03\n",
      "   5.26121212e-03   5.10279415e-03   5.32342913e-03   4.61317226e-03\n",
      "   3.19392816e-03   5.70044620e-03   5.20968484e-03   5.22209983e-03\n",
      "   5.17882500e-03   5.29204309e-03   5.36386063e-03   5.31700253e-03\n",
      "   4.97335242e-03   5.22622885e-03   5.18467510e-03   5.03673637e-03\n",
      "   4.97609796e-03   2.03399162e-04   8.81238884e-05   5.32159256e-03\n",
      "   1.26968801e-03   4.93800035e-03   2.54739006e-03   1.04936538e-03\n",
      "   1.53076288e-03   2.63673835e-03   5.22650953e-04   9.02153602e-07\n",
      "   1.79942625e-04   2.05550552e-03   2.53480370e-03   3.21197149e-04\n",
      "   1.30026636e-03   3.12778889e-03   3.60686594e-04   6.69357309e-04\n",
      "   5.12563216e-04   1.58096279e-03   1.99471973e-03   1.43026677e-03\n",
      "   8.49863402e-10   1.23601767e-05   5.24004828e-03   2.03787163e-03\n",
      "   3.58979800e-04   1.03601173e-03   3.29334966e-06   1.02984870e-06\n",
      "   3.05071389e-05   3.58862686e-03   2.53843154e-07   5.54670766e-03\n",
      "   2.48480592e-05   4.40554344e-04   1.11129464e-06   4.29514237e-03\n",
      "   1.39156822e-04   8.54725679e-10   1.11591976e-06   1.61883436e-04\n",
      "   5.97699312e-04   7.70272800e-06   5.29958960e-03   1.07328315e-05\n",
      "   3.31321295e-04   1.10608414e-06   6.30634486e-06   4.81412094e-03\n",
      "   1.46749517e-06   1.42930162e-06   1.01189448e-06   4.13096905e-06\n",
      "   3.41245641e-06   5.56546915e-03   2.10302619e-06   4.97063203e-03\n",
      "   3.83993145e-03   4.41399425e-06   1.88962833e-04   5.33281965e-03\n",
      "   8.47360238e-10   3.78922523e-05   3.01207419e-05   4.79439646e-03\n",
      "   9.66628886e-06   3.00545100e-04   5.04076807e-03   2.22393940e-03\n",
      "   5.30036259e-03   1.34881893e-05   4.69941506e-03   1.21159201e-05\n",
      "   1.39730548e-06   4.89729596e-03   2.01353244e-03   1.05717417e-03\n",
      "   4.98682074e-03   6.77757082e-04   1.53141527e-03   1.92947383e-03\n",
      "   1.73668761e-03   4.22650203e-03   4.90876706e-03   4.42184682e-04\n",
      "   1.27667809e-05   4.90208063e-03   5.17208036e-03   5.24997106e-03\n",
      "   1.27549458e-04   4.93979175e-03   2.56165001e-03   6.82412705e-04\n",
      "   2.78718118e-03   5.16738370e-03   4.86660376e-03   3.42682069e-05\n",
      "   9.90591245e-04   4.99256654e-03   2.49133399e-03   4.50566331e-05\n",
      "   3.96967912e-03   5.15118428e-03   2.28544907e-03   2.83726531e-05\n",
      "   3.50425765e-03   3.33136973e-06   5.25510695e-05   2.77468439e-06\n",
      "   4.06213896e-03   7.27102952e-06   1.13918555e-04   5.06633613e-03\n",
      "   5.58608677e-03   4.96068038e-03   3.28622627e-05   5.60347363e-03\n",
      "   5.26221981e-03   1.62833254e-03   4.23381600e-04   1.15965500e-04\n",
      "   1.78673683e-04   2.42430414e-03   8.88530340e-04   4.74442990e-04\n",
      "   5.59573709e-05   1.16100756e-03   1.64614711e-03   5.66094229e-03\n",
      "   1.73502252e-03   8.67414252e-10   5.44413831e-03   5.16601047e-03\n",
      "   5.71004953e-03   4.68659867e-03   8.65867012e-04   4.99400729e-03\n",
      "   1.67544364e-04   3.79463006e-03   2.86668213e-03   5.22431917e-03\n",
      "   5.06570889e-03   5.12898760e-03   5.45152789e-03   5.58608677e-03\n",
      "   5.35735441e-03   5.07817883e-03   5.59527660e-03   5.46431122e-03\n",
      "   5.11293299e-03   8.48636439e-10   4.90238750e-03   8.48559193e-06\n",
      "   5.03815664e-03   1.81835261e-04   1.28057638e-06   3.51693529e-06\n",
      "   5.00133587e-03   5.24018606e-06   9.41561848e-06   5.13571198e-04\n",
      "   4.81000636e-03   2.34627092e-04   1.82829925e-03   2.72766245e-03\n",
      "   4.54439258e-04   7.44379242e-04   1.48993940e-03   3.13458592e-03\n",
      "   3.23801208e-03   2.70620524e-03   5.04229730e-03   4.00760770e-03\n",
      "   4.62216180e-04   4.44269879e-03   5.30288508e-03   4.02273115e-04\n",
      "   4.08765674e-03   3.41896061e-03   8.84582903e-07   2.45921547e-03\n",
      "   4.25232481e-03   2.75354087e-03   1.25487757e-04   1.18258013e-03\n",
      "   2.70339897e-05   1.55204951e-04   2.00283923e-03   2.63663172e-03\n",
      "   3.46487900e-03   4.15330396e-05   8.93307174e-07   1.17588525e-06\n",
      "   9.55545511e-06   1.08144411e-06   6.46770932e-05   6.06858839e-06\n",
      "   4.59952062e-05   3.62357787e-06   1.87403584e-05   4.39381802e-05\n",
      "   4.72387904e-03   1.18720175e-06   1.86837008e-06   1.12865598e-06\n",
      "   5.57096384e-04   5.60001237e-03   7.57641283e-06   5.17220516e-03\n",
      "   4.90948698e-03   5.32566663e-03   2.72764487e-06   4.31717141e-03\n",
      "   2.23759958e-03   2.63273942e-05   3.05754980e-07   1.20912762e-06\n",
      "   5.38681867e-03   2.46024183e-06   5.65695670e-03   1.52328982e-06\n",
      "   4.64140717e-03   9.60769057e-07   5.15675079e-03   3.35237687e-03\n",
      "   2.97406595e-03   2.93267122e-03   9.97626921e-04   2.63523962e-03\n",
      "   1.05950970e-03   1.65591843e-03   1.27794081e-03   8.76902603e-04\n",
      "   6.64064864e-05   2.20402580e-04   3.50753893e-03   5.06924046e-03\n",
      "   4.04810862e-07   5.93305333e-04   5.26103331e-03   3.53278847e-06\n",
      "   3.85735211e-06   8.35768788e-10   4.02404461e-03   3.83963197e-04\n",
      "   4.79863607e-04   8.43336980e-04   4.88624489e-03   1.14515307e-03\n",
      "   5.16673829e-03   2.29286801e-04   1.83230522e-03   1.08022790e-03\n",
      "   5.89878880e-04   4.70185978e-03   1.06494909e-03   5.19314781e-03\n",
      "   1.88013457e-03   4.78568207e-03   5.23134973e-03   1.21126163e-06\n",
      "   2.62273964e-03   5.13461977e-03   5.19370334e-03   3.30856116e-03\n",
      "   5.19461278e-03   3.26014265e-06   2.09067226e-03   2.06420707e-04\n",
      "   8.41245684e-10   4.65386373e-04   2.79047573e-03   5.58544509e-03\n",
      "   4.65794653e-03   5.27781993e-03   5.25046373e-03   1.81183068e-03\n",
      "   1.64548855e-03   5.09595685e-03   5.69045404e-03   6.77064992e-04\n",
      "   3.09191455e-05   2.56046660e-05   7.96419685e-04   4.08224482e-03\n",
      "   4.77608433e-03   4.90265759e-03   8.06057797e-05   5.41555602e-03\n",
      "   5.51876007e-03   5.19289495e-03   6.66908280e-04   9.87334526e-04\n",
      "   5.48520079e-03   5.33906929e-03   5.51634142e-03   1.37191173e-05\n",
      "   2.49801633e-06   4.94219642e-03   1.46344780e-06   1.04561658e-03\n",
      "   3.41806344e-06   4.83710831e-03   4.56274580e-03   4.95833391e-03\n",
      "   5.05302660e-03   4.79600113e-03   3.68626695e-03   5.50176948e-03\n",
      "   3.24582561e-06   8.67508565e-10   1.84509429e-06   2.79474571e-05\n",
      "   2.93840167e-05   5.41721657e-03   1.54110035e-06   2.25181748e-06\n",
      "   8.71328454e-10   3.05950851e-03   8.49007975e-10   5.51938456e-07\n",
      "   2.22299178e-03   3.30909120e-06   4.91204485e-03   9.60030593e-04\n",
      "   2.48571346e-03   1.65285461e-03   5.07025467e-03   4.94529819e-03\n",
      "   4.43817007e-05   5.21100033e-03   5.34955831e-03   4.56845574e-03\n",
      "   5.27640805e-03   5.21454820e-03   5.34872478e-03   8.70303662e-10\n",
      "   4.84800618e-03   2.02770834e-03   3.43895657e-03   7.41547393e-03\n",
      "   4.61492734e-03   1.31989084e-03   1.88561160e-06   1.59969045e-06\n",
      "   1.14577733e-06   1.63324023e-06   3.16959313e-06   1.17631105e-04]\n",
      "\n",
      "Prediction took 579.285 s\n"
     ]
    }
   ],
   "source": [
    "# Get predictions\n",
    "start = time.time()\n",
    "\n",
    "predictions = model.predict_classes(test_texts_vec_mtx)\n",
    "#predictions_rnd = np.round_(predictions, decimals=0, out=None)\n",
    "predictions_probs = model.predict(test_texts_vec_mtx)\n",
    "\n",
    "print('%-20s' % \"predictions[0]\",':', predictions[0])\n",
    "#print('%-20s' % \"predictions_rnd[0]:\",':',predictions_rnd[0])\n",
    "print('%-20s' % \"predictions_probs[0]\",':', predictions_probs[0])\n",
    "print(\"\\nPrediction took %g s\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>395</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>160</td>\n",
       "      <td>160</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>32</td>\n",
       "      <td>32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1513</td>\n",
       "      <td>1513</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1508</td>\n",
       "      <td>1508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>1520</td>\n",
       "      <td>1520</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>6</td>\n",
       "      <td>6</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>350</td>\n",
       "      <td>350</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1465</td>\n",
       "      <td>1465</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>586</td>\n",
       "      <td>586</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>115</td>\n",
       "      <td>115</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>48</td>\n",
       "      <td>48</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>71</td>\n",
       "      <td>71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>1505</td>\n",
       "      <td>1505</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>1552</td>\n",
       "      <td>1552</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>1502</td>\n",
       "      <td>1502</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>1499</td>\n",
       "      <td>1499</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>1462</td>\n",
       "      <td>1462</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>1494</td>\n",
       "      <td>1494</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>1480</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>1532</td>\n",
       "      <td>1532</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>1474</td>\n",
       "      <td>1474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>1545</td>\n",
       "      <td>1545</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>1378</td>\n",
       "      <td>1378</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>1498</td>\n",
       "      <td>1498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>1528</td>\n",
       "      <td>1528</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>1052</td>\n",
       "      <td>1052</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>1482</td>\n",
       "      <td>1482</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>1484</td>\n",
       "      <td>1484</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>1461</td>\n",
       "      <td>1461</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>367</th>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>368</th>\n",
       "      <td>8</td>\n",
       "      <td>8</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>369</th>\n",
       "      <td>1474</td>\n",
       "      <td>1474</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>372</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>373</th>\n",
       "      <td>850</td>\n",
       "      <td>850</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>374</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>376</th>\n",
       "      <td>613</td>\n",
       "      <td>613</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>378</th>\n",
       "      <td>1553</td>\n",
       "      <td>1553</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>379</th>\n",
       "      <td>278</td>\n",
       "      <td>278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>380</th>\n",
       "      <td>635</td>\n",
       "      <td>635</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>381</th>\n",
       "      <td>441</td>\n",
       "      <td>441</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>382</th>\n",
       "      <td>1507</td>\n",
       "      <td>1507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>383</th>\n",
       "      <td>1250</td>\n",
       "      <td>1250</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>384</th>\n",
       "      <td>5</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>1330</td>\n",
       "      <td>1330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>386</th>\n",
       "      <td>1549</td>\n",
       "      <td>1549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>387</th>\n",
       "      <td>1493</td>\n",
       "      <td>1493</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>388</th>\n",
       "      <td>1498</td>\n",
       "      <td>1498</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>389</th>\n",
       "      <td>1534</td>\n",
       "      <td>1534</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>390</th>\n",
       "      <td>1464</td>\n",
       "      <td>1464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>391</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>392</th>\n",
       "      <td>1554</td>\n",
       "      <td>1554</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>393</th>\n",
       "      <td>514</td>\n",
       "      <td>514</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>394</th>\n",
       "      <td>874</td>\n",
       "      <td>874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>395</th>\n",
       "      <td>2126</td>\n",
       "      <td>2126</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>396</th>\n",
       "      <td>1292</td>\n",
       "      <td>1292</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>397</th>\n",
       "      <td>433</td>\n",
       "      <td>433</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>401</th>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>403</th>\n",
       "      <td>27</td>\n",
       "      <td>27</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>283640</td>\n",
       "      <td>283640</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>355 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted     395     All\n",
       "True                     \n",
       "1             160     160\n",
       "2              32      32\n",
       "3            1513    1513\n",
       "4            1508    1508\n",
       "5            1520    1520\n",
       "6               6       6\n",
       "9             350     350\n",
       "10           1465    1465\n",
       "11            586     586\n",
       "12            115     115\n",
       "13             48      48\n",
       "14             71      71\n",
       "15           1505    1505\n",
       "16           1552    1552\n",
       "17           1502    1502\n",
       "18           1499    1499\n",
       "19           1462    1462\n",
       "20           1494    1494\n",
       "21           1480    1480\n",
       "22           1532    1532\n",
       "23           1474    1474\n",
       "24           1545    1545\n",
       "25           1378    1378\n",
       "26           1498    1498\n",
       "27           1528    1528\n",
       "28           1052    1052\n",
       "30           1484    1484\n",
       "31           1482    1482\n",
       "32           1484    1484\n",
       "33           1461    1461\n",
       "...           ...     ...\n",
       "367             7       7\n",
       "368             8       8\n",
       "369          1474    1474\n",
       "372             1       1\n",
       "373           850     850\n",
       "374             1       1\n",
       "376           613     613\n",
       "378          1553    1553\n",
       "379           278     278\n",
       "380           635     635\n",
       "381           441     441\n",
       "382          1507    1507\n",
       "383          1250    1250\n",
       "384             5       5\n",
       "385          1330    1330\n",
       "386          1549    1549\n",
       "387          1493    1493\n",
       "388          1498    1498\n",
       "389          1534    1534\n",
       "390          1464    1464\n",
       "391             1       1\n",
       "392          1554    1554\n",
       "393           514     514\n",
       "394           874     874\n",
       "395          2126    2126\n",
       "396          1292    1292\n",
       "397           433     433\n",
       "401             1       1\n",
       "403            27      27\n",
       "All        283640  283640\n",
       "\n",
       "[355 rows x 2 columns]"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#pd.crosstab(pd.Series(y_test_ary.ravel()), pd.Series(predictions_rnd.ravel()), rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "pd.crosstab(pd.Series(np_utils.categorical_probas_to_classes(y_test_ary)), pd.Series(predictions), rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th>Predicted</th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>All</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>True</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>285926</td>\n",
       "      <td>81</td>\n",
       "      <td>286007</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>163</td>\n",
       "      <td>1410</td>\n",
       "      <td>1573</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>All</th>\n",
       "      <td>286089</td>\n",
       "      <td>1491</td>\n",
       "      <td>287580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "Predicted       0     1     All\n",
       "True                           \n",
       "0          285926    81  286007\n",
       "1             163  1410    1573\n",
       "All        286089  1491  287580"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "pd.crosstab(pd.Series(np_utils.categorical_probas_to_classes(y_test_ary)), pd.Series(predictions.ravel()), rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model.get_config()\n",
    "def prediction_to_str(clf_prediction, category_id):\n",
    "    if(clf_prediction > 0.5):\n",
    "        return str(category_id)\n",
    "    else:\n",
    "        return 'not ' + str(category_id)\n",
    "\n",
    "def predict(description_str, word_index_, clf_, max_length_, category_id_):\n",
    "    #seq_ = tok_.texts_to_sequences([description_str])\n",
    "    seq_ = texts_to_sequences_custom([description_str.lower()], word_index_)\n",
    "    seq_pad = sequence.pad_sequences(seq_, maxlen = max_length_)\n",
    "    clf_prediction = clf_.predict(seq_pad)\n",
    "    \n",
    "    #print(seq_)\n",
    "    #print(seq_pad)\n",
    "    \n",
    "    # Prediction to string\n",
    "    clf_prediction_str = prediction_to_str(clf_prediction[0][0], category_id_)\n",
    "    \n",
    "    return clf_prediction_str\n",
    "    #return clf_prediction[0][0]\n",
    "\n",
    "def predict_2(description_str, word_index_, clf_, max_length_, category_id_):\n",
    "    #seq_ = tok_.texts_to_sequences([description_str])\n",
    "    seq_ = texts_to_sequences_custom([description_str.lower()], word_index_)\n",
    "    seq_pad = sequence.pad_sequences(seq_, maxlen = max_length_)\n",
    "    #clf_prediction = clf_.predict(seq_pad)\n",
    "    clf_prediction = clf_.predict_classes(seq_pad)\n",
    "    \n",
    "    #print(seq_)\n",
    "    #print(seq_pad)\n",
    "    \n",
    "    # Prediction to string\n",
    "    #clf_prediction_str = prediction_to_str(clf_prediction[0][0], category_id_)\n",
    "    clf_prediction = le.inverse_transform(clf_prediction)\n",
    "    \n",
    "    if(clf_prediction == ['Positive']):\n",
    "        return str(category_id_)\n",
    "    else:\n",
    "        return 'not ' + str(category_id_)\n",
    "    \n",
    "    \n",
    "def predict_proba(description_str, tok_, clf_, max_length_):\n",
    "    #seq_ = tok_.texts_to_sequences([description_str])\n",
    "    seq_ = texts_to_sequences_custom([description_str], tok_.word_index)\n",
    "    seq_pad = sequence.pad_sequences(seq_, maxlen = max_length_)\n",
    "    clf_prediction_proba = clf_.predict_proba(seq_pad, verbose=0)\n",
    "    \n",
    "    return clf_prediction_proba[0][0]\n",
    "\n",
    "\n",
    "# id_ = 'table Setr'\n",
    "# p = predict(id_, best_model_aux['Tokenizer'], best_model, best_model_aux['Max length'], best_model_aux['Category ID'])\n",
    "# pp = predict_proba(id_, best_model_aux['Tokenizer'], best_model, best_model_aux['Max length'])\n",
    "# print(p)\n",
    "# print(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0 ====================================================================================================\n",
      "Old model prediction:\n",
      "item: tekton 2655 flare nut wrench set metric 6piece\n",
      "Seq max len: 30\n",
      "not 927\n",
      "4.17323e-08\n",
      "\n",
      "Fresh model prediction:\n",
      "item: tekton 2655 flare nut wrench set metric 6piece\n",
      "Seq max len: 100\n",
      "1/1 [==============================] - 0s\n",
      "not 927\n",
      "\n",
      "1 ====================================================================================================\n",
      "Old model prediction:\n",
      "item: tekton 2780 10slot screwdriver holder and organizer\n",
      "Seq max len: 30\n",
      "not 927\n",
      "0.121584\n",
      "\n",
      "Fresh model prediction:\n",
      "item: tekton 2780 10slot screwdriver holder and organizer\n",
      "Seq max len: 100\n",
      "1/1 [==============================] - 0s\n",
      "not 927\n",
      "\n",
      "2 ====================================================================================================\n",
      "Old model prediction:\n",
      "item: titan 17237 insulated electrical screwdriver set  7 piece\n",
      "Seq max len: 30\n",
      "not 927\n",
      "0.998989\n",
      "\n",
      "Fresh model prediction:\n",
      "item: titan 17237 insulated electrical screwdriver set  7 piece\n",
      "Seq max len: 100\n",
      "1/1 [==============================] - 0s\n",
      "927\n",
      "\n",
      "3 ====================================================================================================\n",
      "Old model prediction:\n",
      "item: tool sorter screwdriver organizer red\n",
      "Seq max len: 30\n",
      "not 927\n",
      "0.405264\n",
      "\n",
      "Fresh model prediction:\n",
      "item: tool sorter screwdriver organizer red\n",
      "Seq max len: 100\n",
      "1/1 [==============================] - 0s\n",
      "927\n",
      "\n",
      "4 ====================================================================================================\n",
      "Old model prediction:\n",
      "item: torin sdh15rt magnetic screwdriver holder\n",
      "Seq max len: 30\n",
      "not 927\n",
      "0.994613\n",
      "\n",
      "Fresh model prediction:\n",
      "item: torin sdh15rt magnetic screwdriver holder\n",
      "Seq max len: 100\n",
      "1/1 [==============================] - 0s\n",
      "927\n",
      "\n",
      "5 ====================================================================================================\n",
      "Old model prediction:\n",
      "item: wera 05020013001 joker combination wrenchset 11 pieces\n",
      "Seq max len: 30\n",
      "not 927\n",
      "0.0210169\n",
      "\n",
      "Fresh model prediction:\n",
      "item: wera 05020013001 joker combination wrenchset 11 pieces\n",
      "Seq max len: 100\n",
      "1/1 [==============================] - 0s\n",
      "not 927\n",
      "\n",
      "6 ====================================================================================================\n",
      "Old model prediction:\n",
      "item: wera kk vde 60i62i68i18 insulated pouch set with interchangeable blades 18piece\n",
      "Seq max len: 30\n",
      "not 927\n",
      "0.995252\n",
      "\n",
      "Fresh model prediction:\n",
      "item: wera kk vde 60i62i68i18 insulated pouch set with interchangeable blades 18piece\n",
      "Seq max len: 100\n",
      "1/1 [==============================] - 0s\n",
      "927\n",
      "\n",
      "7 ====================================================================================================\n",
      "Old model prediction:\n",
      "item: wiha 28103 magnetic 14 bit holder stubby 57mm pliers screwdriver\n",
      "Seq max len: 30\n",
      "not 927\n",
      "0.972863\n",
      "\n",
      "Fresh model prediction:\n",
      "item: wiha 28103 magnetic 14 bit holder stubby 57mm pliers screwdriver\n",
      "Seq max len: 100\n",
      "1/1 [==============================] - 0s\n",
      "927\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "\n",
    "## load old model\n",
    "model_file = \"category_927_nets__traindata5000_vectrain5000_model.h5\"\n",
    "aux_file = \"category_927_nets__traindata5000_vectrain5000_aux.pkl\"\n",
    "old_best_model_ = load_model(model_file)\n",
    "old_best_model_aux_ = get_model_file_aux(aux_file)\n",
    "old_tok_ = old_best_model_aux_['Tokenizer']\n",
    "old_word_index_ = old_best_model_aux_['Tokenizer'].word_index\n",
    "\n",
    "## use fresh model\n",
    "best_model_ = best_model\n",
    "best_model_aux_ = best_model_aux\n",
    "tok_ = tok\n",
    "word_index_ = word_index\n",
    "\n",
    "item_d = 'NieR: Automata™ DEMO 120161128 (Playable Demo)'\n",
    "\n",
    "# screwdrivers check\n",
    "scrw_items = [\n",
    "\"tekton 2655 flare nut wrench set metric 6piece\"\n",
    ",\"tekton 2780 10slot screwdriver holder and organizer\"\n",
    ",\"titan 17237 insulated electrical screwdriver set  7 piece\"\n",
    ",\"tool sorter screwdriver organizer red\"\n",
    ",\"torin sdh15rt magnetic screwdriver holder\"  #wrong predict\n",
    ",\"wera 05020013001 joker combination wrenchset 11 pieces\"\n",
    ",\"wera kk vde 60i62i68i18 insulated pouch set with interchangeable blades 18piece\" # tricky\n",
    ",\"wiha 28103 magnetic 14 bit holder stubby 57mm pliers screwdriver\" # tricky, wrong predict\n",
    "]\n",
    "\n",
    "for n, i in enumerate(scrw_items):\n",
    "    item_d = i\n",
    "    \n",
    "    print(str(n) + ' ' + '='*100)\n",
    "    \n",
    "    print('Old model prediction:')\n",
    "    print('item:',item_d)\n",
    "    print('Seq max len:', old_best_model_aux_['Max length'])\n",
    "    print(predict(modif_1(item_d), old_tok_, old_best_model_, old_best_model_aux_['Max length'], '927'))\n",
    "    print(predict_proba(item_d, word_index, old_best_model_, old_best_model_aux_['Max length']))\n",
    "\n",
    "\n",
    "    print('\\nFresh model prediction:')\n",
    "    print('item:',item_d)\n",
    "    print('Seq max len:', best_model_aux_['Max length'])\n",
    "    print(predict_2(modif_1(item_d), word_index, best_model_, best_model_aux_['Max length'], '927'))\n",
    "    #print(predict_proba(item_d, tok_, best_model_, best_model_aux_['Max length']))\n",
    "\n",
    "    print()\n",
    "\n",
    "    #tt = train_df.loc[0:10,['description_mod1']]\n",
    "    #tt['pred'] = tt['description_mod1'].apply(lambda x: predict(x, best_model_aux_['Tokenizer'], best_model, best_model_aux_['Max length'], best_model_aux_['Category ID']))\n",
    "    #tt['prob'] = tt['description_mod1'].apply(lambda x: predict_proba(x, best_model_aux_['Tokenizer'], best_model, best_model_aux_['Max length']))"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
