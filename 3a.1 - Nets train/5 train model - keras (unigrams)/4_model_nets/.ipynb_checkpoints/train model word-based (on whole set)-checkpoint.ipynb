{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keras \n",
    "# Classifier: LSTM \n",
    "# Classification type: multi-class (404 classes)\n",
    "# Output nodes: #of classes with softmax\n",
    "\n",
    "# word2vec model: \n",
    "# word2vect_class_specififc__vec64_win1__dict_sample_5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using Theano backend.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing process_string.py \n",
      "from /Users/altay.amanbay/Desktop/new node booster/experiments/3a.1 - Nets train/5 train model - keras (unigrams)/2_common_aux_script ...\n",
      "\n",
      "3.5.2 |Anaconda 4.2.0 (x86_64)| (default, Jul  2 2016, 17:52:12) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from keras.datasets import reuters\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Dropout, Activation, LSTM, Bidirectional, Flatten, Lambda\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer, one_hot, text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializations\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "from keras.utils.visualize_util import plot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os    \n",
    "os.environ['THEANO_FLAGS'] = \"device=gpu0\" \n",
    "import theano\n",
    "#theano.config.device = 'gpu0'\n",
    "theano.config.floatX = 'float32'\n",
    "\n",
    "\n",
    "# import custom code\n",
    "import os\n",
    "import sys\n",
    "pardir = os.path.abspath(os.path.join(os.getcwd(), '../'))\n",
    "script_path = pardir + \"/2_common_aux_script\"\n",
    "print('Importing process_string.py \\nfrom ' + script_path + \" ...\\n\")\n",
    "sys.path.append(script_path)\n",
    "from process_string import process_string\n",
    "sys.path.remove(script_path)\n",
    "\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.2\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(theano.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def NGramGenerator_wordwise_interval(phrase, min_ngram, max_ngram):\n",
    "    all_ngram_lists = []\n",
    "\n",
    "    printable_ = 'abcdefghijklmnopqrstuvwxyz0123456789 '\n",
    "    s_split = \"\".join((char if char in printable_ else \"\") for char in phrase).split()\n",
    "    #phrase_processed = process_string(phrase)\n",
    "    #s_split = phrase_processed.split()\n",
    "    \n",
    "    for n in range(max_ngram, min_ngram - 1, -1):\n",
    "        n_gram = [s_split[i:i+n] for i in range(len(s_split)-n+1)]\n",
    "        all_ngram_lists.extend(n_gram)\n",
    "        \n",
    "    all_ngrams = []\n",
    "    for n_gram in all_ngram_lists:\n",
    "        all_ngrams.extend([' '.join(n_gram)])\n",
    "    \n",
    "    return all_ngrams\n",
    "\n",
    "def get_word2index(texts_ls_):\n",
    "    word2index_ = {}\n",
    "\n",
    "    c = 1\n",
    "    for text_str in texts_ls_:\n",
    "        text_tokens_ls = text_str.lower().split()\n",
    "        for token in text_tokens_ls:\n",
    "            if(token not in word2index_):\n",
    "                word2index_[token] = c\n",
    "                c = c + 1\n",
    "                \n",
    "    return word2index_\n",
    "\n",
    "def train_df_preprocess(top_words_, texts_ls_, max_pad_length_):\n",
    "    # texts_ls_: list of texts strings\n",
    "    \n",
    "    tok = Tokenizer(top_words_)\n",
    "    tok.fit_on_texts(texts_ls_)\n",
    "\n",
    "    words = []\n",
    "    for iter in range(top_words):\n",
    "        words += [key for key,value in tok.word_index.items() if value==iter+1]\n",
    "\n",
    "    #Class for vectorizing texts, or/and turning texts into sequences \n",
    "    #(=list of word indexes, where the word of rank i in the dataset (starting at 1) has index i).\n",
    "    texts_vec_ls = tok.texts_to_sequences(texts_ls_)#turns text to sequence, stating which word comes in what place\n",
    "    texts_vec_mtx = sequence.pad_sequences(texts_vec_ls, maxlen=max_pad_length_)#pad sequence, essentially padding it with 0's at the end\n",
    "    \n",
    "    return texts_vec_mtx\n",
    "\n",
    "def text_2_vec(text_str, word2index_):\n",
    "    # text_str: text string\n",
    "    \n",
    "    text_tokens_ls = text_str.lower().split()\n",
    "    \n",
    "    text_vec = []\n",
    "    for token in text_tokens_ls:\n",
    "        if token in word2index_:\n",
    "            text_vec.append(word2index_[token])\n",
    "        else:\n",
    "            text_vec.append(0)\n",
    "            \n",
    "    return text_vec\n",
    "\n",
    "def train_df_preprocess_2(texts_ls_, word2index_, max_pad_length_):\n",
    "    # texts_ls_: list of texts strings\n",
    "    \n",
    "    texts_vec_ls = []\n",
    "    for text_ in texts_ls_:\n",
    "        #print(text_)\n",
    "        #print(type(text_))\n",
    "        text_vec = text_2_vec(text_, word2index_)\n",
    "        texts_vec_ls.append(text_vec)\n",
    "    \n",
    "    texts_vec_ary = sequence.pad_sequences(texts_vec_ls, maxlen=max_pad_length_)\n",
    "    \n",
    "    return texts_vec_ary\n",
    "\n",
    "def texts_to_sequences_custom(texts_ls, word_index_, new_words_to_zero = False):\n",
    "    texts_seq = []\n",
    "    \n",
    "    for text in texts_ls:\n",
    "        #text_split = text.lower().split()\n",
    "        text_split = NGramGenerator_wordwise_interval(text,1,1)\n",
    "        seq = []\n",
    "        for token in text_split:\n",
    "            if(token in word_index_):\n",
    "                seq.append(word_index_[token])\n",
    "            elif(new_words_to_zero):\n",
    "                seq.append(0)\n",
    "                \n",
    "        texts_seq.append(seq)\n",
    "#         for k,v in word_index_.items():\n",
    "#             if(v == 395):\n",
    "#                 print(k,v)\n",
    "    return texts_seq\n",
    "\n",
    "\n",
    "def get_model_file_aux(model_file_aux_name):\n",
    "    with open(model_file_aux_name, 'rb') as pickle_file:\n",
    "        model_file_aux = pickle.load(pickle_file)\n",
    "    return model_file_aux\n",
    "\n",
    "def clone_tokens(text_str, cloning_factor = 1):\n",
    "    token_clones_ls = []\n",
    "    for token in text_str.split():\n",
    "        token_clones_ls.extend([token]*cloning_factor)\n",
    "    return ' '.join(token_clones_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples data shape: (587983, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_mod1</th>\n",
       "      <th>category_id_mod1</th>\n",
       "      <th>category_full_path_mod1</th>\n",
       "      <th>description_cloned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aveeno baby wash  shampoo lightly scented 8 ou...</td>\n",
       "      <td>206</td>\n",
       "      <td>Baby Products &gt; Bathing &amp; Skin Care &gt; Soaps &amp; ...</td>\n",
       "      <td>aveeno aveeno aveeno aveeno aveeno baby baby b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>earths best chlorine free diapers size 4 120 c...</td>\n",
       "      <td>213</td>\n",
       "      <td>Baby Products &gt; Diapering &gt; Disposable Diapers</td>\n",
       "      <td>earths earths earths earths earths best best b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doa5lr costume catalog lr10 addon content</td>\n",
       "      <td>320</td>\n",
       "      <td>Electronics &amp; Accessories &gt; Video Games &gt; Other</td>\n",
       "      <td>doa5lr doa5lr doa5lr doa5lr doa5lr costume cos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lightweight oxford l in slim and white</td>\n",
       "      <td>152</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Tops &amp; Tees ...</td>\n",
       "      <td>lightweight lightweight lightweight lightweigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>now foods nutritional yeast flakes 10ounce</td>\n",
       "      <td>525</td>\n",
       "      <td>Health &amp; Beauty &gt; Vitamins &amp; Dietary Supplements</td>\n",
       "      <td>now now now now now foods foods foods foods fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    description_mod1 category_id_mod1  \\\n",
       "0  aveeno baby wash  shampoo lightly scented 8 ou...              206   \n",
       "1  earths best chlorine free diapers size 4 120 c...              213   \n",
       "2          doa5lr costume catalog lr10 addon content              320   \n",
       "3             lightweight oxford l in slim and white              152   \n",
       "4         now foods nutritional yeast flakes 10ounce              525   \n",
       "\n",
       "                             category_full_path_mod1  \\\n",
       "0  Baby Products > Bathing & Skin Care > Soaps & ...   \n",
       "1     Baby Products > Diapering > Disposable Diapers   \n",
       "2    Electronics & Accessories > Video Games > Other   \n",
       "3  Apparel & Accessories > Apparel > Tops & Tees ...   \n",
       "4   Health & Beauty > Vitamins & Dietary Supplements   \n",
       "\n",
       "                                  description_cloned  \n",
       "0  aveeno aveeno aveeno aveeno aveeno baby baby b...  \n",
       "1  earths earths earths earths earths best best b...  \n",
       "2  doa5lr doa5lr doa5lr doa5lr doa5lr costume cos...  \n",
       "3  lightweight lightweight lightweight lightweigh...  \n",
       "4  now now now now now foods foods foods foods fo...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read sampled descriptionary\n",
    "\n",
    "path = pardir+'/1_data/'\n",
    "file_name = 'sampled_descriptionary_sample_size_5000.csv'\n",
    "file_name = 'scorecards_for_fasttext.csv'\n",
    "samples_df = pd.read_csv(path + file_name)\n",
    "\n",
    "# Rename columns\n",
    "samples_df.rename(columns={\n",
    "                           #'description': 'description_mod1', \n",
    "                           '\\ufeff\"description\"': 'description_mod1',\n",
    "                           'category_id': 'category_id_mod1',\n",
    "                           'category_path': 'category_full_path_mod1'}, inplace=True)\n",
    "\n",
    "# Drop rows with NaN in any column\n",
    "samples_df.dropna()\n",
    "\n",
    "# Process description_mod1 strings by process_string function\n",
    "samples_df['description_mod1'] = samples_df['description_mod1'].apply(lambda x: process_string(x))\n",
    "\n",
    "# Drop rows where token count less than 1 in description_mod1 column\n",
    "selected_indices = samples_df['description_mod1'].apply(lambda x: len(str(x).split()) > 1)\n",
    "samples_df = samples_df[selected_indices]\n",
    "\n",
    "# Drop duplicates\n",
    "samples_df.drop_duplicates(subset=['description_mod1','category_full_path_mod1'], inplace = True, keep='first')\n",
    "samples_df.drop_duplicates(subset=['description_mod1'], inplace = True, keep=False)\n",
    "samples_df.reset_index(drop=True, inplace=True) \n",
    "\n",
    "# Clone tokens\n",
    "samples_df['description_cloned'] = samples_df['description_mod1'].apply(lambda x: clone_tokens(x, cloning_factor=5))\n",
    "\n",
    "# Drop 'screwdrivers' from descriptionary\n",
    "#samples_df = samples_df.loc[samples_df.category_id_mod1 != 927,:]\n",
    "\n",
    "# Drop index column\n",
    "#samples_df.drop(labels=['index'], axis=1, inplace=True)\n",
    "\n",
    "print('samples data shape:',samples_df.shape)\n",
    "samples_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweak unbiased data (drop classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Photos & Paper Products                                  50400\n",
       "Books                                                    33416\n",
       "Other                                                    29343\n",
       "Health & Beauty > Makeup                                 24738\n",
       "Online Services                                          22120\n",
       "Health & Beauty > Vitamins & Dietary Supplements         20874\n",
       "Jewelry & Watches > Earrings > Women                     18535\n",
       "Apparel & Accessories > Apparel > Tops & Tees > Women    13440\n",
       "Pet Supplies > Dogs & Cats > Dog Food & Treats           11791\n",
       "Grocery & Gourmet Food > Restaurant & Takeout            10333\n",
       "Name: category_full_path_mod1, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check for class count\n",
    "\n",
    "stats_sr = samples_df[\"category_full_path_mod1\"].value_counts()\n",
    "stats_sr[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned samples data shape: (484432, 4)\n"
     ]
    }
   ],
   "source": [
    "# Delete certain categories\n",
    "\n",
    "samples_df = samples_df.loc[samples_df['category_full_path_mod1'].str.contains(\"Other\")==False,:]\n",
    "samples_df = samples_df.loc[samples_df.category_full_path_mod1 != \"Books\",:]\n",
    "#samples_df = samples_df.loc[samples_df.category_full_path_mod1 != \"Photos & Paper Products\",:]\n",
    "\n",
    "class_1 = 'Pet Supplies > Dogs & Cats > Dog Food & Treats'\n",
    "class_1 = 'Photos & Paper Products'\n",
    "class_2 = 'Apparel & Accessories > Apparel > Tops & Tees > Women'\n",
    "class_2 = 'Apparel & Accessories > Accessories > Luggage, Backpacks & Laptop Bags'\n",
    "classes_selected = class_1+'|'+class_2\n",
    "#samples_df = samples_df.loc[samples_df['category_full_path_mod1'].str.contains(classes_selected)==True,:]\n",
    "print('Pruned samples data shape:',samples_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweak unbiased data (equalize class numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_mod1</th>\n",
       "      <th>category_id_mod1</th>\n",
       "      <th>category_full_path_mod1</th>\n",
       "      <th>description_cloned</th>\n",
       "      <th>count</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aveeno baby wash  shampoo lightly scented 8 ou...</td>\n",
       "      <td>206</td>\n",
       "      <td>Baby Products &gt; Bathing &amp; Skin Care &gt; Soaps &amp; ...</td>\n",
       "      <td>aveeno aveeno aveeno aveeno aveeno baby baby b...</td>\n",
       "      <td>297</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>earths best chlorine free diapers size 4 120 c...</td>\n",
       "      <td>213</td>\n",
       "      <td>Baby Products &gt; Diapering &gt; Disposable Diapers</td>\n",
       "      <td>earths earths earths earths earths best best b...</td>\n",
       "      <td>2456</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lightweight oxford l in slim and white</td>\n",
       "      <td>152</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Tops &amp; Tees ...</td>\n",
       "      <td>lightweight lightweight lightweight lightweigh...</td>\n",
       "      <td>13440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>now foods nutritional yeast flakes 10ounce</td>\n",
       "      <td>525</td>\n",
       "      <td>Health &amp; Beauty &gt; Vitamins &amp; Dietary Supplements</td>\n",
       "      <td>now now now now now foods foods foods foods fo...</td>\n",
       "      <td>20874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>navitas naturals organic goji berries 8 ounce ...</td>\n",
       "      <td>407</td>\n",
       "      <td>Grocery &amp; Gourmet Food &gt; Snack Foods &gt; Dried F...</td>\n",
       "      <td>navitas navitas navitas navitas navitas natura...</td>\n",
       "      <td>182</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    description_mod1 category_id_mod1  \\\n",
       "0  aveeno baby wash  shampoo lightly scented 8 ou...              206   \n",
       "1  earths best chlorine free diapers size 4 120 c...              213   \n",
       "3             lightweight oxford l in slim and white              152   \n",
       "4         now foods nutritional yeast flakes 10ounce              525   \n",
       "5  navitas naturals organic goji berries 8 ounce ...              407   \n",
       "\n",
       "                             category_full_path_mod1  \\\n",
       "0  Baby Products > Bathing & Skin Care > Soaps & ...   \n",
       "1     Baby Products > Diapering > Disposable Diapers   \n",
       "3  Apparel & Accessories > Apparel > Tops & Tees ...   \n",
       "4   Health & Beauty > Vitamins & Dietary Supplements   \n",
       "5  Grocery & Gourmet Food > Snack Foods > Dried F...   \n",
       "\n",
       "                                  description_cloned  count  \n",
       "0  aveeno aveeno aveeno aveeno aveeno baby baby b...    297  \n",
       "1  earths earths earths earths earths best best b...   2456  \n",
       "3  lightweight lightweight lightweight lightweigh...  13440  \n",
       "4  now now now now now foods foods foods foods fo...  20874  \n",
       "5  navitas navitas navitas navitas navitas natura...    182  "
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Map class counts\n",
    "\n",
    "samples_df['count'] = samples_df['category_full_path_mod1'].map(stats_sr)\n",
    "samples_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples data shape after picking max N samples from each class: (203321, 5)\n"
     ]
    }
   ],
   "source": [
    "## samples_df is unbiased by classes (category_full_path_mod1)\n",
    "## fix by picking N samples from each class\n",
    "\n",
    "#N=samples_df.category_full_path_mod1.value_counts(normalize=True).iloc[0] * samples_df.shape[0]\n",
    "samples_df = samples_df.sample(frac=1).groupby('category_full_path_mod1', sort=False).head(1000)\n",
    "print('samples data shape after picking max N samples from each class:',samples_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat sample_df into train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_mod1</th>\n",
       "      <th>category_id_mod1</th>\n",
       "      <th>category_full_path_mod1</th>\n",
       "      <th>description_cloned</th>\n",
       "      <th>target_le</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aveeno baby wash  shampoo lightly scented 8 ou...</td>\n",
       "      <td>206</td>\n",
       "      <td>Baby Products &gt; Bathing &amp; Skin Care &gt; Soaps &amp; ...</td>\n",
       "      <td>aveeno aveeno aveeno aveeno aveeno baby baby b...</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>earths best chlorine free diapers size 4 120 c...</td>\n",
       "      <td>213</td>\n",
       "      <td>Baby Products &gt; Diapering &gt; Disposable Diapers</td>\n",
       "      <td>earths earths earths earths earths best best b...</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    description_mod1 category_id_mod1  \\\n",
       "0  aveeno baby wash  shampoo lightly scented 8 ou...              206   \n",
       "1  earths best chlorine free diapers size 4 120 c...              213   \n",
       "\n",
       "                             category_full_path_mod1  \\\n",
       "0  Baby Products > Bathing & Skin Care > Soaps & ...   \n",
       "1     Baby Products > Diapering > Disposable Diapers   \n",
       "\n",
       "                                  description_cloned  target_le  \n",
       "0  aveeno aveeno aveeno aveeno aveeno baby baby b...        169  \n",
       "1  earths earths earths earths earths best best b...        175  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Concat original train set and sampled descriptionary\n",
    "\n",
    "#train_df = pd.concat([train_df, samples_df], axis=0)\n",
    "train_df = samples_df\n",
    "#train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# description into chars\n",
    "#train_df['description_mod1'] = train_df['description_mod1'].apply(lambda x: process_string(x))\n",
    "\n",
    "# deduplicate\n",
    "#train_df.drop_duplicates(subset=['description_mod1'], inplace = True, keep=False)\n",
    "#print('train data shape (deduplicate):',train_df.shape)\n",
    "    \n",
    "# Encode target feature\n",
    "le = LabelEncoder()\n",
    "le.fit(train_df['category_full_path_mod1'])\n",
    "train_df['target_le'] = le.transform(train_df['category_full_path_mod1'])\n",
    "\n",
    "\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "string_w_max_tokens_1: 81\n",
      "string_w_max_tokens_2: 405\n"
     ]
    }
   ],
   "source": [
    "string_w_max_tokens_1 = train_df['description_mod1'].map(lambda x: len(str(x).split())).max()\n",
    "string_w_max_tokens_2 = train_df['description_cloned'].map(lambda x: len(str(x).split())).max()\n",
    "print('string_w_max_tokens_1:',string_w_max_tokens_1)\n",
    "print('string_w_max_tokens_2:',string_w_max_tokens_2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input matrix for HIERARCHICAL LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word_index size: 141483\n",
      "train_texts_vec_mtx_ shape: (484432, 30, 1)\n",
      "y_ary_cat shape: (484432, 861)\n"
     ]
    }
   ],
   "source": [
    "## Create input matrix for HIERARCHICAL LSTM\n",
    "\n",
    "top_words = None\n",
    "tok = Tokenizer(nb_words = top_words)\n",
    "tok.fit_on_texts(train_df.description_mod1)\n",
    "word_index = tok.word_index\n",
    "print('word_index size:',len(word_index))\n",
    "\n",
    "ngram_len = 1\n",
    "max_description_length = 30-ngram_len+1\n",
    "\n",
    "train_texts_vec_mtx_ = np.zeros((len(train_df), max_description_length, ngram_len), dtype='int32')\n",
    "for i, sentence_str in enumerate(train_df.description_mod1):\n",
    "    ngrams_ls = NGramGenerator_wordwise_interval(sentence_str, ngram_len, ngram_len)\n",
    "    for j, ngram_str in enumerate(ngrams_ls):\n",
    "        if j< max_description_length:\n",
    "            tokens_ls = NGramGenerator_wordwise_interval(ngram_str,1,1)\n",
    "            k=0\n",
    "            for token in tokens_ls:\n",
    "                if k<ngram_len:\n",
    "                    train_texts_vec_mtx_[i,j,k] = word_index[token]\n",
    "                    k=k+1  \n",
    "                    \n",
    "print('train_texts_vec_mtx_ shape:', train_texts_vec_mtx_.shape)\n",
    "\n",
    "y_ary = np.array(list(train_df['target_le']))\n",
    "y_ary_cat = np_utils.to_categorical(train_df['target_le'])\n",
    "print('y_ary_cat shape:',y_ary_cat.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input matrix for normal LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484432\n",
      "(484432, 861)\n",
      "word_index size: 141483\n",
      "train_texts_vec_mtx shape: (484432, 30)\n"
     ]
    }
   ],
   "source": [
    "# Create input matrix for normal LSTM\n",
    "\n",
    "# Split into train and test\n",
    "X_ls = np.array(list(train_df['description_mod1']))\n",
    "#X_ls = np.array(list(train_df['description_cloned']))\n",
    "y_ary = np.array(list(train_df['target_le']))\n",
    "y_ary_cat = np_utils.to_categorical(train_df['target_le'])\n",
    "\n",
    "#X_train_ls, X_test_ls, y_train_ary, y_test_ary = train_test_split(X_ls, y_ary, test_size = 0.3)\n",
    "#X_train_ls, X_test_ls, y_train_ary, y_test_ary = train_test_split(X_ls, y_ary_cat, test_size = 0.3)\n",
    "print(len(X_ls))\n",
    "print(y_ary_cat.shape)\n",
    "\n",
    "\n",
    "top_words = None\n",
    "max_description_length = 30 #string_w_max_tokens_1\n",
    "\n",
    "tok = Tokenizer(nb_words = top_words)\n",
    "tok.fit_on_texts(X_ls)\n",
    "word_index = tok.word_index\n",
    "print('word_index size:',len(word_index))\n",
    "\n",
    "#train_texts_vec_ls = tok.texts_to_sequences(X_train_ls)\n",
    "train_texts_vec_ls = texts_to_sequences_custom(X_ls, word_index, new_words_to_zero = False)\n",
    "train_texts_vec_mtx = sequence.pad_sequences(train_texts_vec_ls, maxlen = max_description_length)\n",
    "\n",
    "print('train_texts_vec_mtx shape:',train_texts_vec_mtx.shape)\n",
    "list(word_index)[0:5]\n",
    "\n",
    "# Delete objects\n",
    "X_ls = None\n",
    "y_ary = None\n",
    "tok = None\n",
    "train_texts_vec_ls = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[  0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0   0\n",
      "   0   0   0   0   0   0   0   0  44 147 886 107]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "141483"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# test check\n",
    "i = 100\n",
    "print(train_texts_vec_mtx[i])\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create random embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word index size: 141483\n",
      "embedding matrix shape: (141484, 32)\n",
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [-0.29038349 -0.3967836   0.46887872  0.41284853  0.15527183 -0.20647202\n",
      "  -0.05619827  0.37759045 -0.30625056  0.29131078  0.40400027 -0.1658854\n",
      "   0.06171373 -0.18156729  0.31655114 -0.32507217 -0.16188482  0.28201956\n",
      "   0.44501744  0.2954131  -0.21233307  0.08122893 -0.05593474 -0.28424983\n",
      "   0.12955629 -0.35590723  0.04077065 -0.42050908  0.20095938  0.45070885\n",
      "   0.18986001 -0.33806296]]\n"
     ]
    }
   ],
   "source": [
    "# Create RANDOM embedding vectors for each word in word index (lower cell code preferable)\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(12345) #8\n",
    "embeddings_source = 'random'\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "uniq_token_count = len(word_index)\n",
    "print('word index size:', uniq_token_count)\n",
    "\n",
    "is_random_embeddings = True\n",
    "embedding_matrix = np.zeros((uniq_token_count + 1, embedding_vecor_length))\n",
    "if(is_random_embeddings == True):\n",
    "    for word, i in word_index.items():\n",
    "        #embedding_vector = np.random.uniform(.1, size=(1, embedding_vecor_length))\n",
    "        embedding_vector = np.random.uniform(-0.5, 0.5, embedding_vecor_length)\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "else:\n",
    "    c = 0\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = np.random.uniform(c, c+100, size=(1, embedding_vecor_length))\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        c=c+6000\n",
    "    scaler = int('1'+'0'*len(str(c)))\n",
    "    embedding_matrix=embedding_matrix/scaler\n",
    "\n",
    "        \n",
    "print('embedding matrix shape:',embedding_matrix.shape)\n",
    "print(embedding_matrix[0:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create word embeddings from trained Word2Vec model\n",
    "from gensim.models import word2vec, Phrases\n",
    "embeddings_source = 'word2vec'\n",
    "\n",
    "# Load model\n",
    "file_path_1_1 = pardir+\"/3_model_word2vec_vec64_win1__dict_sample_5000/word2vect_class_specific_unigrams__vec64_win1__dict_sample_5000\"\n",
    "file_path_1_2 = pardir+\"/3_model_word2vec_vec64_win1__dict_sample_5000/word2vect_class_specific_unigrams__vec128_win1__dict_sample_5000\"\n",
    "file_path_1_3 = pardir+\"/3_model_word2vec_vec64_win1__dict_sample_5000/word2vect_vec_64_win30__dict_sample_5000\"\n",
    "file_path_1_4 = pardir+\"/3_model_word2vec_vec64_win1__dict_sample_5000/word2vect_unigrams_interrelations__vec64_win1__dict_sample_5000\"\n",
    "file_path_1_5 = '/Users/altay.amanbay/Desktop/word2vec_new_trial/3_word2vec_modeling/word2vect_vec64_win1_sample10_iter100__dict_sample_5000'\n",
    "file_path_1_6 = '/Users/altay.amanbay/Desktop/word2vec_new_trial/3_word2vec_modeling/word2vect_class_specififc_unigrams__vec64_win1_sample0_iter1000__dict_sample_5000'\n",
    "file_path_1_7 = '/Users/altay.amanbay/Desktop/word2vec_new_trial/3_word2vec_modeling/word2vect_class_specififc_unigrams__vec64_win1_sample0_iter100__dict_sample_5000'\n",
    "file_path_1_8 = '/Users/altay.amanbay/Desktop/word2vec_new_trial/3_word2vec_modeling/word2vect_class_specififc_unigrams__vec64_win1_sample10_iter100__dict_sample_5000'\n",
    "\n",
    "file_path_1_9 ='/Users/altay.amanbay/Desktop/word2vec_new_trial/3_word2vec_modeling/word2vect_class_specififc_unigrams__vec64_win1_sample0.001_iter1000__dict_sample_5000'\n",
    "file_path_1_10 ='/Users/altay.amanbay/Desktop/word2vec_new_trial/3_word2vec_modeling/'\n",
    "#model = word2vec.Word2Vec.load(file_path_1_4)\n",
    "\n",
    "file_path_2_1 = pardir+\"/3_model_fasttext/fasttext__vec64_win5__dict_sampled_5000.vec\"\n",
    "file_path_2_2 = pardir+\"/3_model_fasttext/fasttext__vec64_win1__dict_sampled_5000.vec\"\n",
    "file_path_2_3 = pardir+\"/3_model_fasttext/fasttext__vec128_win1__dict_sampled_5000.vec\"\n",
    "file_path_2_4 = pardir+\"/3_model_fasttext/fasttext__vec400_win2__scorecards.vec\"\n",
    "model = word2vec.Word2Vec.load_word2vec_format(file_path_2_4) # for fasttext model\n",
    "\n",
    "#print(model.vocab.keys())\n",
    "#sys.exit()\n",
    "\n",
    "# word vector embeddings from model into dictionary\n",
    "word2vec_dict={}\n",
    "for word in model.vocab.keys():\n",
    "    try:\n",
    "        word2vec_dict[word]=model[word]\n",
    "    except:    \n",
    "        pass\n",
    "print('Loaded %s word vectors.' % len(word2vec_dict))\n",
    "    \n",
    "embedding_vecor_length = len(model[word])\n",
    "print('embedding_vecor_length:',embedding_vecor_length)\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_vecor_length))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = word2vec_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print('\\nembedding matrix shape:',embedding_matrix.shape)\n",
    "print(embedding_matrix[0]) # first cell should be all zeros\n",
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Best model result holder\n",
    "best_model_aux = {}\n",
    "best_model_aux['Max length'] = max_description_length\n",
    "best_model_aux['Best score'] = 0\n",
    "best_model_aux['texts_to_sequences'] = texts_to_sequences_custom\n",
    "best_model_aux['word_index'] = word_index\n",
    "best_model_aux['Label encoder'] = le\n",
    "\n",
    "\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optional\n",
    "# Load previous model (if needs to be compared in the following training)\n",
    "#best_model = load_model('category_927_nets_1000_model.h5')\n",
    "#best_model_aux = get_model_file_aux('category_927_nets_1000_model_aux.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Classes count: 861\n"
     ]
    }
   ],
   "source": [
    "# prediction nodes count\n",
    "nb_classes = train_df['category_full_path_mod1'].unique()\n",
    "print('Classes count:', len(nb_classes))\n",
    "\n",
    "# keep the length of original (e.g. unboosted) train set\n",
    "orig_dim = train_texts_vec_mtx.shape[0]\n",
    "\n",
    "train_df = None\n",
    "samples_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model: Hierarchical LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 30, 1)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistribut (None, 30, 16)        4530624     input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (None, 16)            2112        timedistributed_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dense_1 (Dense)                  (None, 861)           14637       lstm_2[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 4,547,373\n",
      "Trainable params: 19,885\n",
      "Non-trainable params: 4,527,488\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch iter #1\n",
      "Epoch 1/10\n",
      "484432/484432 [==============================] - 165s - loss: 4.8744 - acc: 0.1045   \n",
      "Epoch 2/10\n",
      "484432/484432 [==============================] - 173s - loss: 4.1262 - acc: 0.2232   \n",
      "Epoch 3/10\n",
      "484432/484432 [==============================] - 172s - loss: 3.6405 - acc: 0.3197   \n",
      "Epoch 4/10\n",
      "484432/484432 [==============================] - 171s - loss: 3.3931 - acc: 0.3616   \n",
      "Epoch 5/10\n",
      "484432/484432 [==============================] - 172s - loss: 3.2399 - acc: 0.3823   \n",
      "Epoch 6/10\n",
      "484432/484432 [==============================] - 181s - loss: 3.1157 - acc: 0.3984   \n",
      "Epoch 7/10\n",
      "484432/484432 [==============================] - 206s - loss: 3.0152 - acc: 0.4133   \n",
      "Epoch 8/10\n",
      "484432/484432 [==============================] - 183s - loss: 2.9385 - acc: 0.4256   \n",
      "Epoch 9/10\n",
      "484432/484432 [==============================] - 185s - loss: 2.8757 - acc: 0.4358   \n",
      "Epoch 10/10\n",
      "484432/484432 [==============================] - 214s - loss: 2.8205 - acc: 0.4448   \n",
      "Captured improved model\n",
      "Valid accuracy:  0.448137199855\n",
      "\n",
      "Training took 1900.91 s\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "# TRAIN MODEL 1\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "embedding_layer = Embedding(top_words,\n",
    "                            embedding_vecor_length,\n",
    "                            weights=[embedding_matrix],\n",
    "                            input_length=ngram_len,\n",
    "                            trainable=False)\n",
    "\n",
    "sentence_input = Input(shape=(ngram_len,), dtype='int32') \n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "LSTM_ngram = LSTM(16)(embedded_sequences)   # Bidirectional(LSTM(16))(embedded_sequences)\n",
    "ngram_Encoder = Model(sentence_input, LSTM_ngram)\n",
    "\n",
    "sentence_input = Input(shape=(max_description_length, ngram_len), dtype='int32')\n",
    "sentence_encoder = TimeDistributed(ngram_Encoder)(sentence_input)\n",
    "LSTM_sentence = LSTM(16)(sentence_encoder)   # Bidirectional(LSTM(16))(sentence_encoder)\n",
    "preds = Dense(len(nb_classes), activation='softmax')(LSTM_sentence)\n",
    "model = Model(sentence_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try loss=categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(1):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx_, y_ary_cat, nb_epoch=10, batch_size=batch_size_)\n",
    "    \n",
    "    scores = model.evaluate(train_texts_vec_mtx_, y_ary_cat, verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        print('Model not improved')\n",
    "        print('Valid accuracy: ',scores[1])\n",
    "    #    break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))\n",
    "\n",
    "\n",
    "# 1  ================================================================================\n",
    "# Condition 1:\n",
    "# train data: scorecards_for_fasttext.csv\n",
    "# trainable=False\n",
    "# samples_df['category_full_path_mod1'].str.contains(\"Other\")==False\n",
    "# samples_df['category_full_path_mod1'] != 'Books'\n",
    "# ngram_len = 1, classes = 861\n",
    "\n",
    "# Accuracy progress:\n",
    "# loss: 4.6068 - acc: 0.1441  Valid accuracy:  0.209977870991\n",
    "# loss: 3.9868 - acc: 0.2639  Valid accuracy:  0.292889404498\n",
    "# loss: 3.6656 - acc: 0.3083  Valid accuracy:  0.322813108961\n",
    "# loss: 3.4465 - acc: 0.3395  Valid accuracy:  0.35384119959\n",
    "# .\n",
    "# .\n",
    "# 10th\n",
    "# loss: 2.8205 - acc: 0.4448  Valid accuracy:  0.448137199855"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model: Hierarchical LSTM with Attention layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_2 (InputLayer)             (None, 30, 1)         0                                            \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_2 (TimeDistribut (None, 30, 16)        4530912     input_2[0][0]                    \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (None, 30, 16)        2112        timedistributed_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_3 (TimeDistribut (None, 30, 16)        272         lstm_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "attlayer_2 (AttLayer)            (None, 16)            16          timedistributed_3[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dense_3 (Dense)                  (None, 861)           14637       attlayer_2[0][0]                 \n",
      "====================================================================================================\n",
      "Total params: 4,547,949\n",
      "Trainable params: 4,547,949\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch iter #1\n",
      "Epoch 1/1\n",
      "484432/484432 [==============================] - 1741s - loss: 3.2566 - acc: 0.3402  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.543275423589\n",
      "\n",
      "Epoch iter #2\n",
      "Epoch 1/1\n",
      "484432/484432 [==============================] - 2725s - loss: 1.8646 - acc: 0.6068  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.669084618687\n",
      "\n",
      "Epoch iter #3\n",
      "Epoch 1/1\n",
      "484432/484432 [==============================] - 2789s - loss: 1.4373 - acc: 0.6873  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.725622997655\n",
      "\n",
      "Epoch iter #4\n",
      "Epoch 1/1\n",
      "484432/484432 [==============================] - 2767s - loss: 1.2177 - acc: 0.7321  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.76296157149\n",
      "\n",
      "Epoch iter #5\n",
      "Epoch 1/1\n",
      "484432/484432 [==============================] - 3268s - loss: 1.0731 - acc: 0.7615  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.787022740034\n",
      "\n",
      "Epoch iter #6\n",
      "Epoch 1/1\n",
      "484432/484432 [==============================] - 3051s - loss: 0.9658 - acc: 0.7831  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.807328995607\n",
      "\n",
      "Epoch iter #7\n",
      "Epoch 1/1\n",
      "484432/484432 [==============================] - 2731s - loss: 0.8811 - acc: 0.8007  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.82103164118\n",
      "\n",
      "Epoch iter #8\n",
      "Epoch 1/1\n",
      "484432/484432 [==============================] - 2713s - loss: 0.8122 - acc: 0.8156  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.835132691482\n",
      "\n",
      "Epoch iter #9\n",
      "Epoch 1/1\n",
      "484432/484432 [==============================] - 2722s - loss: 0.7551 - acc: 0.8275  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.847388281534\n",
      "\n",
      "Epoch iter #10\n",
      "Epoch 1/1\n",
      "484432/484432 [==============================] - 2727s - loss: 0.7075 - acc: 0.8379  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.855242841101\n",
      "\n",
      "Training took 27882.3 s\n"
     ]
    }
   ],
   "source": [
    "from keras.layers import Input\n",
    "from keras.models import Model\n",
    "\n",
    "# TRAIN MODEL 1\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "embedding_layer = Embedding(top_words\n",
    "                            ,embedding_vecor_length\n",
    "                            ,weights=[embedding_matrix]\n",
    "                            ,input_length=ngram_len\n",
    "                            ,trainable=True\n",
    "                            ,init='glorot_uniform'\n",
    "                            )\n",
    "\n",
    "sentence_input = Input(shape=(ngram_len,), dtype='int32') \n",
    "embedded_sequences = embedding_layer(sentence_input)\n",
    "LSTM_ngram = LSTM(16, return_sequences=True)(embedded_sequences)    # Bidirectional(LSTM(16))(embedded_sequences)\n",
    "l_dense = TimeDistributed(Dense(16))(LSTM_ngram)\n",
    "l_att = AttLayer()(l_dense)\n",
    "#l_att = AttentionWithContext()(l_dense)\n",
    "ngram_Encoder = Model(sentence_input, l_att)\n",
    "\n",
    "sentence_input = Input(shape=(max_description_length, ngram_len), dtype='int32')\n",
    "sentence_encoder = TimeDistributed(ngram_Encoder)(sentence_input)\n",
    "LSTM_sentence = LSTM(16, return_sequences=True)(sentence_encoder)   # Bidirectional(LSTM(16))(sentence_encoder)\n",
    "l_dense_sent = TimeDistributed(Dense(16))(LSTM_sentence)\n",
    "l_att_sent = AttLayer()(l_dense_sent)\n",
    "#l_att_sent = AttentionWithContext()(l_dense_sent)\n",
    "preds = Dense(len(nb_classes), activation='softmax')(l_att_sent)\n",
    "model = Model(sentence_input, preds)\n",
    "\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try loss=categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=2) #stop training when loss didn't improve for 2 epochs\n",
    "for ep in range(10):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx_, y_ary_cat, nb_epoch=1, batch_size=batch_size_, shuffle=True) #, callbacks=[early_stopping]\n",
    "    \n",
    "    scores = model.evaluate(train_texts_vec_mtx_, y_ary_cat, verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        print('Model not improved')\n",
    "        print('Valid accuracy: ',scores[1])\n",
    "    #    break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))\n",
    "\n",
    "# 1  ================================================================================\n",
    "# Condition 1:\n",
    "# train dat: scorecards_for_fasttext.csv\n",
    "# trainable=False\n",
    "# samples_df['category_full_path_mod1'].str.contains(\"Other\")==False\n",
    "# samples_df['category_full_path_mod1'] != 'Books'\n",
    "# ngram_len = 1, classes = 861 # no difference for ngram_len = 2\n",
    "# no difference for AttLayer(), AttentionWithContext()\n",
    "\n",
    "# Accuracy progress:\n",
    "# loss: 4.1684 - acc: 0.2170  Valid accuracy:  0.302552267398\n",
    "# loss: 3.3941 - acc: 0.3548  Valid accuracy:  0.383362370776\n",
    "# loss: 3.0805 - acc: 0.4096  Valid accuracy:  0.428786702778\n",
    "# loss: 2.8864 - acc: 0.4410  Valid accuracy:  0.455839003204\n",
    "# .\n",
    "# .\n",
    "# .\n",
    "# 10th\n",
    "# loss: 2.4279 - acc: 0.5177  Valid accuracy:  0.521408577468\n",
    "\n",
    "# 2  ================================================================================\n",
    "# Condition 2:\n",
    "# train data: scorecards_for_fasttext.csv\n",
    "# trainable=True, init='glorot_uniform'\n",
    "# samples_df['category_full_path_mod1'].str.contains(\"Other\")==False\n",
    "# samples_df['category_full_path_mod1'] != 'Books'\n",
    "# ngram_len = 1, classes = 861\n",
    "# no difference for AttLayer(), AttentionWithContext()\n",
    "\n",
    "# Accuracy progress:\n",
    "# loss: 3.3483 - acc: 0.3130  Valid accuracy:  0.499686230472\n",
    "# loss: 1.9544 - acc: 0.5774  Valid accuracy:  0.643729150841\n",
    "# loss: 1.4911 - acc: 0.6736  Valid accuracy:  0.718173035638\n",
    "# loss: 1.2525 - acc: 0.7242  Valid accuracy:  0.755014119629\n",
    "# loss: 1.0931 - acc: 0.7578  Valid accuracy:  0.788760858077\n",
    "# loss: 0.9756 - acc: 0.7821  Valid accuracy:  0.805784919246\n",
    "# loss: 0.8856 - acc: 0.8008  Valid accuracy:  0.823702810714\n",
    "# loss: 0.8143 - acc: 0.8154  Valid accuracy:  0.837917396043\n",
    "# loss: 0.7564 - acc: 0.8283  Valid accuracy:  0.845518050005\n",
    "# loss: 0.7087 - acc: 0.8383  Valid accuracy:  0.856855038478\n",
    "# .\n",
    "# .\n",
    "# 21th\n",
    "# loss: 0.4684 - acc: 0.8889  Valid accuracy:  0.901172920038\n",
    "\n",
    "# with AttLayer()\n",
    "# Accuracy progress:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Fasttext"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def means(x):\n",
    "    return K.mean(x, axis=1)\n",
    "\n",
    "def max_(x):\n",
    "    return K.max(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings source: random\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_6 (Embedding)          (None, 30, 32)        4527488     embedding_input_6[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lambda_4 (Lambda)                (None, 32)            0           embedding_6[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_4 (Dense)                  (None, 861)           28413       lambda_4[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 4,555,901\n",
      "Trainable params: 4,555,901\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch iter #1\n",
      "Epoch 1/1\n",
      "484432/484432 [==============================] - 1599s - loss: 3.4516 - acc: 0.3761  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.578107969746\n",
      "\n",
      "Epoch iter #2\n",
      "Epoch 1/1\n",
      "484432/484432 [==============================] - 2583s - loss: 1.8670 - acc: 0.6388  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.687543349737\n",
      "\n",
      "Epoch iter #3\n",
      "Epoch 1/1\n",
      "484432/484432 [==============================] - 2811s - loss: 1.3975 - acc: 0.7148  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.742787429402\n",
      "\n",
      "Epoch iter #4\n",
      "Epoch 1/1\n",
      "484432/484432 [==============================] - 2869s - loss: 1.1563 - acc: 0.7562  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.77322926644\n",
      "\n",
      "Training took 9935.12 s\n"
     ]
    }
   ],
   "source": [
    "# TRAIN MODEL 1\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "model = Sequential()\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ---- Embedding layer -----------------------------------------------------------------\n",
    "print('Embeddings source:',embeddings_source)\n",
    "embedding_layer = Embedding(top_words\n",
    "                            ,embedding_vecor_length\n",
    "                            ,weights=[embedding_matrix]\n",
    "                            ,input_length = max_description_length\n",
    "                            ,trainable=True\n",
    "                            ,init='glorot_uniform')\n",
    "model.add(embedding_layer)\n",
    "model.add(Lambda(means, output_shape=(embedding_vecor_length,)))\n",
    "\n",
    "## OUTPUT: classes layer\n",
    "## ======================================================================================\n",
    "model.add(Dense(len(nb_classes), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try loss=categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(4):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx, y_ary_cat, nb_epoch=1, batch_size=batch_size_)\n",
    "    \n",
    "    scores = model.evaluate(train_texts_vec_mtx, y_ary_cat, verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        print('Model not improved')\n",
    "        print('Valid accuracy: ',scores[1])\n",
    "    #    break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))\n",
    "\n",
    "\n",
    "# 1  ================================================================================\n",
    "# Condition 1:\n",
    "# train data: scorecards_for_fasttext.csv\n",
    "# trainable=True, init='glorot_uniform'\n",
    "# samples_df['category_full_path_mod1'].str.contains(\"Other\")==False\n",
    "# samples_df['category_full_path_mod1'] != 'Books'\n",
    "# ngram_len = 1, classes = 861\n",
    "# trainable=True, init='glorot_uniform'\n",
    "\n",
    "# Accuracy progress:\n",
    "# loss: 3.4516 - acc: 0.3761  Valid accuracy:  0.578107969746\n",
    "# loss: 1.8670 - acc: 0.6388  Valid accuracy:  0.687543349737\n",
    "# loss: 1.3975 - acc: 0.7148  Valid accuracy:  0.742787429402\n",
    "# loss: 1.1563 - acc: 0.7562  Valid accuracy:  0.77322926644\n",
    "# 1000 sec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_5 (Embedding)          (None, 30, 1)         141484      embedding_input_5[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_5 (LSTM)                    (None, 32)            4352        embedding_5[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_10 (Dense)                 (None, 861)           28413       lstm_5[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 174,249\n",
      "Trainable params: 174,249\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch iter #1\n",
      "Epoch 1/1\n",
      "236608/484432 [=============>................] - ETA: 90s - loss: 3.8063 - acc: 0.2570"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-18-aa1b6c78ea5a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     44\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     45\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch iter #'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 46\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts_vec_mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ary_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     47\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     48\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts_vec_mtx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0morig_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ary_cat\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0morig_dim\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1194\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAIN MODEL 1\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "model = Sequential()\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ---- Embedding layer -----------------------------------------------------------------\n",
    "#print('Embeddings source:',embeddings_source)\n",
    "embedding_layer = Embedding(top_words\n",
    "                            ,embedding_vecor_length\n",
    "                            #,weights=[embedding_matrix]\n",
    "                            ,input_length = max_description_length\n",
    "                            ,init='glorot_uniform'\n",
    "                            ,trainable=True)\n",
    "model.add(embedding_layer)\n",
    "#model.add(BatchNormalization()) #axis=1\n",
    "\n",
    "## LSTM 1\n",
    "## ======================================================================================\n",
    "LSTM_1 = LSTM(32, return_sequences=False, activation='linear')\n",
    "model.add(LSTM_1)\n",
    "#LSTM_1 = SimpleRNN(128,return_sequences=False)\n",
    "#model.add(LSTM(8,return_sequences=False, activation='linear'))\n",
    "#model.add(LSTM(64,return_sequences=False, activation='linear'))\n",
    "#LSTM_1 = LSTM(128,return_sequences=False, dropout_W = 0.3, dropout_U = 0.3)\n",
    "\n",
    "\n",
    "## Dense 1\n",
    "## ======================================================================================\n",
    "Dense_1 = Dense(200,activation='sigmoid')\n",
    "#model.add(Dense_1)\n",
    "\n",
    "\n",
    "## OUTPUT: classes layer\n",
    "## ======================================================================================\n",
    "model.add(Dense(len(nb_classes), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try loss=categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(10):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx, y_ary_cat, nb_epoch=1, batch_size=batch_size_)\n",
    "    \n",
    "    scores = model.evaluate(train_texts_vec_mtx[0:orig_dim], y_ary_cat[0:orig_dim], verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        print('Model not improved')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "    #    break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))\n",
    "\n",
    "# word2vect_vec64_win1_sample0_iter100__dict_sample_5000\n",
    "# Accuracy: loss: start: 0.6854, end: 0.8602\n",
    "\n",
    "# word2vect_vec64_win1_sample0_iter1000__dict_sample_5000\n",
    "# Accuracy: loss: start: 0.6958\n",
    "\n",
    "# word2vect_vec64_win1_sample10_iter100__dict_sample_5000\n",
    "# Accuracy: loss: start: 0.7053\n",
    "\n",
    "# word2vect_class_specififc_unigrams__vec64_win1_sample0_iter1000__dict_sample_5000\n",
    "# Accuracy: loss: start: 0.7368, 0.8493\n",
    "\n",
    "# word2vect_class_specififc_unigrams__vec64_win1_sample0_iter100__dict_sample_5000\n",
    "# Accuracy: loss: start: 0.7198, \n",
    "\n",
    "# word2vect_class_specififc_unigrams__vec64_win1_sample10_iter100__dict_sample_5000\n",
    "# Accuracy: loss: start: 0.7388\n",
    "\n",
    "\n",
    "# 1  ================================================================================\n",
    "# Condition 1:\n",
    "# Embedding + LSTM\n",
    "# train dat: scorecards_for_fasttext.csv\n",
    "# trainable=False\n",
    "# hidden units: 16\n",
    "# samples_df['category_full_path_mod1'].str.contains(\"Other\")==False\n",
    "# samples_df['category_full_path_mod1'] != 'Books'\n",
    "\n",
    "# Accuracy progress:\n",
    "# loss: 3.9365 - acc: 0.2722  Valid accuracy:  0.339447022492\n",
    "# loss: 3.3892 - acc: 0.3639  Valid accuracy:  0.381209333818\n",
    "# loss: 3.2052 - acc: 0.3929  Valid accuracy:  0.401532103577\n",
    "# loss: 3.0906 - acc: 0.4100  Valid accuracy:  0.416644647752\n",
    "# loss: 3.0094 - acc: 0.4218  Valid accuracy:  0.427795851637\n",
    "# .\n",
    "# .\n",
    "# 20th\n",
    "# loss: 2.5908 - acc: 0.4885  Valid accuracy:  0.490392872477\n",
    "\n",
    "# 2  ================================================================================\n",
    "# Condition 2:\n",
    "# Embedding + LSTM\n",
    "# train dat: scorecards_for_fasttext.csv\n",
    "# trainable=True, init='glorot_uniform'\n",
    "# hidden units: 16, 32 (almost same results)\n",
    "# samples_df['category_full_path_mod1'].str.contains(\"Other\")==False\n",
    "# samples_df['category_full_path_mod1'] != 'Books'\n",
    "\n",
    "# Accuracy progress:\n",
    "# loss: 2.2884 - acc: 0.5328  Valid accuracy:  0.656050797635\n",
    "# loss: 1.3082 - acc: 0.7069  Valid accuracy:  0.758475905803\n",
    "# loss: 1.0187 - acc: 0.7653  Valid accuracy:  0.805570234832\n",
    "# loss: 0.8438 - acc: 0.8029  Valid accuracy:  0.840631915315\n",
    "# loss: 0.7238 - acc: 0.8281  Valid accuracy:  0.85519742709\n",
    "# .\n",
    "# 13th\n",
    "# loss: 0.3682 - acc: 0.9069  Valid accuracy:  0.924053324306\n",
    "\n",
    "# for hidden units 32:\n",
    "# 14th\n",
    "# loss: 0.2676 - acc: 0.9291  Valid accuracy:  0.947082356244"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.36778330085543481"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_aux['Best score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch iter #1\n",
      "Epoch 1/1\n",
      "484432/484432 [==============================] - 2806s - loss: 0.3334 - acc: 0.9150  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.934331340622\n",
      "\n",
      "Epoch iter #2\n",
      "Epoch 1/1\n",
      "484432/484432 [==============================] - 2836s - loss: 0.3098 - acc: 0.9196  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.938292680913\n",
      "\n",
      "Epoch iter #3\n",
      "Epoch 1/1\n",
      "484432/484432 [==============================] - 2869s - loss: 0.2893 - acc: 0.9243  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.94294555273\n",
      "\n",
      "Epoch iter #4\n",
      "Epoch 1/1\n",
      "484432/484432 [==============================] - 2981s - loss: 0.2676 - acc: 0.9291  \n",
      "Captured improved model\n",
      "Valid accuracy:  0.947082356244\n",
      "\n",
      "Epoch iter #5\n",
      "Epoch 1/1\n",
      "484432/484432 [==============================] - 3042s - loss: 0.2544 - acc: 0.9325  \n",
      "Model not improved\n",
      "Valid accuracy:  0.947082356244\n",
      "\n"
     ]
    }
   ],
   "source": [
    "for ep in range(5):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx, y_ary_cat, nb_epoch=1, batch_size=batch_size_)\n",
    "    \n",
    "    scores = model.evaluate(train_texts_vec_mtx[0:orig_dim], y_ary_cat[0:orig_dim], verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        print('Model not improved')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "    #    break\n",
    "    print()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train set boosting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples data shape: (587983, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_mod1</th>\n",
       "      <th>category_id_mod1</th>\n",
       "      <th>category_full_path_mod1</th>\n",
       "      <th>target_le</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aveeno baby wash  shampoo lightly scented 8 ou...</td>\n",
       "      <td>206</td>\n",
       "      <td>Baby Products &gt; Bathing &amp; Skin Care &gt; Soaps &amp; ...</td>\n",
       "      <td>206</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>earths best chlorine free diapers size 4 120 c...</td>\n",
       "      <td>213</td>\n",
       "      <td>Baby Products &gt; Diapering &gt; Disposable Diapers</td>\n",
       "      <td>212</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doa5lr costume catalog lr10 addon content</td>\n",
       "      <td>320</td>\n",
       "      <td>Electronics &amp; Accessories &gt; Video Games &gt; Other</td>\n",
       "      <td>298</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lightweight oxford l in slim and white</td>\n",
       "      <td>152</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Tops &amp; Tees ...</td>\n",
       "      <td>90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>now foods nutritional yeast flakes 10ounce</td>\n",
       "      <td>525</td>\n",
       "      <td>Health &amp; Beauty &gt; Vitamins &amp; Dietary Supplements</td>\n",
       "      <td>504</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    description_mod1 category_id_mod1  \\\n",
       "0  aveeno baby wash  shampoo lightly scented 8 ou...              206   \n",
       "1  earths best chlorine free diapers size 4 120 c...              213   \n",
       "2          doa5lr costume catalog lr10 addon content              320   \n",
       "3             lightweight oxford l in slim and white              152   \n",
       "4         now foods nutritional yeast flakes 10ounce              525   \n",
       "\n",
       "                             category_full_path_mod1  target_le  \n",
       "0  Baby Products > Bathing & Skin Care > Soaps & ...        206  \n",
       "1     Baby Products > Diapering > Disposable Diapers        212  \n",
       "2    Electronics & Accessories > Video Games > Other        298  \n",
       "3  Apparel & Accessories > Apparel > Tops & Tees ...         90  \n",
       "4   Health & Beauty > Vitamins & Dietary Supplements        504  "
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read sampled descriptionary\n",
    "\n",
    "path = pardir+'/1_data/'\n",
    "file_name = 'sampled_descriptionary_sample_size_5000.csv'\n",
    "file_name = 'scorecards_for_fasttext.csv'\n",
    "train_df = pd.read_csv(path + file_name)\n",
    "\n",
    "# Rename columns\n",
    "train_df.rename(columns={\n",
    "                           #'description': 'description_mod1', \n",
    "                           '\\ufeff\"description\"': 'description_mod1',\n",
    "                           'category_id': 'category_id_mod1',\n",
    "                           'category_path': 'category_full_path_mod1'}, inplace=True)\n",
    "\n",
    "# Drop rows with NaN in any column\n",
    "train_df.dropna()\n",
    "\n",
    "# Process description_mod1 strings by process_string function\n",
    "train_df['description_mod1'] = train_df['description_mod1'].apply(lambda x: process_string(x))\n",
    "\n",
    "# Drop rows where token count less than 1 in description_mod1 column\n",
    "selected_indices = train_df['description_mod1'].apply(lambda x: len(str(x).split()) > 1)\n",
    "train_df = train_df[selected_indices]\n",
    "\n",
    "# Drop duplicates\n",
    "train_df.drop_duplicates(subset=['description_mod1','category_full_path_mod1'], inplace = True, keep='first')\n",
    "train_df.drop_duplicates(subset=['description_mod1'], inplace = True, keep=False)\n",
    "train_df.reset_index(drop=True, inplace=True) \n",
    "\n",
    "train_df['target_le'] = best_model_aux['Label encoder'].transform(train_df['category_full_path_mod1'])\n",
    "\n",
    "print('samples data shape:',train_df.shape)\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "587584/587983 [============================>.] - ETA: 0s"
     ]
    }
   ],
   "source": [
    "# 1 \n",
    "# Get predictions\n",
    "X_ls = np.array(list(train_df['description_mod1']))\n",
    "seq_ = texts_to_sequences_custom(X_ls, best_model_aux['word_index'], new_words_to_zero = False)\n",
    "seq_pad = sequence.pad_sequences(seq_, maxlen = 30)\n",
    "\n",
    "predictions = best_model.predict_classes(seq_pad)\n",
    "\n",
    "X_ls = None\n",
    "seq_ = None\n",
    "seq_pad = None\n",
    "old_best_model_ = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_mod1</th>\n",
       "      <th>category_id_mod1</th>\n",
       "      <th>category_full_path_mod1</th>\n",
       "      <th>target_le</th>\n",
       "      <th>Predictions_le</th>\n",
       "      <th>Predictions</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aveeno baby wash  shampoo lightly scented 8 ou...</td>\n",
       "      <td>206</td>\n",
       "      <td>Baby Products &gt; Bathing &amp; Skin Care &gt; Soaps &amp; ...</td>\n",
       "      <td>206</td>\n",
       "      <td>312</td>\n",
       "      <td>Grocery &amp; Gourmet Food &gt; Beverages &gt; Juices</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>earths best chlorine free diapers size 4 120 c...</td>\n",
       "      <td>213</td>\n",
       "      <td>Baby Products &gt; Diapering &gt; Disposable Diapers</td>\n",
       "      <td>212</td>\n",
       "      <td>212</td>\n",
       "      <td>Baby Products &gt; Diapering &gt; Disposable Diapers</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doa5lr costume catalog lr10 addon content</td>\n",
       "      <td>320</td>\n",
       "      <td>Electronics &amp; Accessories &gt; Video Games &gt; Other</td>\n",
       "      <td>298</td>\n",
       "      <td>298</td>\n",
       "      <td>Electronics &amp; Accessories &gt; Video Games &gt; Other</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lightweight oxford l in slim and white</td>\n",
       "      <td>152</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Tops &amp; Tees ...</td>\n",
       "      <td>90</td>\n",
       "      <td>90</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Tops &amp; Tees ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>now foods nutritional yeast flakes 10ounce</td>\n",
       "      <td>525</td>\n",
       "      <td>Health &amp; Beauty &gt; Vitamins &amp; Dietary Supplements</td>\n",
       "      <td>504</td>\n",
       "      <td>240</td>\n",
       "      <td>Books</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    description_mod1 category_id_mod1  \\\n",
       "0  aveeno baby wash  shampoo lightly scented 8 ou...              206   \n",
       "1  earths best chlorine free diapers size 4 120 c...              213   \n",
       "2          doa5lr costume catalog lr10 addon content              320   \n",
       "3             lightweight oxford l in slim and white              152   \n",
       "4         now foods nutritional yeast flakes 10ounce              525   \n",
       "\n",
       "                             category_full_path_mod1  target_le  \\\n",
       "0  Baby Products > Bathing & Skin Care > Soaps & ...        206   \n",
       "1     Baby Products > Diapering > Disposable Diapers        212   \n",
       "2    Electronics & Accessories > Video Games > Other        298   \n",
       "3  Apparel & Accessories > Apparel > Tops & Tees ...         90   \n",
       "4   Health & Beauty > Vitamins & Dietary Supplements        504   \n",
       "\n",
       "   Predictions_le                                        Predictions  \n",
       "0             312        Grocery & Gourmet Food > Beverages > Juices  \n",
       "1             212     Baby Products > Diapering > Disposable Diapers  \n",
       "2             298    Electronics & Accessories > Video Games > Other  \n",
       "3              90  Apparel & Accessories > Apparel > Tops & Tees ...  \n",
       "4             240                                              Books  "
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2\n",
    "# Append predictions to df\n",
    "train_df['Predictions_le'] = list(predictions)\n",
    "train_df['Predictions'] = train_df['Predictions_le'].apply(lambda x: best_model_aux['Label encoder'].inverse_transform(x))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(358126,)\n",
      "train_texts_vec_mtx_boost shape: (358126, 30)\n",
      "y_ary_cat_boost: (358126, 1021)\n"
     ]
    }
   ],
   "source": [
    "# 3\n",
    "# Get FP items to be boosted\n",
    "X_ls_boost = np.array(list(train_df.loc[train_df.target_le != train_df.Predictions_le,'description_mod1']))\n",
    "print(X_ls_boost.shape)\n",
    "\n",
    "train_texts_vec_ls_boost = texts_to_sequences_custom(X_ls_boost, best_model_aux['word_index'], new_words_to_zero = False)\n",
    "train_texts_vec_mtx_boost = sequence.pad_sequences(train_texts_vec_ls_boost, maxlen = best_model_aux['Max length'])\n",
    "print('train_texts_vec_mtx_boost shape:',train_texts_vec_mtx_boost.shape)\n",
    "\n",
    "ind = train_df.loc[train_df.target_le != train_df.Predictions_le,'description_mod1'].index.values\n",
    "y_ary_cat_boost = y_ary_cat[ind]\n",
    "print('y_ary_cat_boost:',y_ary_cat_boost.shape)\n",
    "\n",
    "X_ls_boost = None\n",
    "train_texts_vec_ls_boost = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train_texts_vec_mtx: (587983, 30)\n",
      "Original y_ary_cat: (587983, 1021)\n",
      "Boosted train_texts_vec_mtx: (978219, 30)\n",
      "Boosted y_ary_cat: (978219, 1021)\n"
     ]
    }
   ],
   "source": [
    "# 4\n",
    "# Boost train set\n",
    "#print('Original train_texts_vec_mtx:',train_texts_vec_mtx.shape)\n",
    "#print('Original y_ary_cat:',y_ary_cat.shape)\n",
    "\n",
    "#train_texts_vec_mtx = np.concatenate([train_texts_vec_mtx,train_texts_vec_mtx_boost], axis=0)\n",
    "#print('Boosted train_texts_vec_mtx:',train_texts_vec_mtx.shape)\n",
    "\n",
    "#y_ary_cat = np.concatenate([y_ary_cat,y_ary_cat_boost], axis=0)\n",
    "#print('Boosted y_ary_cat:',y_ary_cat.shape)\n",
    "\n",
    "#train_texts_vec_mtx_boost = None\n",
    "#y_ary_cat_boost = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original train_texts_vec_mtx: (978219, 30)\n",
      "Original y_ary_cat: (978219, 1021)\n",
      "Boosted train_texts_vec_mtx: (342347, 30)\n",
      "Boosted y_ary_cat: (342347, 1021)\n"
     ]
    }
   ],
   "source": [
    "# print('Original train_texts_vec_mtx:',train_texts_vec_mtx.shape)\n",
    "# print('Original y_ary_cat:',y_ary_cat.shape)\n",
    "\n",
    "# train_texts_vec_mtx = train_texts_vec_mtx_boost\n",
    "# print('Boosted train_texts_vec_mtx:',train_texts_vec_mtx.shape)\n",
    "\n",
    "# y_ary_cat = y_ary_cat_boost\n",
    "# print('Boosted y_ary_cat:',y_ary_cat.shape)\n",
    "\n",
    "# train_texts_vec_mtx_boost = None\n",
    "# y_ary_cat_boost = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_1 (Embedding)          (None, 30, 32)        5317632     embedding_input_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_1 (LSTM)                    (None, 16)            3136        embedding_1[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "dense_2 (Dense)                  (None, 1021)          17357       lstm_1[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 5,338,125\n",
      "Trainable params: 20,493\n",
      "Non-trainable params: 5,317,632\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch iter #1\n",
      "Epoch 1/1\n",
      "358126/358126 [==============================] - 82s - loss: 4.5147 - acc: 0.1049    \n",
      "Model not improved\n",
      "Valid accuracy:  0.201453103236\n",
      "\n",
      "Epoch iter #2\n",
      "Epoch 1/1\n",
      "358126/358126 [==============================] - 94s - loss: 4.3919 - acc: 0.1350    \n",
      "Model not improved\n",
      "Valid accuracy:  0.186687370213\n",
      "\n",
      "Epoch iter #3\n",
      "Epoch 1/1\n",
      "358126/358126 [==============================] - 74s - loss: 4.3152 - acc: 0.1489    \n",
      "Model not improved\n",
      "Valid accuracy:  0.192576996274\n",
      "\n",
      "Training took 1523.14 s\n"
     ]
    }
   ],
   "source": [
    "# Continue training with boosted train set\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(3):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx_boost, y_ary_cat_boost, nb_epoch=1, batch_size=batch_size_)\n",
    "    \n",
    "    #scores = model.evaluate(train_texts_vec_mtx[0:orig_dim], y_ary_cat[0:orig_dim], verbose=0)\n",
    "    scores = model.evaluate(train_texts_vec_mtx, y_ary_cat, verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        print('Model not improved')\n",
    "        print('Valid accuracy: ',scores[1])\n",
    "    #    break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.41776037742603001"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "best_model_aux['Best score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2 (CNN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings source: random\n",
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_8 (Embedding)          (None, 30, 64)        10635264    embedding_input_8[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_29 (Convolution1D) (None, 28, 64)        12352       embedding_8[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_29 (MaxPooling1D)   (None, 26, 64)        0           convolution1d_29[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_30 (Convolution1D) (None, 26, 64)        8256        maxpooling1d_29[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_30 (MaxPooling1D)   (None, 24, 64)        0           convolution1d_30[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_31 (Convolution1D) (None, 12, 128)       32896       maxpooling1d_30[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_31 (MaxPooling1D)   (None, 10, 128)       0           convolution1d_31[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "convolution1d_32 (Convolution1D) (None, 5, 128)        65664       maxpooling1d_31[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "maxpooling1d_32 (MaxPooling1D)   (None, 3, 128)        0           convolution1d_32[0][0]           \n",
      "____________________________________________________________________________________________________\n",
      "flatten_8 (Flatten)              (None, 384)           0           maxpooling1d_32[0][0]            \n",
      "____________________________________________________________________________________________________\n",
      "dense_15 (Dense)                 (None, 600)           231000      flatten_8[0][0]                  \n",
      "____________________________________________________________________________________________________\n",
      "dense_16 (Dense)                 (None, 1021)          613621      dense_15[0][0]                   \n",
      "====================================================================================================\n",
      "Total params: 11,599,053\n",
      "Trainable params: 963,789\n",
      "Non-trainable params: 10,635,264\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch iter #1\n",
      "Epoch 1/1\n",
      "257664/587983 [============>.................] - ETA: 516s - loss: 4.2637 - acc: 0.1999"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-20-f47faff00f6d>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     74\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     75\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch iter #'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 76\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts_vec_mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ary_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     77\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     78\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts_vec_mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ary_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1194\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAIN MODEL 2 (CNN)\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "model = Sequential()\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ---- Embedding layer -----------------------------------------------------------------\n",
    "print('Embeddings source:',embeddings_source)\n",
    "embedding_layer = Embedding(top_words, \n",
    "                            embedding_vecor_length, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length = max_description_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "#model.add(BatchNormalization()) #axis=1\n",
    "\n",
    "nb_feature_maps = 8\n",
    "n_gram = 1\n",
    "#model.add(Reshape(1, max_description_length, embedding_vecor_length))\n",
    "#model.add(Convolution2D(nb_feature_maps, 1, n_gram, embedding_vecor_length))\n",
    "#model.add(MaxPooling2D(poolsize=(max_description_length - n_gram + 1, 1)))\n",
    "#model.add(Flatten())\n",
    "\n",
    "# Conv 1\n",
    "nb_filter_1 = 64\n",
    "conv1D_1 = Conv1D(nb_filter=nb_filter_1   # feature maps\n",
    "                  ,filter_length=3        # kernel size\n",
    "                  ,subsample_length=1     # strides\n",
    "                  ,border_mode='valid'    # padding: same, valid\n",
    "                  ,activation='relu'\n",
    "                 )\n",
    "model.add(conv1D_1)\n",
    "model.add(MaxPooling1D(pool_length = 3, stride = 1)) #model.output_shape[1]\n",
    "\n",
    "# Conv 2\n",
    "nb_filter_2 = 64\n",
    "conv1D_2 = Conv1D(nb_filter=nb_filter_2,filter_length=2, subsample_length=1,border_mode='same',activation='relu')\n",
    "model.add(conv1D_2)\n",
    "model.add(MaxPooling1D(pool_length = 3, stride = 1))\n",
    "\n",
    "# Conv 3\n",
    "nb_filter_3 = 128\n",
    "conv1D_3 = Conv1D(nb_filter=nb_filter_3,filter_length=4, subsample_length=2,border_mode='same',activation='relu')\n",
    "model.add(conv1D_3)\n",
    "model.add(MaxPooling1D(pool_length = 3, stride = 1))\n",
    "\n",
    "# Conv 4\n",
    "nb_filter_4 = 128\n",
    "conv1D_4 = Conv1D(nb_filter=nb_filter_4,filter_length=4, subsample_length=2,border_mode='same',activation='relu')\n",
    "model.add(conv1D_4)\n",
    "model.add(MaxPooling1D(pool_length = 3, stride = 1))\n",
    "\n",
    "\n",
    "\n",
    "model.add(Flatten())\n",
    "\n",
    "\n",
    "## Dense 1\n",
    "## ======================================================================================\n",
    "Dense_1 = Dense(600,activation='relu')\n",
    "model.add(Dense_1)\n",
    "\n",
    "\n",
    "## OUTPUT: classes layer\n",
    "## ======================================================================================\n",
    "model.add(Dense(len(nb_classes), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try loss=categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(20):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx, y_ary_cat, nb_epoch=1, batch_size=batch_size_)\n",
    "    \n",
    "    scores = model.evaluate(train_texts_vec_mtx, y_ary_cat, verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    #else:\n",
    "    #    break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FUNCTION FOR TRAIN MODEL 2\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializations\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializations.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "embedding_2 (Embedding)          (None, 30, 32)        4527488     embedding_input_2[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "lstm_2 (LSTM)                    (None, 30, 32)        8320        embedding_2[0][0]                \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_1 (TimeDistribut (None, 30, 32)        1056        lstm_2[0][0]                     \n",
      "____________________________________________________________________________________________________\n",
      "lstm_3 (LSTM)                    (None, 32)            8320        timedistributed_1[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "dense_5 (Dense)                  (None, 861)           28413       lstm_3[0][0]                     \n",
      "====================================================================================================\n",
      "Total params: 4,573,597\n",
      "Trainable params: 46,109\n",
      "Non-trainable params: 4,527,488\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch iter #1\n",
      "Epoch 1/1\n",
      "484432/484432 [==============================] - 352s - loss: 4.0701 - acc: 0.2515   \n",
      "Captured improved model\n",
      "Valid accuracy:  0.372849027314\n",
      "\n",
      "Epoch iter #2\n",
      "Epoch 1/1\n",
      "445632/484432 [==========================>...] - ETA: 28s - loss: 3.0876 - acc: 0.4159"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-17-ed9271394f74>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     43\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mep\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mrange\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m20\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     44\u001b[0m     \u001b[0mprint\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'Epoch iter #'\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mstr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mep\u001b[0m\u001b[0;34m+\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 45\u001b[0;31m     \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts_vec_mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ary_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnb_epoch\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mbatch_size_\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     46\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     47\u001b[0m     \u001b[0mscores\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mmodel\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrain_texts_vec_mtx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_ary_cat\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/models.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch, **kwargs)\u001b[0m\n\u001b[1;32m    670\u001b[0m                               \u001b[0mclass_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mclass_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    671\u001b[0m                               \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0msample_weight\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 672\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    674\u001b[0m     def evaluate(self, x, y, batch_size=32, verbose=1,\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, x, y, batch_size, nb_epoch, verbose, callbacks, validation_split, validation_data, shuffle, class_weight, sample_weight, initial_epoch)\u001b[0m\n\u001b[1;32m   1194\u001b[0m                               \u001b[0mval_f\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_f\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mval_ins\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mval_ins\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mshuffle\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mshuffle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1195\u001b[0m                               \u001b[0mcallback_metrics\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mcallback_metrics\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1196\u001b[0;31m                               initial_epoch=initial_epoch)\n\u001b[0m\u001b[1;32m   1197\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1198\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mevaluate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m32\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mverbose\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0msample_weight\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;32mNone\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/engine/training.py\u001b[0m in \u001b[0;36m_fit_loop\u001b[0;34m(self, f, ins, out_labels, batch_size, nb_epoch, verbose, callbacks, val_f, val_ins, shuffle, callback_metrics, initial_epoch)\u001b[0m\n\u001b[1;32m    889\u001b[0m                 \u001b[0mbatch_logs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'size'\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_ids\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    890\u001b[0m                 \u001b[0mcallbacks\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mon_batch_begin\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mbatch_index\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mbatch_logs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 891\u001b[0;31m                 \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mf\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mins_batch\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    892\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    893\u001b[0m                     \u001b[0mouts\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mouts\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/keras/backend/theano_backend.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, inputs)\u001b[0m\n\u001b[1;32m    957\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__call__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    958\u001b[0m         \u001b[0;32massert\u001b[0m \u001b[0misinstance\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtuple\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 959\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfunction\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0minputs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    960\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    961\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/theano/compile/function_module.py\u001b[0m in \u001b[0;36m__call__\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m    857\u001b[0m         \u001b[0mt0_fn\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    858\u001b[0m         \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 859\u001b[0;31m             \u001b[0moutputs\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    860\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mException\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    861\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mhasattr\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfn\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'position_of_error'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# TRAIN MODEL 2\n",
    "# Attention-based\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "model = Sequential()\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ---- Embedding layer -----------------------------------------------------------------\n",
    "embedding_layer = Embedding(top_words, \n",
    "                            embedding_vecor_length, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length = max_description_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "## LSTM 1\n",
    "## ======================================================================================\n",
    "LSTM_1 = LSTM(32,return_sequences=True)\n",
    "model.add(LSTM_1)\n",
    "\n",
    "## Attention 1\n",
    "## ======================================================================================\n",
    "TimeDist_1 = TimeDistributed(Dense(32))  #, input_shape=(max_description_length, embedding_vecor_length)\n",
    "model.add(TimeDist_1)\n",
    "model.add(LSTM(32))\n",
    "#model.add(AttLayer())\n",
    "\n",
    "## Dense 1\n",
    "## ======================================================================================\n",
    "Dense_1 = Dense(128,activation='sigmoid')\n",
    "#model.add(Dense_1)\n",
    "\n",
    "## Output classes layer\n",
    "## ======================================================================================\n",
    "model.add(Dense(len(nb_classes), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try loss=categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(20):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx, y_ary_cat, nb_epoch=1, batch_size=batch_size_)\n",
    "    \n",
    "    scores = model.evaluate(train_texts_vec_mtx, y_ary_cat, verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))\n",
    "\n",
    "# Accuracy: loss: 0.2722 - acc: 0.9140"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_model_aux['Best score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Nets design\n",
    "#from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "plot(model, to_file='/Users/altay.amanbay/Desktop/model.png')\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FUNCTION FOR TRAIN MODEL 3\n",
    "\n",
    "class AttentionWithContext(Layer):\n",
    "    \"\"\"\n",
    "        Attention operation, with a context/query vector, for temporal data.\n",
    "        Supports Masking.\n",
    "        Follows the work of Yang et al. [https://www.cs.cmu.edu/~diyiy/docs/naacl16.pdf]\n",
    "        \"Hierarchical Attention Networks for Document Classification\"\n",
    "        by using a context vector to assist the attention\n",
    "        # Input shape\n",
    "            3D tensor with shape: `(samples, steps, features)`.\n",
    "        # Output shape\n",
    "            2D tensor with shape: `(samples, features)`.\n",
    "        :param kwargs:\n",
    "        Just put it on top of an RNN Layer (GRU/LSTM/SimpleRNN) with return_sequences=True.\n",
    "        The dimensions are inferred based on the output shape of the RNN.\n",
    "        Example:\n",
    "            model.add(LSTM(64, return_sequences=True))\n",
    "            model.add(AttentionWithContext())\n",
    "        \"\"\"\n",
    "\n",
    "    def __init__(self,\n",
    "                 W_regularizer=None, u_regularizer=None, b_regularizer=None,\n",
    "                 W_constraint=None, u_constraint=None, b_constraint=None,\n",
    "                 bias=True, **kwargs):\n",
    "\n",
    "        self.supports_masking = True\n",
    "        self.init = initializations.get('glorot_uniform')\n",
    "\n",
    "        self.W_regularizer = regularizers.get(W_regularizer)\n",
    "        self.u_regularizer = regularizers.get(u_regularizer)\n",
    "        self.b_regularizer = regularizers.get(b_regularizer)\n",
    "\n",
    "        self.W_constraint = constraints.get(W_constraint)\n",
    "        self.u_constraint = constraints.get(u_constraint)\n",
    "        self.b_constraint = constraints.get(b_constraint)\n",
    "\n",
    "        self.bias = bias\n",
    "        super(AttentionWithContext, self).__init__(**kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape) == 3\n",
    "\n",
    "        self.W = self.add_weight((input_shape[-1], input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_W'.format(self.name),\n",
    "                                 regularizer=self.W_regularizer,\n",
    "                                 constraint=self.W_constraint)\n",
    "        if self.bias:\n",
    "            self.b = self.add_weight((input_shape[-1],),\n",
    "                                     initializer='zero',\n",
    "                                     name='{}_b'.format(self.name),\n",
    "                                     regularizer=self.b_regularizer,\n",
    "                                     constraint=self.b_constraint)\n",
    "\n",
    "        self.u = self.add_weight((input_shape[-1],),\n",
    "                                 initializer=self.init,\n",
    "                                 name='{}_u'.format(self.name),\n",
    "                                 regularizer=self.u_regularizer,\n",
    "                                 constraint=self.u_constraint)\n",
    "\n",
    "        super(AttentionWithContext, self).build(input_shape)\n",
    "\n",
    "    def compute_mask(self, input, input_mask=None):\n",
    "        # do not pass the mask to the next layers\n",
    "        return None\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        uit = K.dot(x, self.W)\n",
    "\n",
    "        if self.bias:\n",
    "            uit += self.b\n",
    "\n",
    "        uit = K.tanh(uit)\n",
    "        ait = K.dot(uit, self.u)\n",
    "\n",
    "        a = K.exp(ait)\n",
    "\n",
    "        # apply mask after the exp. will be re-normalized next\n",
    "        if mask is not None:\n",
    "            # Cast the mask to floatX to avoid float64 upcasting in theano\n",
    "            a *= K.cast(mask, K.floatx())\n",
    "\n",
    "        # in some cases especially in the early stages of training the sum may be almost zero\n",
    "        # and this results in NaN's. A workaround is to add a very small positive number  to the sum.\n",
    "        # a /= K.cast(K.sum(a, axis=1, keepdims=True), K.floatx())\n",
    "        a /= K.cast(K.sum(a, axis=1, keepdims=True) + K.epsilon(), K.floatx())\n",
    "\n",
    "        a = K.expand_dims(a)\n",
    "        weighted_input = x * a\n",
    "        return K.sum(weighted_input, axis=1)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return input_shape[0], input_shape[-1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAIN MODEL 3\n",
    "# Attention-based\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "model = Sequential()\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ---- Embedding layer -----------------------------------------------------------------\n",
    "embedding_layer = Embedding(top_words, \n",
    "                            embedding_vecor_length, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length = max_description_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "## LSTM 1\n",
    "## ======================================================================================\n",
    "LSTM_1 = LSTM(128,return_sequences=True)\n",
    "# model.add(LSTM_1)\n",
    "\n",
    "## Attention 1\n",
    "## ======================================================================================\n",
    "TimeDist_1 = TimeDistributed(Dense(200))  #, input_shape=(max_description_length, embedding_vecor_length)\n",
    "model.add(TimeDist_1)\n",
    "#model.add(AttentionWithContext())\n",
    "\n",
    "LSTM_2 = LSTM(128,return_sequences=False)\n",
    "model.add(LSTM_2)\n",
    "\n",
    "## Dense 1\n",
    "## ======================================================================================\n",
    "Dense_1 = Dense(128,activation='sigmoid')\n",
    "#model.add(Dense_1)\n",
    "\n",
    "## Output classes layer\n",
    "## ======================================================================================\n",
    "model.add(Dense(len(nb_classes), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try loss=categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(20):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx, y_ary_cat, nb_epoch=1, batch_size=batch_size_)\n",
    "    \n",
    "    scores = model.evaluate(train_texts_vec_mtx, y_ary_cat, verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))\n",
    "\n",
    "Accuracy: loss: 0.2677 - acc: 0.9152\n",
    "Valid accuracy:  0.923365750258"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "best_model_aux['Best score']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Nets design\n",
    "#from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "plot(model, to_file='/Users/altay.amanbay/Desktop/model.png')\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model 4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAIN MODEL 4\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "model = Sequential()\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ---- Embedding layer -----------------------------------------------------------------\n",
    "embedding_layer = Embedding(top_words, \n",
    "                            embedding_vecor_length, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length = max_description_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "## LSTM 1\n",
    "## ======================================================================================\n",
    "LSTM_1 = LSTM(128,return_sequences=True, activation='softmax')\n",
    "model.add(LSTM_1)\n",
    "\n",
    "## Attention 1\n",
    "## ======================================================================================\n",
    "TimeDist_1 = TimeDistributed(Dense(200))  #, input_shape=(max_description_length, embedding_vecor_length)\n",
    "model.add(TimeDist_1)\n",
    "#model.add(AttentionWithContext())\n",
    "\n",
    "LSTM_2 = LSTM(128,return_sequences=False)\n",
    "model.add(LSTM_2)\n",
    "\n",
    "## Output classes layer\n",
    "## ======================================================================================\n",
    "model.add(Dense(len(nb_classes), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try loss=categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(20):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx, y_ary_cat, nb_epoch=1, batch_size=batch_size_)\n",
    "    \n",
    "    scores = model.evaluate(train_texts_vec_mtx, y_ary_cat, verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))\n",
    "\n",
    "# Fails with acc: 0.0636"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model 5"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAIN MODEL 5\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "model = Sequential()\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ---- Embedding layer -----------------------------------------------------------------\n",
    "embedding_layer = Embedding(top_words, \n",
    "                            embedding_vecor_length, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length = max_description_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "## LSTM 1\n",
    "## ======================================================================================\n",
    "LSTM_1 = LSTM(128,return_sequences=True)\n",
    "#model.add(LSTM_1)\n",
    "\n",
    "## Attention 1\n",
    "## ======================================================================================\n",
    "TimeDist_1 = TimeDistributed(Dense(200))  #, input_shape=(max_description_length, embedding_vecor_length)\n",
    "model.add(TimeDist_1)\n",
    "#model.add(AttLayer())\n",
    "#model.add(AttentionWithContext())\n",
    "\n",
    "LSTM_2 = LSTM(128,return_sequences=True)\n",
    "model.add(LSTM_2)\n",
    "model.add(AttentionWithContext())\n",
    "## Output classes layer\n",
    "## ======================================================================================\n",
    "model.add(Dense(len(nb_classes), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try loss=categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(20):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx, y_ary_cat, nb_epoch=1, batch_size=batch_size_)\n",
    "    \n",
    "    scores = model.evaluate(train_texts_vec_mtx, y_ary_cat, verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))\n",
    "\n",
    "# Fails with acc: "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model 6"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAIN MODEL 6 \n",
    "# with word vect doc2vect_vec_64_win30__dict_sample_5000\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "model = Sequential()\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ---- Embedding layer -----------------------------------------------------------------\n",
    "embedding_layer = Embedding(top_words, \n",
    "                            embedding_vecor_length, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length = max_description_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "## LSTM 1\n",
    "## ======================================================================================\n",
    "LSTM_1 = LSTM(128,return_sequences=True)\n",
    "model.add(LSTM_1)\n",
    "\n",
    "## Attention 1\n",
    "## ======================================================================================\n",
    "TimeDist_1 = TimeDistributed(Dense(200))  #, input_shape=(max_description_length, embedding_vecor_length)\n",
    "model.add(TimeDist_1)\n",
    "model.add(AttLayer())\n",
    "\n",
    "## Output classes layer\n",
    "## ======================================================================================\n",
    "model.add(Dense(len(nb_classes), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try loss=categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(20):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx, y_ary_cat, nb_epoch=1, batch_size=batch_size_)\n",
    "    \n",
    "    scores = model.evaluate(train_texts_vec_mtx, y_ary_cat, verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))\n",
    "\n",
    "# Fails\n",
    "# ____________________________________________________________________________________________________\n",
    "# Layer (type)                     Output Shape          Param #     Connected to                     \n",
    "# ====================================================================================================\n",
    "# embedding_4 (Embedding)          (None, 30, 64)        15036864    embedding_input_4[0][0]          \n",
    "# ____________________________________________________________________________________________________\n",
    "# lstm_4 (LSTM)                    (None, 30, 128)       98816       embedding_4[0][0]                \n",
    "# ____________________________________________________________________________________________________\n",
    "# timedistributed_4 (TimeDistribut (None, 30, 200)       25800       lstm_4[0][0]                     \n",
    "# ____________________________________________________________________________________________________\n",
    "# attlayer_1 (AttLayer)            (None, 200)           200         timedistributed_4[0][0]          \n",
    "# ____________________________________________________________________________________________________\n",
    "# dense_7 (Dense)                  (None, 398)           79998       attlayer_1[0][0]                 \n",
    "# ====================================================================================================\n",
    "# Total params: 15,241,678\n",
    "# Trainable params: 204,814\n",
    "# Non-trainable params: 15,036,864\n",
    "# ____________________________________________________________________________________________________\n",
    "# None\n",
    "# Epoch iter #1\n",
    "# Epoch 1/1\n",
    "# 938810/938810 [==============================] - 1057s - loss: 1.0193 - acc: 0.7235  \n",
    "# Captured improved model\n",
    "# Valid accuracy:  0.801597767386"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model 7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# TRAIN MODEL 6 \n",
    "# with word word2vect_unigrams_interrelations__vec64_win1__dict_sample_5000\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "model = Sequential()\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ---- Embedding layer -----------------------------------------------------------------\n",
    "embedding_layer = Embedding(top_words, \n",
    "                            embedding_vecor_length, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length = max_description_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "## LSTM 1\n",
    "## ======================================================================================\n",
    "LSTM_1 = LSTM(128,return_sequences=False, dropout_W = 0.3, dropout_U = 0.3)\n",
    "model.add(LSTM_1)\n",
    "\n",
    "## Attention 1\n",
    "## ======================================================================================\n",
    "TimeDist_1 = TimeDistributed(Dense(200))  #, input_shape=(max_description_length, embedding_vecor_length)\n",
    "#model.add(TimeDist_1)\n",
    "#model.add(AttLayer())\n",
    "\n",
    "## Output classes layer\n",
    "## ======================================================================================\n",
    "model.add(Dense(len(nb_classes), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try loss=categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(20):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx, y_ary_cat, nb_epoch=1, batch_size=batch_size_)\n",
    "    \n",
    "    scores = model.evaluate(train_texts_vec_mtx, y_ary_cat, verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Model 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from keras import backend as K\n",
    "from keras.layers import LSTM\n",
    "\n",
    "class AttentionLSTM(LSTM):\n",
    "    def __init__(self, output_dim, attention_vec, **kwargs):\n",
    "        self.attention_vec = attention_vec\n",
    "        super(AttentionLSTM, self).__init__(output_dim, **kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        super(AttentionLSTM, self).build(input_shape)\n",
    "\n",
    "        assert hasattr(self.attention_vec, '_keras_shape')\n",
    "        attention_dim = self.attention_vec._keras_shape[1]\n",
    "\n",
    "        self.U_a = self.inner_init((self.output_dim, self.output_dim),\n",
    "                                   name='{}_U_a'.format(self.name))\n",
    "        self.b_a = K.zeros((self.output_dim,), name='{}_b_a'.format(self.name))\n",
    "\n",
    "        self.U_m = self.inner_init((attention_dim, self.output_dim),\n",
    "                                   name='{}_U_m'.format(self.name))\n",
    "        self.b_m = K.zeros((self.output_dim,), name='{}_b_m'.format(self.name))\n",
    "\n",
    "        self.U_s = self.inner_init((self.output_dim, self.output_dim),\n",
    "                                   name='{}_U_s'.format(self.name))\n",
    "        self.b_s = K.zeros((self.output_dim,), name='{}_b_s'.format(self.name))\n",
    "\n",
    "        self.trainable_weights += [self.U_a, self.U_m, self.U_s,\n",
    "                                   self.b_a, self.b_m, self.b_s]\n",
    "\n",
    "        if self.initial_weights is not None:\n",
    "            self.set_weights(self.initial_weights)\n",
    "            del self.initial_weights\n",
    "\n",
    "    def step(self, x, states):\n",
    "        h, [h, c] = super(AttentionLSTM, self).step(x, states)\n",
    "        attention = states[4]\n",
    "\n",
    "        m = K.tanh(K.dot(h, self.U_a) + attention + self.b_a)\n",
    "        s = K.exp(K.dot(m, self.U_s) + self.b_s)\n",
    "        h = h * s\n",
    "\n",
    "        return h, [h, c]\n",
    "\n",
    "    def get_constants(self, x):\n",
    "        constants = super(AttentionLSTM, self).get_constants(x)\n",
    "        constants.append(K.dot(self.attention_vec, self.U_m) + self.b_m)\n",
    "        return constants"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAIN MODEL 8\n",
    "# embeddings source: word2vect_class_specific_unigrams__vec64_win1__dict_sample_5000\n",
    "from keras.layers import Lambda, Input\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "model = Sequential()\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ---- Embedding layer -----------------------------------------------------------------\n",
    "embedding_layer = Embedding(top_words, \n",
    "                            embedding_vecor_length, \n",
    "                            weights=[embedding_matrix], \n",
    "                            input_length = max_description_length,\n",
    "                            trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.add(BatchNormalization(axis=1))\n",
    "LSTM_1 = LSTM(128,return_sequences=False)\n",
    "model.add(LSTM_1)\n",
    "#model.add(Flatten())\n",
    "#model.add(Dense(1920))\n",
    "#model.add(Dense(960))\n",
    "#model.add(Dense(480))\n",
    "#model.add(Dense(240))\n",
    "#model.add(Dense(120))\n",
    "#model.add(Dense(240))\n",
    "\n",
    "#model.add(Lambda(lambda x: 2 * x))\n",
    "#model.add(MultiplicationLayer())\n",
    "#model.add(AttentionWithContext())\n",
    "#model.add(AttLayer())\n",
    "\n",
    "## LSTM 1\n",
    "## ======================================================================================\n",
    "LSTM_1 = LSTM(64,return_sequences=False, dropout_W = 0.3, dropout_U = 0.3)\n",
    "#model.add(LSTM_1)\n",
    "\n",
    "## Attention 1\n",
    "## ======================================================================================\n",
    "TimeDist_1 = TimeDistributed(Dense(200))  #, input_shape=(max_description_length, embedding_vecor_length)\n",
    "#model.add(TimeDist_1)\n",
    "\n",
    "## Output classes layer\n",
    "## ======================================================================================\n",
    "model.add(Dense(len(nb_classes), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try loss=categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(20):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx, y_ary_cat, nb_epoch=1, batch_size=batch_size_)\n",
    "    \n",
    "    scores = model.evaluate(train_texts_vec_mtx, y_ary_cat, verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Save model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Save model and aux file\n",
    "\n",
    "best_model.save('keras_model__LSTM_32__val_accuracy_94.h5')\n",
    "\n",
    "best_model_aux_name = 'keras_model__LSTM_32__val_accuracy_94.pkl'\n",
    "with open(best_model_aux_name, 'wb') as pickle_file:\n",
    "    pickle.dump(best_model_aux, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluation (optional)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Final evaluation of the model\n",
    "start = time.time()\n",
    "\n",
    "scores = model.evaluate(train_texts_vec_mtx, y_train_ary, verbose=0)\n",
    "print(\"Accuracy on train set: %.2f%%\" % (scores[1]*100))\n",
    "scores = model.evaluate(test_texts_vec_mtx, y_test_ary, verbose=0)\n",
    "print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "\n",
    "print(\"\\nEvaluation took %g s\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Get predictions\n",
    "start = time.time()\n",
    "\n",
    "predictions = model.predict_classes(test_texts_vec_mtx)\n",
    "#predictions_rnd = np.round_(predictions, decimals=0, out=None)\n",
    "predictions_probs = model.predict(test_texts_vec_mtx)\n",
    "\n",
    "print('%-20s' % \"predictions[0]\",':', predictions[0])\n",
    "#print('%-20s' % \"predictions_rnd[0]:\",':',predictions_rnd[0])\n",
    "#print('%-20s' % \"predictions_probs[0]\",':', predictions_probs[0])\n",
    "print(\"\\nPrediction took %g s\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Manual check which nodes are not distinguishable for nets\n",
    "class_ = 392\n",
    "print(le.inverse_transform(class_))\n",
    "print(X_test_ls[np_utils.categorical_probas_to_classes(y_test_ary)==class_],'\\n')\n",
    "\n",
    "class_ = 390\n",
    "print(le.inverse_transform(class_))\n",
    "print(X_test_ls[np_utils.categorical_probas_to_classes(y_test_ary)==class_],'\\n')\n",
    "\n",
    "class_ = 389\n",
    "print(le.inverse_transform(class_))\n",
    "print(X_test_ls[np_utils.categorical_probas_to_classes(y_test_ary)==class_],'\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#pd.crosstab(pd.Series(y_test_ary.ravel()), pd.Series(predictions_rnd.ravel()), rownames=['True'], colnames=['Predicted'], margins=True)\n",
    "pd.crosstab(pd.Series(np_utils.categorical_probas_to_classes(y_test_ary)), pd.Series(predictions), rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "pd.crosstab(pd.Series(np_utils.categorical_probas_to_classes(y_test_ary)), pd.Series(predictions.ravel()), rownames=['True'], colnames=['Predicted'], margins=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#model.get_config()\n",
    "def prediction_to_str(clf_prediction, category_id):\n",
    "    if(clf_prediction > 0.5):\n",
    "        return str(category_id)\n",
    "    else:\n",
    "        return 'not ' + str(category_id)\n",
    "\n",
    "def predict(description_str, word_index, clf_, max_length_, category_id_):\n",
    "    #seq_ = tok_.texts_to_sequences([description_str])\n",
    "    seq_ = texts_to_sequences_custom([description_str.lower()], word_index)\n",
    "    seq_pad = sequence.pad_sequences(seq_, maxlen = max_length_)\n",
    "    #clf_prediction = clf_.predict(seq_pad)\n",
    "    clf_prediction = old_best_model_.predict_classes(seq_pad, verbose=0)\n",
    "    \n",
    "    #print(seq_)\n",
    "    #print(seq_pad)\n",
    "    \n",
    "    # Prediction to string\n",
    "    #clf_prediction_str = prediction_to_str(clf_prediction[0][0], category_id_)\n",
    "    clf_prediction_str = clf_prediction\n",
    "    \n",
    "    return clf_prediction_str\n",
    "    #return clf_prediction[0][0]\n",
    "\n",
    "def predict_2(description_str, word_index, clf_, max_length_, category_id_):\n",
    "    #seq_ = tok_.texts_to_sequences([description_str])\n",
    "    seq_ = texts_to_sequences_custom([description_str.lower()], word_index)\n",
    "    seq_pad = sequence.pad_sequences(seq_, maxlen = max_length_)\n",
    "    #clf_prediction = clf_.predict(seq_pad)\n",
    "    clf_prediction = model.predict_classes(seq_pad)\n",
    "    \n",
    "    #print(seq_)\n",
    "    #print(seq_pad)\n",
    "    \n",
    "    # Prediction to string\n",
    "    #clf_prediction_str = prediction_to_str(clf_prediction[0][0], category_id_)\n",
    "    clf_prediction = le.inverse_transform(clf_prediction)\n",
    "    \n",
    "    if(clf_prediction == ['Positive']):\n",
    "        return str(category_id_)\n",
    "    else:\n",
    "        return 'not ' + str(category_id_)\n",
    "    \n",
    "    \n",
    "def predict_proba(description_str, word_index, clf_, max_length_):\n",
    "    #seq_ = tok_.texts_to_sequences([description_str])\n",
    "    seq_ = texts_to_sequences_custom([description_str], word_index)\n",
    "    seq_pad = sequence.pad_sequences(seq_, maxlen = max_length_)\n",
    "    clf_prediction_proba = clf_.predict_proba(seq_pad, verbose=0)\n",
    "    \n",
    "    return clf_prediction_proba[0][0]\n",
    "\n",
    "\n",
    "# id_ = 'table Setr'\n",
    "# p = predict(id_, best_model_aux['Tokenizer'], best_model, best_model_aux['Max length'], best_model_aux['Category ID'])\n",
    "# pp = predict_proba(id_, best_model_aux['Tokenizer'], best_model, best_model_aux['Max length'])\n",
    "# print(p)\n",
    "# print(pp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model_file = \"whole set train model/nets_model__word2vec_class_specific_unigrams.h5\"\n",
    "aux_file = \"whole set train model/nets_model__word2vec_class_specific_unigrams_aux.pkl\"\n",
    "old_best_model_ = load_model(model_file)\n",
    "old_best_model_aux_ = get_model_file_aux(aux_file)\n",
    "\n",
    "old_word_index_ = old_best_model_aux_['word_index']\n",
    "le = old_best_model_aux_['Label encoder']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "predictions = old_best_model_.predict_classes(train_texts_vec_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_confusion_matrix(cm, classes,\n",
    "                          normalize=False,\n",
    "                          title='Confusion matrix',\n",
    "                          cmap=plt.cm.Blues):\n",
    "    \"\"\"\n",
    "    This function prints and plots the confusion matrix.\n",
    "    Normalization can be applied by setting `normalize=True`.\n",
    "    \"\"\"\n",
    "    plt.imshow(cm, interpolation='nearest', cmap=cmap)\n",
    "    plt.title(title)\n",
    "    plt.colorbar()\n",
    "    tick_marks = np.arange(len(classes))\n",
    "    plt.xticks(tick_marks, classes, rotation=90)\n",
    "    plt.yticks(tick_marks, classes)\n",
    "\n",
    "    if normalize:\n",
    "        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]\n",
    "        print(\"Normalized confusion matrix\")\n",
    "    else:\n",
    "        print('Confusion matrix, without normalization')\n",
    "\n",
    "    print(cm)\n",
    "\n",
    "    thresh = cm.max() / 2.\n",
    "    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):\n",
    "        plt.text(j, i, cm[i, j],\n",
    "                 horizontalalignment=\"center\",\n",
    "                 color=\"white\" if cm[i, j] > thresh else \"black\")\n",
    "\n",
    "    plt.tight_layout()\n",
    "    plt.ylabel('True label')\n",
    "    plt.xlabel('Predicted label')\n",
    "    \n",
    "\n",
    "# Compute confusion matrix\n",
    "cnf_matrix = confusion_matrix(np_utils.categorical_probas_to_classes(y_ary_cat), predictions)\n",
    "np.set_printoptions(precision=2)\n",
    "\n",
    "#class_names = [ix_to_class[i] for i in range(101)]\n",
    "class_names = predictions\n",
    "\n",
    "plt.figure()\n",
    "fig = plt.gcf()\n",
    "fig.set_size_inches(32, 32)\n",
    "plot_confusion_matrix(cnf_matrix, classes=class_names,\n",
    "                      title='Confusion matrix, without normalization',\n",
    "                      cmap=plt.cm.cool)\n",
    "plt.show()\n",
    "\n",
    "pd.crosstab(pd.Series(np_utils.categorical_probas_to_classes(y_ary_cat)), pd.Series(predictions), rownames=['True'], colnames=['Predicted'], margins=True)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "X_ls = np.array(list(train_df['description_mod1']))\n",
    "seq_ = texts_to_sequences_custom(X_ls, old_word_index_)\n",
    "seq_pad = sequence.pad_sequences(seq_, maxlen = 30)\n",
    "\n",
    "old_best_model_ = best_model\n",
    "predictions = old_best_model_.predict_classes(seq_pad)\n",
    "\n",
    "train_df['Predictions_le'] = list(predictions)\n",
    "train_df['Predictions'] = train_df['Predictions_le'].apply(lambda x: le.inverse_transform(x))\n",
    "train_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_df.to_csv('Predictions__unigrams_interrelations.csv',index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "aveeno baby wash  shampoo lightly scented 8 ounce pack of 2\n",
      "Baby Products > Bathing & Skin Care > Soaps & Cleansers\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_mod1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>991</th>\n",
       "      <td>bosch dcb724b3 714inch by 24t framing circular...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1273</th>\n",
       "      <td>heavy metal bluray disc</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2444</th>\n",
       "      <td>chemical guys bufhexkits8p  hexlogic buffing p...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4259</th>\n",
       "      <td>greenworks 20672 gmax 40v liion 8inch cordless...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4310</th>\n",
       "      <td>cd 5 pan wip tc</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                       description_mod1\n",
       "991   bosch dcb724b3 714inch by 24t framing circular...\n",
       "1273                            heavy metal bluray disc\n",
       "2444  chemical guys bufhexkits8p  hexlogic buffing p...\n",
       "4259  greenworks 20672 gmax 40v liion 8inch cordless...\n",
       "4310                                    cd 5 pan wip tc"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(samples_df.loc[0,'description_mod1'])\n",
    "print(samples_df.loc[0,'category_full_path_mod1'])\n",
    "\n",
    "samples_df.loc[samples_df.category_full_path_mod1.str.contains('Hand Tools'),['description_mod1']].head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dict_keys(['Label encoder', 'Max length', 'texts_to_sequences', 'word_index', 'Best score'])\n",
      "0 ====================================================================================================\n",
      "Old model prediction:\n",
      "item: aveeno baby wash  shampoo lightly scented 8 ounce pack of 2\n",
      "Seq max len: 30\n",
      "[169]\n",
      "['Baby Products > Bathing & Skin Care > Soaps & Cleansers']\n",
      "8.36832e-28\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# TEST\n",
    "\n",
    "## load old model\n",
    "model_file = \"LSTM_16 w val_accuracy 92/keras_model__LSTM_16__val_acc_92.h5\"\n",
    "aux_file = \"LSTM_16 w val_accuracy 92/keras_model__LSTM_16__val_acc_92.pkl\"\n",
    "old_best_model_ = load_model(model_file)\n",
    "old_best_model_aux_ = get_model_file_aux(aux_file)\n",
    "print(old_best_model_aux_.keys())\n",
    "#old_tok_ = old_best_model_aux_['Tokenizer']\n",
    "#old_word_index_ = old_best_model_aux_['Tokenizer'].word_index\n",
    "old_word_index_ = old_best_model_aux_['word_index']\n",
    "texts_to_sequences_custom = old_best_model_aux_['texts_to_sequences']\n",
    "\n",
    "\n",
    "## use fresh model\n",
    "# best_model_ = best_model\n",
    "# best_model_aux_ = best_model_aux\n",
    "# tok_ = tok\n",
    "# word_index_ = word_index\n",
    "\n",
    "item_d = 'NieR: Automata DEMO 120161128 (Playable Demo)'\n",
    "\n",
    "# screwdrivers check\n",
    "scrw_items = [\n",
    "\"aveeno baby wash  shampoo lightly scented 8 ounce pack of 2\"\n",
    "#,\"tekton 2655 flare nut wrench set metric 6piece\"\n",
    "#,\"tekton 2780 10slot screwdriver holder and organizer\"\n",
    "#,\"titan 17237 insulated electrical screwdriver set  7 piece\"\n",
    "#,\"tool sorter screwdriver organizer red\"\n",
    "#,\"torin sdh15rt magnetic screwdriver holder\"  #wrong predict\n",
    "#,\"wera 05020013001 joker combination wrenchset 11 pieces\"\n",
    "#,\"wera kk vde 60i62i68i18 insulated pouch set with interchangeable blades 18piece\" # tricky\n",
    "#,\"wiha 28103 magnetic 14 bit holder stubby 57mm pliers screwdriver\" # tricky, wrong predict\n",
    "]\n",
    "\n",
    "for n, i in enumerate(scrw_items):\n",
    "    item_d = i\n",
    "    \n",
    "    print(str(n) + ' ' + '='*100)\n",
    "    \n",
    "    print('Old model prediction:')\n",
    "    print('item:',item_d)\n",
    "    print('Seq max len:', old_best_model_aux_['Max length'])\n",
    "    pred = predict(item_d, old_word_index_, old_best_model_, old_best_model_aux_['Max length'], '927')\n",
    "    print(pred)\n",
    "    print(old_best_model_aux_['Label encoder'].inverse_transform([pred])[0])\n",
    "    print(predict_proba(item_d, old_word_index_, old_best_model_, old_best_model_aux_['Max length']))\n",
    "\n",
    "\n",
    "#     print('\\nFresh model prediction:')\n",
    "#     print('item:',item_d)\n",
    "#     print('Seq max len:', best_model_aux_['Max length'])\n",
    "#     print(predict_2(item_d, tok_, best_model_, best_model_aux_['Max length'], '927'))\n",
    "#     #print(predict_proba(item_d, tok_, best_model_, best_model_aux_['Max length']))\n",
    "\n",
    "    print()\n",
    "\n",
    "    #tt = train_df.loc[0:10,['description_mod1']]\n",
    "    #tt['pred'] = tt['description_mod1'].apply(lambda x: predict(x, best_model_aux_['Tokenizer'], best_model, best_model_aux_['Max length'], best_model_aux_['Category ID']))\n",
    "    #tt['prob'] = tt['description_mod1'].apply(lambda x: predict_proba(x, best_model_aux_['Tokenizer'], best_model, best_model_aux_['Max length']))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "12"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "w = old_best_model_.layers[1].get_weights()\n",
    "len(w)\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
