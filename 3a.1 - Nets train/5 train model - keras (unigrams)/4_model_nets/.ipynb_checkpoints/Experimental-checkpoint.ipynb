{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Keras \n",
    "# Classifier: LSTM \n",
    "# Classification type: multi-class (404 classes)\n",
    "# Output nodes: #of classes with softmax\n",
    "\n",
    "# word2vec model: \n",
    "# word2vect_class_specififc__vec64_win1__dict_sample_5000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing process_string.py \n",
      "from /Users/altay.amanbay/Desktop/new node booster/experiments/3a.1 - Nets train/5 train model - keras (unigrams)/2_common_aux_script ...\n",
      "\n",
      "3.5.2 |Anaconda 4.2.0 (x86_64)| (default, Jul  2 2016, 17:52:12) \n",
      "[GCC 4.2.1 Compatible Apple LLVM 4.2 (clang-425.0.28)]\n"
     ]
    }
   ],
   "source": [
    "from __future__ import print_function\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import time\n",
    "\n",
    "from keras.datasets import reuters\n",
    "from keras.wrappers.scikit_learn import KerasClassifier\n",
    "from keras.layers.wrappers import TimeDistributed\n",
    "from keras.models import load_model, Sequential\n",
    "from keras.layers.embeddings import Embedding\n",
    "from keras.layers import Dense, Dropout, Input, Activation, LSTM, Bidirectional, Flatten, Lambda, RepeatVector\n",
    "from keras.layers.recurrent import SimpleRNN\n",
    "from keras.layers.convolutional import Conv1D\n",
    "from keras.layers.pooling import MaxPooling1D\n",
    "from keras.layers.noise import GaussianNoise\n",
    "from keras.utils import np_utils\n",
    "from keras.preprocessing.text import Tokenizer, one_hot, text_to_word_sequence\n",
    "from keras.preprocessing import sequence\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializations\n",
    "from keras import regularizers\n",
    "from keras import constraints\n",
    "from keras.layers.normalization import BatchNormalization\n",
    "from keras.callbacks import EarlyStopping\n",
    "\n",
    "\n",
    "from keras.utils.visualize_util import plot\n",
    "from IPython.display import SVG\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "import os    \n",
    "os.environ['THEANO_FLAGS'] = \"device=gpu0\" \n",
    "import theano\n",
    "#theano.config.device = 'gpu0'\n",
    "theano.config.floatX = 'float32'\n",
    "\n",
    "\n",
    "# import custom code\n",
    "import os\n",
    "import sys\n",
    "pardir = os.path.abspath(os.path.join(os.getcwd(), '../'))\n",
    "script_path = pardir + \"/2_common_aux_script\"\n",
    "print('Importing process_string.py \\nfrom ' + script_path + \" ...\\n\")\n",
    "sys.path.append(script_path)\n",
    "from process_string import process_string\n",
    "sys.path.remove(script_path)\n",
    "\n",
    "\n",
    "print(sys.version)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.8.2\n"
     ]
    }
   ],
   "source": [
    "import keras\n",
    "print(theano.__version__)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Functions\n",
    "\n",
    "def NGramGenerator_wordwise_interval(phrase, min_ngram, max_ngram):\n",
    "    all_ngram_lists = []\n",
    "\n",
    "    #printable_ = 'abcdefghijklmnopqrstuvwxyz0123456789 '\n",
    "    #s_split = \"\".join((char if char in printable_ else \"\") for char in phrase).split()\n",
    "    phrase_processed = process_string(phrase)\n",
    "    s_split = phrase_processed.split()\n",
    "    \n",
    "    for n in range(max_ngram, min_ngram - 1, -1):\n",
    "        n_gram = [s_split[i:i+n] for i in range(len(s_split)-n+1)]\n",
    "        all_ngram_lists.extend(n_gram)\n",
    "        \n",
    "    all_ngrams = []\n",
    "    for n_gram in all_ngram_lists:\n",
    "        all_ngrams.extend([' '.join(n_gram)])\n",
    "    \n",
    "    return all_ngrams\n",
    "\n",
    "def get_word2index(texts_ls_):\n",
    "    word2index_ = {}\n",
    "\n",
    "    c = 1\n",
    "    for text_str in texts_ls_:\n",
    "        text_tokens_ls = text_str.lower().split()\n",
    "        for token in text_tokens_ls:\n",
    "            if(token not in word2index_):\n",
    "                word2index_[token] = c\n",
    "                c = c + 1\n",
    "                \n",
    "    return word2index_\n",
    "\n",
    "def train_df_preprocess(top_words_, texts_ls_, max_pad_length_):\n",
    "    # texts_ls_: list of texts strings\n",
    "    \n",
    "    tok = Tokenizer(top_words_)\n",
    "    tok.fit_on_texts(texts_ls_)\n",
    "\n",
    "    words = []\n",
    "    for iter in range(top_words):\n",
    "        words += [key for key,value in tok.word_index.items() if value==iter+1]\n",
    "\n",
    "    #Class for vectorizing texts, or/and turning texts into sequences \n",
    "    #(=list of word indexes, where the word of rank i in the dataset (starting at 1) has index i).\n",
    "    texts_vec_ls = tok.texts_to_sequences(texts_ls_)#turns text to sequence, stating which word comes in what place\n",
    "    texts_vec_mtx = sequence.pad_sequences(texts_vec_ls, maxlen=max_pad_length_)#pad sequence, essentially padding it with 0's at the end\n",
    "    \n",
    "    return texts_vec_mtx\n",
    "\n",
    "def text_2_vec(text_str, word2index_):\n",
    "    # text_str: text string\n",
    "    \n",
    "    text_tokens_ls = text_str.lower().split()\n",
    "    \n",
    "    text_vec = []\n",
    "    for token in text_tokens_ls:\n",
    "        if token in word2index_:\n",
    "            text_vec.append(word2index_[token])\n",
    "        else:\n",
    "            text_vec.append(0)\n",
    "            \n",
    "    return text_vec\n",
    "\n",
    "def train_df_preprocess_2(texts_ls_, word2index_, max_pad_length_):\n",
    "    # texts_ls_: list of texts strings\n",
    "    \n",
    "    texts_vec_ls = []\n",
    "    for text_ in texts_ls_:\n",
    "        #print(text_)\n",
    "        #print(type(text_))\n",
    "        text_vec = text_2_vec(text_, word2index_)\n",
    "        texts_vec_ls.append(text_vec)\n",
    "    \n",
    "    texts_vec_ary = sequence.pad_sequences(texts_vec_ls, maxlen=max_pad_length_)\n",
    "    \n",
    "    return texts_vec_ary\n",
    "\n",
    "def texts_to_sequences_custom(texts_ls, word_index_, new_words_to_zero = False):\n",
    "    texts_seq = []\n",
    "    \n",
    "    for text in texts_ls:\n",
    "        #text_split = text.lower().split()\n",
    "        text_split = NGramGenerator_wordwise_interval(text,1,1)\n",
    "        seq = []\n",
    "        for token in text_split:\n",
    "            if(token in word_index_):\n",
    "                seq.append(word_index_[token])\n",
    "            elif(new_words_to_zero):\n",
    "                seq.append(0)\n",
    "                \n",
    "        texts_seq.append(seq)\n",
    "#         for k,v in word_index_.items():\n",
    "#             if(v == 395):\n",
    "#                 print(k,v)\n",
    "    return texts_seq\n",
    "\n",
    "\n",
    "def get_model_file_aux(model_file_aux_name):\n",
    "    with open(model_file_aux_name, 'rb') as pickle_file:\n",
    "        model_file_aux = pickle.load(pickle_file)\n",
    "    return model_file_aux\n",
    "\n",
    "def clone_tokens(text_str, cloning_factor = 1):\n",
    "    token_clones_ls = []\n",
    "    for token in text_str.split():\n",
    "        token_clones_ls.extend([token]*cloning_factor)\n",
    "    return ' '.join(token_clones_ls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "samples data shape: (587983, 4)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_mod1</th>\n",
       "      <th>category_id_mod1</th>\n",
       "      <th>category_full_path_mod1</th>\n",
       "      <th>description_cloned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aveeno baby wash  shampoo lightly scented 8 ou...</td>\n",
       "      <td>206</td>\n",
       "      <td>Baby Products &gt; Bathing &amp; Skin Care &gt; Soaps &amp; ...</td>\n",
       "      <td>aveeno aveeno aveeno aveeno aveeno baby baby b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>earths best chlorine free diapers size 4 120 c...</td>\n",
       "      <td>213</td>\n",
       "      <td>Baby Products &gt; Diapering &gt; Disposable Diapers</td>\n",
       "      <td>earths earths earths earths earths best best b...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>doa5lr costume catalog lr10 addon content</td>\n",
       "      <td>320</td>\n",
       "      <td>Electronics &amp; Accessories &gt; Video Games &gt; Other</td>\n",
       "      <td>doa5lr doa5lr doa5lr doa5lr doa5lr costume cos...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>lightweight oxford l in slim and white</td>\n",
       "      <td>152</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Tops &amp; Tees ...</td>\n",
       "      <td>lightweight lightweight lightweight lightweigh...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>now foods nutritional yeast flakes 10ounce</td>\n",
       "      <td>525</td>\n",
       "      <td>Health &amp; Beauty &gt; Vitamins &amp; Dietary Supplements</td>\n",
       "      <td>now now now now now foods foods foods foods fo...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    description_mod1 category_id_mod1  \\\n",
       "0  aveeno baby wash  shampoo lightly scented 8 ou...              206   \n",
       "1  earths best chlorine free diapers size 4 120 c...              213   \n",
       "2          doa5lr costume catalog lr10 addon content              320   \n",
       "3             lightweight oxford l in slim and white              152   \n",
       "4         now foods nutritional yeast flakes 10ounce              525   \n",
       "\n",
       "                             category_full_path_mod1  \\\n",
       "0  Baby Products > Bathing & Skin Care > Soaps & ...   \n",
       "1     Baby Products > Diapering > Disposable Diapers   \n",
       "2    Electronics & Accessories > Video Games > Other   \n",
       "3  Apparel & Accessories > Apparel > Tops & Tees ...   \n",
       "4   Health & Beauty > Vitamins & Dietary Supplements   \n",
       "\n",
       "                                  description_cloned  \n",
       "0  aveeno aveeno aveeno aveeno aveeno baby baby b...  \n",
       "1  earths earths earths earths earths best best b...  \n",
       "2  doa5lr doa5lr doa5lr doa5lr doa5lr costume cos...  \n",
       "3  lightweight lightweight lightweight lightweigh...  \n",
       "4  now now now now now foods foods foods foods fo...  "
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read sampled descriptionary\n",
    "\n",
    "path = pardir+'/1_data/'\n",
    "file_name = 'sampled_descriptionary_sample_size_5000.csv'\n",
    "file_name = 'scorecards_for_fasttext.csv'\n",
    "samples_df = pd.read_csv(path + file_name)\n",
    "\n",
    "# Rename columns\n",
    "samples_df.rename(columns={\n",
    "                           #'description': 'description_mod1', \n",
    "                           '\\ufeff\"description\"': 'description_mod1',\n",
    "                           'category_id': 'category_id_mod1',\n",
    "                           'category_path': 'category_full_path_mod1'}, inplace=True)\n",
    "\n",
    "# Drop rows with NaN in any column\n",
    "samples_df.dropna()\n",
    "\n",
    "# Process description_mod1 strings by process_string function\n",
    "samples_df['description_mod1'] = samples_df['description_mod1'].apply(lambda x: process_string(x))\n",
    "\n",
    "# Drop rows where token count less than 1 in description_mod1 column\n",
    "selected_indices = samples_df['description_mod1'].apply(lambda x: len(str(x).split()) > 1)\n",
    "samples_df = samples_df[selected_indices]\n",
    "\n",
    "# Drop duplicates\n",
    "samples_df.drop_duplicates(subset=['description_mod1','category_full_path_mod1'], inplace = True, keep='first')\n",
    "samples_df.drop_duplicates(subset=['description_mod1'], inplace = True, keep=False)\n",
    "samples_df.reset_index(drop=True, inplace=True) \n",
    "\n",
    "# Clone tokens\n",
    "samples_df['description_cloned'] = samples_df['description_mod1'].apply(lambda x: clone_tokens(x, cloning_factor=5))\n",
    "\n",
    "# Drop 'screwdrivers' from descriptionary\n",
    "#samples_df = samples_df.loc[samples_df.category_id_mod1 != 927,:]\n",
    "\n",
    "# Drop index column\n",
    "#samples_df.drop(labels=['index'], axis=1, inplace=True)\n",
    "\n",
    "print('samples data shape:',samples_df.shape)\n",
    "samples_df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweak unbiased data (drop classes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Photos & Paper Products                                  50400\n",
       "Books                                                    33416\n",
       "Other                                                    29343\n",
       "Health & Beauty > Makeup                                 24738\n",
       "Online Services                                          22120\n",
       "Health & Beauty > Vitamins & Dietary Supplements         20874\n",
       "Jewelry & Watches > Earrings > Women                     18535\n",
       "Apparel & Accessories > Apparel > Tops & Tees > Women    13440\n",
       "Pet Supplies > Dogs & Cats > Dog Food & Treats           11791\n",
       "Grocery & Gourmet Food > Restaurant & Takeout            10333\n",
       "Name: category_full_path_mod1, dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Check for class count\n",
    "\n",
    "stats_sr = samples_df[\"category_full_path_mod1\"].value_counts()\n",
    "stats_sr[0:10]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Pruned samples data shape: (484432, 4)\n"
     ]
    }
   ],
   "source": [
    "# Delete certain categories\n",
    "\n",
    "samples_df = samples_df.loc[samples_df['category_full_path_mod1'].str.contains(\"Other\")==False,:]\n",
    "samples_df = samples_df.loc[samples_df.category_full_path_mod1 != \"Books\",:]\n",
    "#samples_df = samples_df.loc[samples_df.category_full_path_mod1 != \"Photos & Paper Products\",:]\n",
    "\n",
    "class_1 = 'Pet Supplies > Dogs & Cats > Dog Food & Treats'\n",
    "class_1 = 'Photos & Paper Products'\n",
    "class_2 = 'Apparel & Accessories > Apparel > Tops & Tees > Women'\n",
    "class_2 = 'Apparel & Accessories > Accessories > Luggage, Backpacks & Laptop Bags'\n",
    "classes_selected = class_1+'|'+class_2\n",
    "#samples_df = samples_df.loc[samples_df['category_full_path_mod1'].str.contains(classes_selected)==True,:]\n",
    "print('Pruned samples data shape:',samples_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Tweak unbiased data (equalize class numbers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Map class counts\n",
    "\n",
    "samples_df['count'] = samples_df['category_full_path_mod1'].map(stats_sr)\n",
    "samples_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## samples_df is unbiased by classes (category_full_path_mod1)\n",
    "## fix by picking N samples from each class\n",
    "\n",
    "#N=samples_df.category_full_path_mod1.value_counts(normalize=True).iloc[0] * samples_df.shape[0]\n",
    "samples_df = samples_df.sample(frac=1).groupby('category_full_path_mod1', sort=False).head(1000)\n",
    "print('samples data shape after picking max N samples from each class:',samples_df.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Concat sample_df into train_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>description_mod1</th>\n",
       "      <th>category_id_mod1</th>\n",
       "      <th>category_full_path_mod1</th>\n",
       "      <th>description_cloned</th>\n",
       "      <th>target_le</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>aveeno baby wash  shampoo lightly scented 8 ou...</td>\n",
       "      <td>206</td>\n",
       "      <td>Baby Products &gt; Bathing &amp; Skin Care &gt; Soaps &amp; ...</td>\n",
       "      <td>aveeno aveeno aveeno aveeno aveeno baby baby b...</td>\n",
       "      <td>169</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>earths best chlorine free diapers size 4 120 c...</td>\n",
       "      <td>213</td>\n",
       "      <td>Baby Products &gt; Diapering &gt; Disposable Diapers</td>\n",
       "      <td>earths earths earths earths earths best best b...</td>\n",
       "      <td>175</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                    description_mod1 category_id_mod1  \\\n",
       "0  aveeno baby wash  shampoo lightly scented 8 ou...              206   \n",
       "1  earths best chlorine free diapers size 4 120 c...              213   \n",
       "\n",
       "                             category_full_path_mod1  \\\n",
       "0  Baby Products > Bathing & Skin Care > Soaps & ...   \n",
       "1     Baby Products > Diapering > Disposable Diapers   \n",
       "\n",
       "                                  description_cloned  target_le  \n",
       "0  aveeno aveeno aveeno aveeno aveeno baby baby b...        169  \n",
       "1  earths earths earths earths earths best best b...        175  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Concat original train set and sampled descriptionary\n",
    "\n",
    "#train_df = pd.concat([train_df, samples_df], axis=0)\n",
    "train_df = samples_df\n",
    "#train_df.reset_index(drop=True, inplace=True)\n",
    "\n",
    "# description into chars\n",
    "#train_df['description_mod1'] = train_df['description_mod1'].apply(lambda x: process_string(x))\n",
    "\n",
    "# deduplicate\n",
    "#train_df.drop_duplicates(subset=['description_mod1'], inplace = True, keep=False)\n",
    "#print('train data shape (deduplicate):',train_df.shape)\n",
    "    \n",
    "# Encode target feature\n",
    "le = LabelEncoder()\n",
    "le.fit(train_df['category_full_path_mod1'])\n",
    "train_df['target_le'] = le.transform(train_df['category_full_path_mod1'])\n",
    "\n",
    "\n",
    "train_df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "string_w_max_tokens_1 = train_df['description_mod1'].map(lambda x: len(str(x).split())).max()\n",
    "string_w_max_tokens_2 = train_df['description_cloned'].map(lambda x: len(str(x).split())).max()\n",
    "print('string_w_max_tokens_1:',string_w_max_tokens_1)\n",
    "print('string_w_max_tokens_2:',string_w_max_tokens_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# test check\n",
    "i = 100\n",
    "print(train_texts_vec_mtx[i])\n",
    "len(word_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create input matrix for normal LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "484432\n",
      "(484432, 861)\n",
      "word_index size: 141483\n",
      "train_texts_vec_mtx shape: (484432, 30)\n"
     ]
    }
   ],
   "source": [
    "# Create input matrix for normal LSTM\n",
    "\n",
    "# Split into train and test\n",
    "X_ls = np.array(list(train_df['description_mod1']))\n",
    "#X_ls = np.array(list(train_df['description_cloned']))\n",
    "y_ary = np.array(list(train_df['target_le']))\n",
    "y_ary_cat = np_utils.to_categorical(train_df['target_le'])\n",
    "\n",
    "#X_train_ls, X_test_ls, y_train_ary, y_test_ary = train_test_split(X_ls, y_ary, test_size = 0.3)\n",
    "#X_train_ls, X_test_ls, y_train_ary, y_test_ary = train_test_split(X_ls, y_ary_cat, test_size = 0.3)\n",
    "print(len(X_ls))\n",
    "print(y_ary_cat.shape)\n",
    "\n",
    "\n",
    "top_words = None\n",
    "max_description_length = 30 #string_w_max_tokens_1\n",
    "\n",
    "tok = Tokenizer(nb_words = top_words)\n",
    "tok.fit_on_texts(X_ls)\n",
    "word_index = tok.word_index\n",
    "print('word_index size:',len(word_index))\n",
    "\n",
    "#train_texts_vec_ls = tok.texts_to_sequences(X_train_ls)\n",
    "train_texts_vec_ls = texts_to_sequences_custom(X_ls, word_index, new_words_to_zero = False)\n",
    "train_texts_vec_mtx = sequence.pad_sequences(train_texts_vec_ls, maxlen = max_description_length)\n",
    "\n",
    "print('train_texts_vec_mtx shape:',train_texts_vec_mtx.shape)\n",
    "list(word_index)[0:5]\n",
    "\n",
    "# Delete objects\n",
    "X_ls = None\n",
    "y_ary = None\n",
    "tok = None\n",
    "train_texts_vec_ls = None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create random embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "word index size: 141483\n",
      "embedding matrix shape: (141484, 32)\n",
      "[[ 0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.          0.          0.          0.\n",
      "   0.          0.          0.          0.        ]\n",
      " [ 0.49490242  0.23302929 -0.38785738 -0.39527143  0.44383371 -0.24519056\n",
      "   0.25464731 -0.03190993  0.32747988 -0.19694983  0.10502732  0.29460503\n",
      "  -0.08434479 -0.1714146   0.00165524 -0.24145589 -0.1944489   0.144096\n",
      "   0.21336866  0.11472475 -0.4821466  -0.43704531 -0.31667933  0.00870557\n",
      "   0.26015829  0.30997441 -0.27700669  0.44542248 -0.32381271 -0.34026166\n",
      "  -0.44787171  0.30137245]]\n"
     ]
    }
   ],
   "source": [
    "# Create RANDOM embedding vectors for each word in word index (lower cell code preferable)\n",
    "# fix random seed for reproducibility\n",
    "np.random.seed(12345) #8\n",
    "embeddings_source = 'random'\n",
    "\n",
    "embedding_vecor_length = 32\n",
    "uniq_token_count = len(word_index)\n",
    "print('word index size:', uniq_token_count)\n",
    "\n",
    "is_random_embeddings = True\n",
    "embedding_matrix = np.zeros((uniq_token_count + 1, embedding_vecor_length))\n",
    "if(is_random_embeddings == True):\n",
    "    for word, i in word_index.items():\n",
    "        #embedding_vector = np.random.uniform(.1, size=(1, embedding_vecor_length))\n",
    "        embedding_vector = np.random.uniform(-0.5, 0.5, embedding_vecor_length)\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "else:\n",
    "    c = 0\n",
    "    for word, i in word_index.items():\n",
    "        embedding_vector = np.random.uniform(c, c+100, size=(1, embedding_vecor_length))\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        c=c+6000\n",
    "    scaler = int('1'+'0'*len(str(c)))\n",
    "    embedding_matrix=embedding_matrix/scaler\n",
    "\n",
    "        \n",
    "print('embedding matrix shape:',embedding_matrix.shape)\n",
    "print(embedding_matrix[0:2])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Create pre-trained embeddings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Create word embeddings from trained Word2Vec model\n",
    "from gensim.models import word2vec, Phrases\n",
    "embeddings_source = 'word2vec'\n",
    "\n",
    "# Load model\n",
    "file_path_1_1 = pardir+\"/3_model_word2vec_vec64_win1__dict_sample_5000/word2vect_class_specific_unigrams__vec64_win1__dict_sample_5000\"\n",
    "file_path_1_2 = pardir+\"/3_model_word2vec_vec64_win1__dict_sample_5000/word2vect_class_specific_unigrams__vec128_win1__dict_sample_5000\"\n",
    "file_path_1_3 = pardir+\"/3_model_word2vec_vec64_win1__dict_sample_5000/word2vect_vec_64_win30__dict_sample_5000\"\n",
    "file_path_1_4 = pardir+\"/3_model_word2vec_vec64_win1__dict_sample_5000/word2vect_unigrams_interrelations__vec64_win1__dict_sample_5000\"\n",
    "file_path_1_5 = '/Users/altay.amanbay/Desktop/word2vec_new_trial/3_word2vec_modeling/word2vect_vec64_win1_sample10_iter100__dict_sample_5000'\n",
    "file_path_1_6 = '/Users/altay.amanbay/Desktop/word2vec_new_trial/3_word2vec_modeling/word2vect_class_specififc_unigrams__vec64_win1_sample0_iter1000__dict_sample_5000'\n",
    "file_path_1_7 = '/Users/altay.amanbay/Desktop/word2vec_new_trial/3_word2vec_modeling/word2vect_class_specififc_unigrams__vec64_win1_sample0_iter100__dict_sample_5000'\n",
    "file_path_1_8 = '/Users/altay.amanbay/Desktop/word2vec_new_trial/3_word2vec_modeling/word2vect_class_specififc_unigrams__vec64_win1_sample10_iter100__dict_sample_5000'\n",
    "\n",
    "file_path_1_9 ='/Users/altay.amanbay/Desktop/word2vec_new_trial/3_word2vec_modeling/word2vect_class_specififc_unigrams__vec64_win1_sample0.001_iter1000__dict_sample_5000'\n",
    "file_path_1_10 ='/Users/altay.amanbay/Desktop/word2vec_new_trial/3_word2vec_modeling/'\n",
    "#model = word2vec.Word2Vec.load(file_path_1_4)\n",
    "\n",
    "file_path_2_1 = pardir+\"/3_model_fasttext/fasttext__vec64_win5__dict_sampled_5000.vec\"\n",
    "file_path_2_2 = pardir+\"/3_model_fasttext/fasttext__vec64_win1__dict_sampled_5000.vec\"\n",
    "file_path_2_3 = pardir+\"/3_model_fasttext/fasttext__vec128_win1__dict_sampled_5000.vec\"\n",
    "file_path_2_4 = pardir+\"/3_model_fasttext/fasttext__vec400_win2__scorecards.vec\"\n",
    "model = word2vec.Word2Vec.load_word2vec_format(file_path_2_4) # for fasttext model\n",
    "\n",
    "#print(model.vocab.keys())\n",
    "#sys.exit()\n",
    "\n",
    "# word vector embeddings from model into dictionary\n",
    "word2vec_dict={}\n",
    "for word in model.vocab.keys():\n",
    "    try:\n",
    "        word2vec_dict[word]=model[word]\n",
    "    except:    \n",
    "        pass\n",
    "print('Loaded %s word vectors.' % len(word2vec_dict))\n",
    "    \n",
    "embedding_vecor_length = len(model[word])\n",
    "print('embedding_vecor_length:',embedding_vecor_length)\n",
    "\n",
    "\n",
    "embedding_matrix = np.zeros((len(word_index) + 1, embedding_vecor_length))\n",
    "for word, i in word_index.items():\n",
    "    embedding_vector = word2vec_dict.get(word)\n",
    "    if embedding_vector is not None:\n",
    "        # words not found in embedding index will be all-zeros.\n",
    "        embedding_matrix[i] = embedding_vector\n",
    "        \n",
    "print('\\nembedding matrix shape:',embedding_matrix.shape)\n",
    "print(embedding_matrix[0]) # first cell should be all zeros\n",
    "print(embedding_matrix[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Best model result holder\n",
    "best_model_aux = {}\n",
    "best_model_aux['Max length'] = max_description_length\n",
    "best_model_aux['Best score'] = 0\n",
    "best_model_aux['texts_to_sequences'] = texts_to_sequences_custom\n",
    "best_model_aux['word_index'] = word_index\n",
    "best_model_aux['Label encoder'] = le\n",
    "\n",
    "\n",
    "best_model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Optional\n",
    "# Load previous model (if needs to be compared in the following training)\n",
    "#best_model = load_model('category_927_nets_1000_model.h5')\n",
    "#best_model_aux = get_model_file_aux('category_927_nets_1000_model_aux.pkl')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# prediction nodes count\n",
    "nb_classes = train_df['category_full_path_mod1'].unique()\n",
    "print('Classes count:', len(nb_classes))\n",
    "\n",
    "# keep the length of original (e.g. unboosted) train set\n",
    "orig_dim = train_texts_vec_mtx.shape[0]\n",
    "\n",
    "train_df = None\n",
    "samples_df = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "model = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Model 0.1 (search for significant words using softmax)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAIN MODEL 1\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "model = Sequential()\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ---- Embedding layer -----------------------------------------------------------------\n",
    "print('Embeddings source:',embeddings_source)\n",
    "embedding_layer = Embedding(top_words\n",
    "                            ,1  # embedding vecor length\n",
    "                            #,weights=[embedding_matrix]\n",
    "                            ,input_length = max_description_length\n",
    "                            ,init='glorot_uniform'\n",
    "                            ,trainable=True)\n",
    "model.add(embedding_layer)\n",
    "model.add(Flatten())\n",
    "\n",
    "## OUTPUT: classes layer\n",
    "## ======================================================================================\n",
    "model.add(Dense(len(nb_classes), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try loss=categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(10):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx, y_ary_cat, nb_epoch=1, batch_size=batch_size_)\n",
    "    \n",
    "    scores = model.evaluate(train_texts_vec_mtx[0:orig_dim], y_ary_cat[0:orig_dim], verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        print('Model not improved')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "    #    break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Encoder model (gives assigned embeddings for words from above model)\n",
    "model_enc = Sequential()\n",
    "model_enc.add(embedding_layer)\n",
    "model_enc.add(Flatten())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "j = 33                                      # row number of description from train set\n",
    "t = train_texts_vec_mtx[j]                  # get description from train set (word indexes)\n",
    "print(t.reshape(1,30))                      # print it\n",
    "print(model_enc.predict(t.reshape(1,30)))   # encode the description to get embedding vectors\n",
    "\n",
    "\n",
    "p = model.predict_classes(t.reshape(1,30), verbose = False)  # get the class number of the description\n",
    "print(le.inverse_transform(p[0]))                            # convert the class number into class name\n",
    "\n",
    "# convert word indexes from description into words\n",
    "for i in t:\n",
    "    if(i != 0):\n",
    "        print(list(word_index.keys())[list(word_index.values()).index(i)], end=' ')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 28, 28)\n",
      "(10000, 28, 28)\n"
     ]
    }
   ],
   "source": [
    "from keras.datasets import mnist\n",
    "import numpy as np\n",
    "(x_train, _), (x_test, _) = mnist.load_data()\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(60000, 784)\n",
      "(10000, 784)\n"
     ]
    }
   ],
   "source": [
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))\n",
    "print(x_train.shape)\n",
    "print(x_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental 0.4 (encoder for clustering)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embeddings source: random\n"
     ]
    }
   ],
   "source": [
    "# CREATE INPUT MATRIX FOR AUTOENCODER\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "model = Sequential()\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ---- Embedding layer -----------------------------------------------------------------\n",
    "print('Embeddings source:',embeddings_source)\n",
    "embedding_layer = Embedding(top_words\n",
    "                            ,embedding_vecor_length\n",
    "                            ,weights=[embedding_matrix]\n",
    "                            ,input_length = max_description_length\n",
    "                            #,init='glorot_uniform'\n",
    "                            ,trainable=False)\n",
    "model.add(embedding_layer)\n",
    "model.compile('rmsprop', 'mse')\n",
    "input_array = model.predict(train_texts_vec_mtx)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "____________________________________________________________________________________________________\n",
      "Layer (type)                     Output Shape          Param #     Connected to                     \n",
      "====================================================================================================\n",
      "input_19 (InputLayer)            (None, 30, 32)        0                                            \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_8 (TimeDistribut (None, 30, 32)        1056        input_19[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "lambda_6 (Lambda)                (None, 32)            0           timedistributed_8[0][0]          \n",
      "____________________________________________________________________________________________________\n",
      "repeatvector_14 (RepeatVector)   (None, 30, 32)        0           lambda_6[0][0]                   \n",
      "____________________________________________________________________________________________________\n",
      "timedistributed_9 (TimeDistribut (None, 30, 32)        1056        repeatvector_14[0][0]            \n",
      "====================================================================================================\n",
      "Total params: 2,112\n",
      "Trainable params: 2,112\n",
      "Non-trainable params: 0\n",
      "____________________________________________________________________________________________________\n",
      "None\n",
      "Epoch iter #1\n",
      "Epoch 1/1\n",
      "484432/484432 [==============================] - 29s - loss: 0.0030 - acc: 0.1392    \n",
      "Model not improved\n",
      "Valid accuracy:  0.696096831432\n",
      "Training took 48.1585 s\n"
     ]
    }
   ],
   "source": [
    "# TRAIN AUTOENCODER\n",
    "from keras.models import Model\n",
    "\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "inputs = Input(shape=(max_description_length, embedding_vecor_length))\n",
    "latent_dim = 10\n",
    "encoded = LSTM(latent_dim)(inputs)\n",
    "#td_1 = TimeDistributed(Dense(32))(inputs)\n",
    "#encoded = Lambda(max_, output_shape=(embedding_vecor_length,))(td_1)\n",
    "\n",
    "decoded = RepeatVector(max_description_length)(encoded)\n",
    "decoded = LSTM(embedding_vecor_length, return_sequences=True)(decoded)\n",
    "\n",
    "#model_encoder = Model(inputs, encoded)\n",
    "model_autoencoder = Model(inputs, decoded)\n",
    "\n",
    "\n",
    "## OUTPUT: classes layer\n",
    "## ======================================================================================\n",
    "#model.add(Dense(len(nb_classes), activation='softmax'))\n",
    "model_autoencoder.compile(loss='binary_crossentropy', optimizer='adam', metrics=['accuracy'])  # try loss=categorical_crossentropy\n",
    "print(model_autoencoder.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(10):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model_autoencoder.fit(input_array, input_array, nb_epoch=1, batch_size=batch_size_)\n",
    "    \n",
    "    scores = model_autoencoder.evaluate(input_array, input_array, verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model_autoencoder\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        print('Model not improved')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Cluster encodings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "encodings_mtx = best_model.predict(input_array)\n",
    "encodings_mtx_resh = encodings_mtx.reshape(484432, 30*32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training took 3861.63 s\n"
     ]
    }
   ],
   "source": [
    "from sklearn.cluster import KMeans\n",
    "k_means = KMeans(n_clusters=850, random_state=0, max_iter=10, n_jobs=-1)\n",
    "\n",
    "start = time.time()\n",
    "k_means.fit(encodings_mtx_resh)\n",
    "#np.array([[[1,1,1],[2,2,2]],[[1,1,1],[2,2,2]]]).reshape(2,6)\n",
    "print(\"Training took %g s\" % (time.time() - start))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[[ 0.00678394  0.00799087 -0.02210714 -0.01669397  0.00652681  0.01796212\n",
      "   0.02027131 -0.02707245  0.02296207 -0.06715409  0.00207059 -0.03023864\n",
      "  -0.02352969 -0.08128262 -0.01729795 -0.09831153 -0.04109302 -0.00228851\n",
      "   0.03313054 -0.04783145 -0.08433102  0.00091587 -0.03782459  0.020688\n",
      "  -0.04172391 -0.04488586  0.0157574  -0.01861063 -0.02058901  0.01759877\n",
      "  -0.01189658  0.01106455]\n",
      " [ 0.01125409  0.00853695 -0.02553343 -0.01259037  0.00618082  0.02462783\n",
      "   0.02399347 -0.04864526  0.0336272  -0.10120874  0.00530424 -0.03515679\n",
      "  -0.02759841 -0.12664749 -0.02308232 -0.17102937 -0.09735309 -0.00255409\n",
      "   0.04413624 -0.05858684 -0.14689499 -0.00384735 -0.04937905  0.02309752\n",
      "  -0.05952082 -0.05680497  0.02181251 -0.02119935 -0.06000166  0.01550376\n",
      "  -0.01037072  0.01127833]\n",
      " [ 0.01302237  0.00808915 -0.02376049 -0.00806668  0.00530856  0.02442732\n",
      "   0.02235194 -0.05994024  0.03609109 -0.11077517  0.00583272 -0.03300028\n",
      "  -0.02487346 -0.14628698 -0.02412327 -0.2203858  -0.1491804  -0.00299945\n",
      "   0.04386203 -0.05615157 -0.18707721 -0.00586815 -0.04885551  0.02196032\n",
      "  -0.06192514 -0.05674175  0.02239873 -0.0194417  -0.10187898  0.01163907\n",
      "  -0.00541937  0.00926937]\n",
      " [ 0.01389303  0.00804151 -0.02116176 -0.00697831  0.00477602  0.02229866\n",
      "   0.02017028 -0.06378851  0.03593458 -0.10760185  0.00533624 -0.03010993\n",
      "  -0.02123527 -0.1527378  -0.02379194 -0.25257334 -0.18794742 -0.00363823\n",
      "   0.04034266 -0.05052957 -0.2099895  -0.00582402 -0.04415162  0.02084849\n",
      "  -0.05808842 -0.05423652  0.02152758 -0.01733806 -0.13789906  0.00915277\n",
      "  -0.00078139  0.00752762]\n",
      " [ 0.01460799  0.00829091 -0.01888315 -0.00837668  0.00449173  0.02025652\n",
      "   0.01864545 -0.06367228  0.03550604 -0.09980627  0.00472348 -0.02791831\n",
      "  -0.01839899 -0.15350647 -0.02329427 -0.2737098  -0.21406725 -0.00428519\n",
      "   0.03691669 -0.04516171 -0.22169578 -0.00499946 -0.03892655  0.02021721\n",
      "  -0.0528593  -0.05222807  0.02071209 -0.01572234 -0.16598755  0.00795776\n",
      "   0.00268978  0.00640435]\n",
      " [ 0.01531605  0.00860236 -0.01713289 -0.01089121  0.00432127  0.01877425\n",
      "   0.0177846  -0.06190772  0.03539293 -0.09141411  0.00424427 -0.02642929\n",
      "  -0.01649451 -0.15216793 -0.02293904 -0.28789732 -0.23078594 -0.00484699\n",
      "   0.03444301 -0.04089345 -0.22680515 -0.0040456  -0.03445908  0.01993211\n",
      "  -0.04806088 -0.05113713  0.02024773 -0.01457944 -0.18683125  0.00753622\n",
      "   0.0050797   0.00573635]\n",
      " [ 0.01599671  0.00885833 -0.01587861 -0.01362109  0.00419473  0.01781943\n",
      "   0.01737709 -0.05973306  0.03559573 -0.08402132  0.00392189 -0.02541923\n",
      "  -0.01528553 -0.15027387 -0.02275371 -0.29761064 -0.24117753 -0.00529808\n",
      "   0.03289578 -0.03772568 -0.22831094 -0.00320094 -0.03101175  0.01984232\n",
      "  -0.04419399 -0.05074498  0.02007336 -0.01377181 -0.20186189  0.00750827\n",
      "   0.00665891  0.00534339]\n",
      " [ 0.01661021  0.00902937 -0.01502308 -0.01610794  0.0040863   0.01725072\n",
      "   0.01723672 -0.05771133  0.0359851  -0.07806921  0.00372358 -0.0247115\n",
      "  -0.01453488 -0.14844947 -0.0226953  -0.30435115 -0.24748623 -0.00564409\n",
      "   0.03203283 -0.035448   -0.22802322 -0.00252472 -0.02849231  0.01985108\n",
      "  -0.04127703 -0.05077258  0.02007453 -0.01318992 -0.21249086  0.00765466\n",
      "   0.00767554  0.00510853]\n",
      " [ 0.01713264  0.00912539 -0.01446173 -0.01817483  0.00399032  0.01693583\n",
      "   0.01723672 -0.05604256  0.03644199 -0.07351387  0.00361118 -0.02419771\n",
      "  -0.01407246 -0.14690816 -0.02271441 -0.30906427 -0.25122011 -0.00590153\n",
      "   0.03161583 -0.03383791 -0.2269796  -0.00200983 -0.02671487  0.01990271\n",
      "  -0.03917094 -0.05101     0.02016136 -0.01276255 -0.21989059  0.0078597\n",
      "   0.00831667  0.00496465]\n",
      " [ 0.01755767  0.00916764 -0.01410524 -0.01979686  0.0039077   0.01677688\n",
      "   0.01729988 -0.054753    0.03688803 -0.07014044  0.00355381 -0.02381522\n",
      "  -0.01378811 -0.14568779 -0.02277261 -0.31236914 -0.25336066 -0.00608863\n",
      "   0.03146458 -0.03271221 -0.22573769 -0.00162924 -0.02549391  0.01996692\n",
      "  -0.03770206 -0.05132435  0.02027757 -0.01244503 -0.22496986  0.00806641\n",
      "   0.00871333  0.00487418]\n",
      " [ 0.01789091  0.00917641 -0.01388549 -0.02101967  0.00383986  0.01670841\n",
      "   0.01738308 -0.05379758  0.03728192 -0.06770119  0.00352945 -0.0235271\n",
      "  -0.01361325 -0.1447579  -0.02284452 -0.3146849  -0.25453633 -0.00622191\n",
      "   0.03145771 -0.03193212 -0.2245688  -0.00135357 -0.02467384  0.0200285\n",
      "  -0.03670869 -0.05164096  0.02039278 -0.01220858 -0.22840878  0.00824959\n",
      "   0.00895389  0.00481598]\n",
      " [ 0.01814427  0.00916709 -0.01375399 -0.02191384  0.00378686  0.01668916\n",
      "   0.01746445 -0.05311038  0.03760808 -0.0659705   0.00352349 -0.02330989\n",
      "  -0.01350576 -0.14406748 -0.02291549 -0.31630242 -0.25514317 -0.00631507\n",
      "   0.0315195  -0.03139601 -0.2235824  -0.00115705 -0.02413413  0.02008093\n",
      "  -0.03605663 -0.0519233   0.0204932  -0.01203341 -0.23070504  0.00840076\n",
      "   0.00909653  0.00477777]\n",
      " [ 0.01833192  0.00914988 -0.01367781 -0.02255175  0.00374736  0.01669436\n",
      "   0.01753456 -0.05262712  0.03786616 -0.06476218  0.00352681 -0.02314727\n",
      "  -0.01343985 -0.14356479 -0.02297825 -0.31742662 -0.25542632 -0.00637899\n",
      "   0.03160588 -0.03103071 -0.2228023  -0.00101889 -0.02378584  0.02012245\n",
      "  -0.03564166 -0.05215776  0.0205745  -0.01190504 -0.23221658  0.0085198\n",
      "   0.00917883  0.00475227]\n",
      " [ 0.01846772  0.00913084 -0.01363534 -0.02299735  0.00371915  0.01670981\n",
      "   0.01759073 -0.0522935   0.03806341 -0.06393074  0.003534   -0.02302689\n",
      "  -0.01339962 -0.1432046  -0.02303016 -0.31820306 -0.25553408 -0.00642207\n",
      "   0.03169332 -0.03078403 -0.22221209 -0.00092302 -0.0235655   0.0201537\n",
      "  -0.03538654 -0.05234326  0.02063715 -0.01181229 -0.23319688  0.00861037\n",
      "   0.00922466  0.00473499]\n",
      " [ 0.018564    0.00911315 -0.01361281 -0.02330287  0.0036998   0.01672789\n",
      "   0.01763358 -0.05206679  0.03821007 -0.06336636  0.00354209 -0.022939\n",
      "  -0.01337524 -0.14295013 -0.02307117 -0.31873566 -0.25555325 -0.00645053\n",
      "   0.03177074 -0.030619   -0.22178003 -0.00085736 -0.02342898  0.02017628\n",
      "  -0.035236   -0.05248484  0.02068374 -0.01174635 -0.23382282  0.00867744\n",
      "   0.00924897  0.00472313]\n",
      " [ 0.01863096  0.00909816 -0.01360167 -0.02350877  0.00368703  0.01674485\n",
      "   0.01766511 -0.05191497  0.03831661 -0.06298825  0.00354961 -0.02287578\n",
      "  -0.01336063 -0.14277266 -0.02310248 -0.3190982  -0.25553286 -0.00646899\n",
      "   0.03183426 -0.03050967 -0.22147186 -0.00081295 -0.02334631  0.02019206\n",
      "  -0.03515182 -0.05258983  0.0207174  -0.01170026 -0.23421583  0.008726\n",
      "   0.00926091  0.00471488]\n",
      " [ 0.01867671  0.00908619 -0.01359678 -0.02364529  0.00367891  0.01675916\n",
      "   0.01768767 -0.05181469  0.03839247 -0.06273817  0.00355596 -0.02283098\n",
      "  -0.01335197 -0.14265038 -0.02312575 -0.31934312 -0.25549909 -0.00648069\n",
      "   0.03188378 -0.03043797 -0.22125687 -0.00078332 -0.02329756  0.02020276\n",
      "  -0.03510829 -0.05266584  0.02074116 -0.01166861 -0.23445812  0.00876049\n",
      "   0.00926602  0.00470908]\n",
      " [ 0.01870743  0.00907703 -0.01359512 -0.02373438  0.00367396  0.01677047\n",
      "   0.01770341 -0.05174934  0.03844551 -0.06257489  0.003561   -0.0227997\n",
      "  -0.01334692 -0.1425671  -0.02314267 -0.31950721 -0.25546467 -0.00648795\n",
      "   0.03192095 -0.03039144 -0.22110964 -0.00076382 -0.0232697   0.02020981\n",
      "  -0.03508865 -0.05271971  0.02075759 -0.01164725 -0.23460449  0.00878455\n",
      "   0.00926754  0.00470496]\n",
      " [ 0.01872773  0.00907025 -0.01359499 -0.02379162  0.00367107  0.01677901\n",
      "   0.01771417 -0.05170732  0.03848198 -0.06246966  0.00356483 -0.0227782\n",
      "  -0.01334402 -0.14251104 -0.02315474 -0.31961623 -0.2554352  -0.00649232\n",
      "   0.03194803 -0.03036159 -0.22101066 -0.00075117 -0.02325443  0.02021433\n",
      "  -0.03508233 -0.05275718  0.02076874 -0.0116331  -0.2346909   0.00880109\n",
      "   0.00926735  0.00470201]\n",
      " [ 0.01874091  0.00906535 -0.01359548 -0.0238278   0.00366948  0.01678521\n",
      "   0.0177214  -0.05168067  0.03850667 -0.06240276  0.00356766 -0.02276362\n",
      "  -0.01334239 -0.14247368 -0.0231632  -0.31968814 -0.25541207 -0.00649487\n",
      "   0.03196726 -0.03034263 -0.22094509 -0.00074308 -0.02324652  0.02021714\n",
      "  -0.03508283 -0.05278277  0.02077616 -0.0116239  -0.2347405   0.00881227\n",
      "   0.00926645  0.00469989]\n",
      " [ 0.01874933  0.0090619  -0.01359613 -0.02385029  0.00366867  0.0167896\n",
      "   0.01772615 -0.05166401  0.03852313 -0.06236082  0.00356968 -0.02275388\n",
      "  -0.0133415  -0.1424491  -0.02316905 -0.31973511 -0.25539508 -0.0064963\n",
      "   0.03198062 -0.03033075 -0.2209024  -0.000738   -0.02324278  0.02021882\n",
      "  -0.0350864  -0.05279996  0.02078102 -0.01161802 -0.23476802  0.00881974\n",
      "   0.00926537  0.00469836]\n",
      " [ 0.01875461  0.00905951 -0.01359673 -0.02386403  0.00366832  0.01679262\n",
      "   0.01772923 -0.05165375  0.03853394 -0.06233491  0.0035711  -0.02274746\n",
      "  -0.01334101 -0.14243305 -0.02317304 -0.31976563 -0.25538322 -0.00649704\n",
      "   0.0319897  -0.03032339 -0.22087504 -0.00073486 -0.02324129  0.02021979\n",
      "  -0.03509091 -0.05281132  0.02078415 -0.01161436 -0.23478258  0.00882465\n",
      "   0.00926436  0.00469726]\n",
      " [ 0.01875785  0.00905789 -0.0135972  -0.02387225  0.00366821  0.01679463\n",
      "   0.01773119 -0.05164756  0.03854093 -0.06231916  0.00357207 -0.0227433\n",
      "  -0.01334076 -0.14242272 -0.02317573 -0.31978527 -0.2553753  -0.0064974\n",
      "   0.03199576 -0.03031889 -0.2208578  -0.00073295 -0.02324094  0.02022032\n",
      "  -0.0350953  -0.05281869  0.02078612 -0.01161212 -0.23478979  0.00882784\n",
      "   0.00926352  0.00469646]\n",
      " [ 0.0187598   0.00905681 -0.01359755 -0.02387706  0.00366822  0.01679596\n",
      "   0.01773241 -0.05164387  0.03854538 -0.06230975  0.00357272 -0.02274063\n",
      "  -0.01334062 -0.14241612 -0.02317751 -0.31979778 -0.25537023 -0.00649754\n",
      "   0.03199973 -0.03031617 -0.2208471  -0.00073182 -0.02324111  0.02022059\n",
      "  -0.03509911 -0.05282339  0.02078734 -0.01161079 -0.23479298  0.00882988\n",
      "   0.00926286  0.00469589]\n",
      " [ 0.01876095  0.0090561  -0.01359778 -0.0238798   0.00366828  0.0167968\n",
      "   0.01773315 -0.05164173  0.03854816 -0.06230424  0.00357316 -0.02273896\n",
      "  -0.01334056 -0.14241196 -0.02317868 -0.31980574 -0.2553671  -0.00649756\n",
      "   0.03200227 -0.03031455 -0.22084059 -0.00073117 -0.02324147  0.0202207\n",
      "  -0.03510219 -0.05282634  0.02078808 -0.01161003 -0.2347941   0.00883117\n",
      "   0.00926237  0.00469547]\n",
      " [ 0.01876161  0.00905565 -0.01359792 -0.02388132  0.00366836  0.01679732\n",
      "   0.0177336  -0.05164051  0.03854988 -0.06230108  0.00357344 -0.02273792\n",
      "  -0.01334052 -0.14240938 -0.02317945 -0.31981075 -0.25536534 -0.00649753\n",
      "   0.03200388 -0.03031361 -0.2208367  -0.00073081 -0.02324185  0.02022073\n",
      "  -0.03510455 -0.05282815  0.02078851 -0.0116096  -0.23479423  0.00883197\n",
      "   0.00926202  0.00469518]\n",
      " [ 0.01876196  0.00905536 -0.013598   -0.02388212  0.00366843  0.01679764\n",
      "   0.01773386 -0.05163983  0.03855092 -0.06229931  0.00357361 -0.02273729\n",
      "  -0.0133405  -0.14240777 -0.02317994 -0.31981385 -0.25536445 -0.00649749\n",
      "   0.03200486 -0.03031307 -0.22083446 -0.00073062 -0.02324218  0.02022073\n",
      "  -0.03510632 -0.05282925  0.02078876 -0.01160938 -0.23479393  0.00883246\n",
      "   0.00926178  0.00469497]\n",
      " [ 0.01876215  0.00905517 -0.01359805 -0.02388253  0.00366849  0.01679782\n",
      "   0.017734   -0.05163946  0.03855152 -0.06229835  0.00357372 -0.02273692\n",
      "  -0.01334048 -0.14240681 -0.02318024 -0.31981575 -0.25536403 -0.00649744\n",
      "   0.03200545 -0.03031276 -0.2208332  -0.00073052 -0.02324245  0.0202207\n",
      "  -0.03510759 -0.05282988  0.0207889  -0.01160928 -0.23479353  0.00883275\n",
      "   0.00926161  0.00469482]\n",
      " [ 0.01876224  0.00905506 -0.01359806 -0.02388271  0.00366853  0.01679792\n",
      "   0.01773408 -0.05163928  0.03855187 -0.06229786  0.00357379 -0.0227367\n",
      "  -0.01334047 -0.1424062  -0.02318044 -0.31981698 -0.25536388 -0.00649739\n",
      "   0.03200579 -0.03031258 -0.22083251 -0.00073048 -0.02324264  0.02022068\n",
      "  -0.03510848 -0.05283024  0.02078897 -0.01160924 -0.23479316  0.00883293\n",
      "   0.00926151  0.00469471]\n",
      " [ 0.01876228  0.009055   -0.01359806 -0.02388279  0.00366857  0.01679797\n",
      "   0.01773412 -0.05163919  0.03855206 -0.0622976   0.00357383 -0.02273658\n",
      "  -0.01334046 -0.14240585 -0.02318056 -0.31981769 -0.25536391 -0.00649736\n",
      "   0.03200599 -0.03031249 -0.22083217 -0.00073046 -0.02324278  0.02022065\n",
      "  -0.0351091  -0.05283043  0.020789   -0.01160923 -0.23479287  0.00883303\n",
      "   0.00926143  0.00469464]]\n"
     ]
    }
   ],
   "source": [
    "print(encodings_mtx[0])\n",
    "#print(input_array[0][-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental Model 0.2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def max_(x):\n",
    "    return K.max(x, axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# TRAIN MODEL 1\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "model = Sequential()\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ---- Embedding layer -----------------------------------------------------------------\n",
    "print('Embeddings source:',embeddings_source)\n",
    "embedding_layer = Embedding(top_words\n",
    "                            ,embedding_vecor_length\n",
    "                            #,weights=[embedding_matrix]\n",
    "                            ,input_length = max_description_length\n",
    "                            ,init='glorot_uniform'\n",
    "                            ,trainable=True)\n",
    "model.add(embedding_layer)\n",
    "\n",
    "## Time Distributed\n",
    "## ======================================================================================\n",
    "TimeDist_1 = TimeDistributed(Dense(32))  #, input_shape=(max_description_length, embedding_vecor_length)\n",
    "model.add(TimeDist_1)\n",
    "model.add(Lambda(max_, output_shape=(embedding_vecor_length,)))\n",
    "model.add(Flatten())\n",
    "\n",
    "## OUTPUT: classes layer\n",
    "## ======================================================================================\n",
    "model.add(Dense(len(nb_classes), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try loss=categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(10):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx, y_ary_cat, nb_epoch=1, batch_size=batch_size_)\n",
    "    \n",
    "    scores = model.evaluate(train_texts_vec_mtx[0:orig_dim], y_ary_cat[0:orig_dim], verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        print('Model not improved')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "    #    break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Experimental model 0.3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# TRAIN MODEL 1\n",
    "start = time.time()\n",
    "\n",
    "# define/initialize model\n",
    "top_words = len(word_index) + 1\n",
    "batch_size_ = 64   # 64\n",
    "\n",
    "model = Sequential()\n",
    "# --------------------------------------------------------------------------------------\n",
    "# ---- Embedding layer -----------------------------------------------------------------\n",
    "print('Embeddings source:',embeddings_source)\n",
    "embedding_layer = Embedding(top_words\n",
    "                            ,1 #embedding_vecor_length\n",
    "                            #,weights=[embedding_matrix]\n",
    "                            ,input_length = max_description_length\n",
    "                            ,init='glorot_uniform'\n",
    "                            ,trainable=True)\n",
    "model.add(embedding_layer)\n",
    "#model.add(BatchNormalization()) #axis=1\n",
    "\n",
    "## LSTM 1\n",
    "## ======================================================================================\n",
    "LSTM_1 = LSTM(32, return_sequences=True, activation='linear')\n",
    "#model.add(LSTM_1)\n",
    "#LSTM_1 = SimpleRNN(128,return_sequences=False)\n",
    "#model.add(LSTM(8,return_sequences=False, activation='linear'))\n",
    "#model.add(LSTM(64,return_sequences=False, activation='linear'))\n",
    "#LSTM_1 = LSTM(128,return_sequences=False, dropout_W = 0.3, dropout_U = 0.3)\n",
    "\n",
    "\n",
    "## Gaussian Noise (optional)\n",
    "## ======================================================================================\n",
    "percent_noise = 0.1\n",
    "noise = 1.0 * percent_noise\n",
    "noise_layer = GaussianNoise(noise)\n",
    "#model.add(noise_layer)\n",
    "\n",
    "# Conv 1\n",
    "nb_filter_1 = 64\n",
    "conv1D_1 = Conv1D(nb_filter=nb_filter_1   # feature maps\n",
    "                  ,filter_length=3        # kernel size\n",
    "                  ,subsample_length=1     # strides\n",
    "                  ,border_mode='valid'    # padding: same, valid\n",
    "                  ,activation='linear'\n",
    "                 )\n",
    "model.add(conv1D_1)\n",
    "model.add(MaxPooling1D(pool_length = 3, stride = 1))\n",
    "model.add(LSTM(64,return_sequences=False, activation='linear'))\n",
    "\n",
    "## OUTPUT: classes layer\n",
    "## ======================================================================================\n",
    "model.add(Dense(len(nb_classes), activation='softmax'))\n",
    "model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])  # try loss=categorical_crossentropy\n",
    "print(model.summary())\n",
    "\n",
    "# start training multiple times with epoch=1\n",
    "for ep in range(10):\n",
    "    print('Epoch iter #' + str(ep+1))\n",
    "    model.fit(train_texts_vec_mtx, y_ary_cat, nb_epoch=1, batch_size=batch_size_)\n",
    "    \n",
    "    scores = model.evaluate(train_texts_vec_mtx[0:orig_dim], y_ary_cat[0:orig_dim], verbose=0)\n",
    "    if(best_model_aux['Best score'] < scores[1]):\n",
    "        best_model_aux['Best score'] = scores[1]\n",
    "        best_model = model\n",
    "        print('Captured improved model')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "        #print(\"Accuracy on test set: %.2f%%\" % (scores[1]*100))\n",
    "    else:\n",
    "        print('Model not improved')\n",
    "        print('Valid accuracy: ',best_model_aux['Best score'])\n",
    "    #    break\n",
    "    print()\n",
    " \n",
    "\n",
    "print(\"Training took %g s\" % (time.time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# FUNCTION: Attention Layer\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.engine.topology import Layer\n",
    "from keras import initializations\n",
    "\n",
    "class AttLayer(Layer):\n",
    "    def __init__(self, **kwargs):\n",
    "        self.init = initializations.get('normal')\n",
    "        #self.input_spec = [InputSpec(ndim=3)]\n",
    "        super(AttLayer, self).__init__(** kwargs)\n",
    "\n",
    "    def build(self, input_shape):\n",
    "        assert len(input_shape)==3\n",
    "        #self.W = self.init((input_shape[-1],1))\n",
    "        self.W = self.init((input_shape[-1],))\n",
    "        #self.input_spec = [InputSpec(shape=input_shape)]\n",
    "        self.trainable_weights = [self.W]\n",
    "        super(AttLayer, self).build(input_shape)  # be sure you call this somewhere!\n",
    "\n",
    "    def call(self, x, mask=None):\n",
    "        eij = K.tanh(K.dot(x, self.W))\n",
    "\n",
    "        ai = K.exp(eij)\n",
    "        weights = ai/K.sum(ai, axis=1).dimshuffle(0,'x')\n",
    "\n",
    "        weighted_input = x*weights.dimshuffle(0,1,'x')\n",
    "        return weighted_input.sum(axis=1)\n",
    "\n",
    "    def get_output_shape_for(self, input_shape):\n",
    "        return (input_shape[0], input_shape[-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# Plot Nets design\n",
    "#from keras.utils import plot_model\n",
    "import matplotlib.pyplot as plt\n",
    "from keras.utils.visualize_util import model_to_dot\n",
    "\n",
    "plot(model, to_file='/Users/altay.amanbay/Desktop/model.png')\n",
    "SVG(model_to_dot(model).create(prog='dot', format='svg'))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
