{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Importing process_string.py \n",
      "from /Users/altay.amanbay/Desktop/new node booster/experiments/4/2_common_aux_script ...\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/sklearn/cross_validation.py:44: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. Also note that the interface of the new CV iterators are different from that of this module. This module will be removed in 0.20.\n",
      "  \"This module will be removed in 0.20.\", DeprecationWarning)\n",
      "/Users/altay.amanbay/anaconda/lib/python3.5/site-packages/sklearn/grid_search.py:43: DeprecationWarning: This module was deprecated in version 0.18 in favor of the model_selection module into which all the refactored classes and functions are moved. This module will be removed in 0.20.\n",
      "  DeprecationWarning)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas\n",
    "import sys\n",
    "import os.path\n",
    "import getpass\n",
    "from optparse import OptionParser\n",
    "import pickle\n",
    "import time\n",
    "from datetime import datetime\n",
    "from sqlalchemy import create_engine\n",
    "from configparser import SafeConfigParser\n",
    "from IPython.display import display, HTML\n",
    "# Feature creator libraries\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "import scipy.sparse\n",
    "# ML classifier libraries\n",
    "from sklearn import tree\n",
    "from sklearn.ensemble import RandomForestClassifier, AdaBoostClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn import svm\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "# Aux libraries for ML classifiers\n",
    "from scipy import stats\n",
    "from sklearn.cross_validation import cross_val_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.grid_search import GridSearchCV\n",
    "from sklearn.grid_search import RandomizedSearchCV\n",
    "from sklearn.multiclass import OneVsRestClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "\n",
    "# import custom code\n",
    "import os\n",
    "import sys\n",
    "pardir = os.path.abspath(os.path.join(os.getcwd(), ''))\n",
    "script_path = pardir + \"/2_common_aux_script\"\n",
    "print('Importing process_string.py \\nfrom ' + script_path + \" ...\\n\")\n",
    "sys.path.append(script_path)\n",
    "from process_string import process_string\n",
    "sys.path.remove(script_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "## Read arguments\n",
    "def get_args():\n",
    "    help_text = \"\"\"CME Node Recall Booster\"\"\"\n",
    "\n",
    "    parser = OptionParser(usage=help_text)\n",
    "    parser.add_option(\"-H\", \"--host\",           dest=\"host\",            default = \"rops.dwh.prod.slicetest.com\", help=\"the url for the DB\", metavar=\"HOST_ADDRESS\")\n",
    "    parser.add_option(\"-d\", \"--db\",             dest=\"database\",        default = \"sliceds\",             help=\"the logical db name\")\n",
    "    parser.add_option(\"-u\", \"--username\",       dest=\"username\",        default = \"infoprod_ops_admin\",  help=\"the username for the DB\", metavar=\"NAME\")\n",
    "    parser.add_option(\"-p\", \"--password\",       dest=\"password\",        help=\"the password for the DB\", metavar=\"PASS\")\n",
    "    parser.add_option(\"-i\", \"--input_table\",    dest=\"input_table\")\n",
    "    parser.add_option(\"-f\", \"--file\",           dest=\"file\")\n",
    "    parser.add_option(\"--pkl\", \"--pickle\",      dest=\"pickle\",          help=\"the pickle file if data is cached\")\n",
    "    parser.add_option(\"--cf\", \"--config_file\",  dest=\"config_file\",     help=\"config_file that has all settings\")\n",
    "\n",
    "    (options, args) = parser.parse_args()\n",
    "\n",
    "    return (options, args)\n",
    "\n",
    "def check_if_file_exists(file_path):\n",
    "    try:\n",
    "        with open(file_path) as infile:\n",
    "            pass\n",
    "    except IOError:\n",
    "        print('  ERROR: '+file_path+' file not found\\n')\n",
    "        sys.exit()\n",
    "\n",
    "def get_db_engine_2(config_set_, username_, password_, host_, database_, port_):\n",
    "    # for RedShift\n",
    "    if(config_set_ == 'redshift'):\n",
    "        url = ''.join(['postgresql://', username_, \":\", password_, \"@\", host_, ':',port_, '/', database_])\n",
    "\n",
    "    # for MsSQL\n",
    "    elif(config_set_ == 'mssql'):\n",
    "        url = ''.join(['mssql+pymssql://', username_, \":\", password_, \"@\", host_, ':',port_, '/', database_])\n",
    "\n",
    "    print(url)\n",
    "    engine = create_engine(url)\n",
    "    return engine\n",
    "\n",
    "def get_db_engine(options):\n",
    "    url = \"\".join([\"postgresql://\", options.username, \":\", options.password, \"@\", options.host, \":5439/\", options.database])\n",
    "    engine = create_engine(url)\n",
    "    return engine\n",
    "\n",
    "def get_data_df_2(options_, args):\n",
    "    parser = SafeConfigParser()\n",
    "\n",
    "    if(options_.config_file):\n",
    "        parser.read(options_.config_file)\n",
    "        \n",
    "        # Get train data source args\n",
    "        if('train_data' in parser.options('training')):\n",
    "            train_args_str = parser.get('training', 'train_data')\n",
    "            config_set, config_element = train_args_str.split('+')\n",
    "        elif('pickle' in parser.options('training')):\n",
    "            config_set = parser.get('training', 'pickle')\n",
    "\n",
    "            pkl_file = parser.get('training', 'pickle')\n",
    "            check_if_file_exists(pkl_file)\n",
    "\n",
    "            print('Reading train data from pickle ' + pkl_file + ' ...')\n",
    "            df = pandas.read_pickle(pkl_file)\n",
    "\n",
    "            print(\"Found \" + str(len(df)) + \" entries\")\n",
    "            return df\n",
    "        else:\n",
    "            print('One of the following options should be in training arg section:')\n",
    "            print('train_data and pickle')\n",
    "            print('or just pickle')\n",
    "            sys.exit()\n",
    "\n",
    "\n",
    "        if(config_set == 'config_file'):\n",
    "            train_file_name = parser.get(config_set, config_element)\n",
    "            check_if_file_exists(train_file_name)\n",
    "\n",
    "            print('Reading train data from file ' + train_file_name + ' ...')\n",
    "            df = pandas.read_csv(train_file_name)\n",
    "            print(train_file_name)\n",
    "            print(df.shape)\n",
    "\n",
    "            #if parser.get('training', 'pickle'):\n",
    "            if('pickle' in parser.options('training')):\n",
    "                print(parser.get('training', 'pickle'))\n",
    "                df.to_pickle(parser.get('training', 'pickle'))\n",
    "\n",
    "        #elif(config_set == 'config_redshift'):\n",
    "        elif(parser.get(config_set, 'db_type') == 'redshift'):\n",
    "            # Get database engine\n",
    "            db_type  = parser.get(config_set, 'db_type')\n",
    "            username = parser.get(config_set, 'username')\n",
    "            password = parser.get(config_set, 'password')\n",
    "            host     = parser.get(config_set, 'host')\n",
    "            database = parser.get(config_set, 'database')\n",
    "            port     = parser.get(config_set, 'port')\n",
    "            engine   = get_db_engine_2(db_type, username, password, host, database, port)\n",
    "            print(\"\\nEstablished connection with the database\")\n",
    "\n",
    "            # Fetch data from db\n",
    "            start = time.time()\n",
    "            #print(\"\\nReading input data from database ...\")\n",
    "            train_data_table_name = parser.get(config_set, config_element)\n",
    "            print(\"\\nReading train data from RedShift table \" + train_data_table_name + \"...\")\n",
    "            df = pandas.read_sql_query('SELECT * FROM ' + train_data_table_name, engine)\n",
    "\n",
    "            ## Get elapsed time\n",
    "            end = time.time()\n",
    "            print(\"Reading data from database took %g s\" % (end - start))\n",
    "\n",
    "            #if parser.get('training', 'pickle'):\n",
    "            if('pickle' in parser.options('training')):\n",
    "                print(parser.get('training', 'pickle'))\n",
    "                df.to_pickle(parser.get('training', 'pickle'))\n",
    "\n",
    "        #elif(config_set == 'config_mssql'):\n",
    "        elif(parser.get(config_set, 'db_type') == 'mssql'):\n",
    "            # Get database engine\n",
    "            db_type  = parser.get(config_set, 'db_type')\n",
    "            username = parser.get(config_set, 'username')\n",
    "            password = parser.get(config_set, 'password')\n",
    "            host     = parser.get(config_set, 'host')\n",
    "            database = parser.get(config_set, 'database')\n",
    "            port     = parser.get(config_set, 'port')\n",
    "            engine   = get_db_engine_2(db_type, username, password, host, database, port)\n",
    "            print(\"\\nEstablished connection with the database\")\n",
    "\n",
    "            # Fetch data from db\n",
    "            start = time.time()\n",
    "            #print(\"\\nReading input data from database ...\")\n",
    "            train_data_table_name = parser.get(config_set, config_element)\n",
    "            print(\"\\nReading train data from MsSQL table \" + train_data_table_name + \"...\")\n",
    "            df = pandas.read_sql_query('SELECT * FROM ' + train_data_table_name, engine)\n",
    "\n",
    "            ## Get elapsed time\n",
    "            end = time.time()\n",
    "            print(\"Reading data from database took %g s\" % (end - start))\n",
    "\n",
    "            #if parser.get('training', 'pickle'):\n",
    "            if('pickle' in parser.options('training')):\n",
    "                print(parser.get('training', 'pickle'))\n",
    "                df.to_pickle(parser.get('training', 'pickle'))\n",
    "\n",
    "        elif parser.get('training', 'pickle'):\n",
    "            pkl_file = parser.get('training', 'pickle')\n",
    "            check_if_file_exists(pkl_file)\n",
    "            df = pandas.read_pickle(pkl_file)\n",
    "\n",
    "        else:\n",
    "            print(\"Need to specify either input db table or pickle file or input file\")\n",
    "            sys.exit()\n",
    "\n",
    "        print(\"Found \" + str(len(df)) + \" entries\")\n",
    "        return df\n",
    "    else:\n",
    "        print(' ERROR: --cf option is not passed')\n",
    "        sys.exit()\n",
    "\n",
    "\n",
    "def get_positives_negatives(df, category):\n",
    "    positives = df[(df.category_full_path_mod1 == category) & (df.type == 'True Positive')].loc[:,'description_mod1']\n",
    "    negatives = df[((df.category_full_path_mod1 != category) & (df.type == 'True Positive')) | \\\n",
    "                   ((df.category_full_path_mod1 == category) & (df.type == 'False Positive'))\n",
    "                  ].loc[:,'description_mod1']\n",
    "    return (positives.drop_duplicates(), negatives.drop_duplicates())\n",
    "\n",
    "def get_vectorized_data(positives, negatives, count_vect=None, tfidf_vect=None):\n",
    "    if count_vect and tfidf_vect:\n",
    "        X_train_counts = count_vect.fit_transform(pandas.concat([positives, negatives]))\n",
    "        X_train_tfidf  = tfidf_vect.fit_transform(pandas.concat([positives, negatives]))\n",
    "        X_train = scipy.sparse.hstack([X_train_counts, X_train_tfidf])\n",
    "        print('      Both of vectorizers',X_train.shape)\n",
    "    elif count_vect and tfidf_vect is None:\n",
    "        X_train = count_vect.fit_transform(pandas.concat([positives, negatives]))\n",
    "        print('      Countvect only',X_train.shape)\n",
    "    elif count_vect is None and tfidf_vect:\n",
    "        X_train  = tfidf_vect.fit_transform(pandas.concat([positives, negatives]))\n",
    "        print('      TfIdf only',X_train.shape)\n",
    "    else:\n",
    "        print('      None of Vectorizers')\n",
    "\n",
    "    Y = [1] * len(positives) + [0] * len(negatives)\n",
    "    return (X_train, Y), count_vect\n",
    "\n",
    "def get_vectorized_data_2(desc_items_df, target_df, count_vect=None, tfidf_vect=None):\n",
    "    if count_vect and tfidf_vect:\n",
    "        X_train_counts = count_vect.fit_transform(desc_items_df)\n",
    "        X_train_tfidf  = tfidf_vect.fit_transform(desc_items_df)\n",
    "        X_train = scipy.sparse.hstack([X_train_counts, X_train_tfidf])\n",
    "        print('      Both of vectorizers',X_train.shape)\n",
    "    elif count_vect and tfidf_vect is None:\n",
    "        X_train = count_vect.fit_transform(desc_items_df)\n",
    "        print('      Countvect only',X_train.shape)\n",
    "    elif count_vect is None and tfidf_vect:\n",
    "        X_train  = tfidf_vect.fit_transform(desc_items_df)\n",
    "        print('      TfIdf only',X_train.shape)\n",
    "    else:\n",
    "        print('      None of Vectorizers')\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    le.fit(target_df)\n",
    "    Y = le.transform(target_df)\n",
    "\n",
    "    #Y = [1] * len(positives) + [0] * len(negatives)\n",
    "    return (X_train, Y), count_vect, le\n",
    "\n",
    "\n",
    "#def get_trained_clf_2(df, category, X_train_counts, Y, count_vect, clf, clf_name):\n",
    "def get_trained_clf_2(df, X_train_counts, Y, count_vect, clf, clf_name):\n",
    "    params = None\n",
    "    gs_clf = None\n",
    "\n",
    "    # set clf into grid search\n",
    "    if(isinstance(clf, tree.DecisionTreeClassifier)):\n",
    "        print('='*100)\n",
    "        print('      Optimizing tree.DecisionTreeClassifier ...')\n",
    "        params = {'criterion':['gini','entropy'],\n",
    "                  'max_depth':[4,5,6,7,8,9,10,11,12,15,20,30,40,50,70,90,120,150]}\n",
    "        gs_clf = RandomizedSearchCV(estimator = clf, param_distributions = params, cv=5, n_jobs = -1)\n",
    "    elif(isinstance(clf, LogisticRegression)):\n",
    "        print('='*100)\n",
    "        print('      Optimizing Logistic Reg ...')\n",
    "        params = {'C': [0.001, 0.01, 0.1, 1, 10, 15, 20, 30, 40, 100, 1000],\n",
    "                  'penalty': ['l1','l2']}\n",
    "        gs_clf = RandomizedSearchCV(estimator = clf, param_distributions = params, cv=5, n_jobs = -1)\n",
    "    elif(isinstance(clf, RandomForestClassifier)):\n",
    "        print('='*100)\n",
    "        print('      Optimizing Random Forest ...')\n",
    "        params = {\"max_depth\": [3, 5, None],\n",
    "                  \"max_features\": [1, 2, 3, 4, 5, 7, 9],\n",
    "                  #\"min_samples_split\": [1.0, 2.0, 3.0, 4.0, 5.0, 7.0, 9.0],\n",
    "                  \"min_samples_split\": [0.1, 0.2, 0.3, 0.4, 0.5, 0.7, 0.9],\n",
    "                  \"min_samples_leaf\": [1, 2, 3, 4, 5, 7, 9],\n",
    "                  \"bootstrap\": [True, False],\n",
    "                  \"criterion\": [\"gini\", \"entropy\"]}\n",
    "        gs_clf = RandomizedSearchCV(estimator = clf, param_distributions = params, cv=5, n_jobs = -1)\n",
    "    elif(isinstance(clf, svm.SVC)):\n",
    "        print('='*100)\n",
    "        print('      Optimizing SVM ...')\n",
    "        C_range = 10.0 ** np.arange(-4, 4)\n",
    "        gamma_range = 10.0 ** np.arange(-4, 4)\n",
    "        kernels = ['rbf','linear','poly','sigmoid']\n",
    "        params = {'C': C_range.tolist(), \n",
    "                  'gamma': gamma_range.tolist(), \n",
    "                  'kernel': kernels}\n",
    "        gs_clf = RandomizedSearchCV(estimator = clf, param_distributions = params, cv=5, n_jobs = -1)\n",
    "    elif(isinstance(clf, MultinomialNB)):\n",
    "        print('='*100)\n",
    "        print('      Optimizing MultinomialNB ...')\n",
    "        params = {'alpha': [1, 0.1, 0.01, 0.001, 0.0001, 0.00001], \n",
    "                  'fit_prior': [True, False]}\n",
    "        gs_clf = RandomizedSearchCV(estimator = clf, param_distributions = params, cv=5, n_jobs = -1, n_iter = 10)\n",
    "    elif(isinstance(clf, AdaBoostClassifier)):\n",
    "        print('='*100)\n",
    "        print('      Optimizing Ada Boost ...')\n",
    "        params = {'learning_rate': stats.expon(scale=1.0), \n",
    "                  'n_estimators': stats.randint(low=20, high=100)}\n",
    "        gs_clf = RandomizedSearchCV(estimator = clf, param_distributions = params, cv=5, n_jobs = -1, n_iter = 10)\n",
    "    elif(isinstance(clf, KNeighborsClassifier)):\n",
    "        print('='*100)\n",
    "        print('      Optimizing KNN Neighbors ...')\n",
    "        params = {'n_neighbors': [i for i in range(2, 10)], \n",
    "                  'weights': ['uniform', 'distance']}\n",
    "        gs_clf = RandomizedSearchCV(estimator = clf, param_distributions = params, cv=5, n_jobs = -1, n_iter = 10)\n",
    "\n",
    "\n",
    "    start = time.time()\n",
    "\n",
    "    ## train classifier for recall and precision measurements\n",
    "    gs_clf.fit(X_train_counts, Y)\n",
    "\n",
    "    print(\"      Optimization process took %g s\" % (time.time()- start))\n",
    "    \n",
    "    ## get validation score for given classifier\n",
    "    print('      Cross validation for ' + clf_name)\n",
    "    #scores = cross_val_score(gs_clf, X_train_counts, Y, scoring='recall', cv=5, average='weighted')\n",
    "    #scores = precision_score(gs_clf, X_train_counts, Y, average='weighted')\n",
    "    \n",
    "\n",
    "    ## Print accuracy\n",
    "    predictions = gs_clf.predict(X_train_counts)\n",
    "    scores = accuracy_score(Y, predictions)\n",
    "    print('\\n      Best score: ', np.mean(scores))\n",
    "    print('      Prediction accuracy score: ', accuracy_score(Y, predictions))\n",
    "    print('      Confusion matrix:')\n",
    "    #display(pandas.crosstab(pandas.Series(Y), predictions, rownames=['True'], colnames=['Predicted'], margins=True))\n",
    "    print('\\n')\n",
    "\n",
    "    ## recall measurement\n",
    "    #false_negatives = df[((df.category_full_path_mod1 == category) & (df.type == 'False Negative'))].loc[:,'description_mod1']\n",
    "    #false_negatives = false_negatives.drop_duplicates()\n",
    "    #X_test_counts = count_vect.transform(false_negatives)\n",
    "    #Y_test = clf.predict(X_test_counts)\n",
    "\n",
    "    ## precision measurement\n",
    "    #false_positives = df[((df.category_full_path_mod1 != category) & (df.type == 'False Negative'))].loc[:,'description_mod1']\n",
    "    #false_positives = false_positives.drop_duplicates()\n",
    "    #X_test_counts2 = count_vect.transform(false_positives)\n",
    "    #Y_test2 = clf.predict(X_test_counts2)\n",
    "\n",
    "    ## Persist classifier and it's scores to dict\n",
    "    results_dict = {}\n",
    "    results_dict[\"Model name\"] = clf_name\n",
    "    #results_dict[\"Cross Validation Score\"] = np.mean(scores)\n",
    "    #results_dict[\"Best Score\"] = gs_clf.best_score_\n",
    "    results_dict[\"Best Score\"] = np.mean(scores)\n",
    "    #results_dict[\"Recall\"] = np.sum(Y_test)*1.0/len(Y_test)\n",
    "    #results_dict[\"Precision\"] = 1 - np.sum(Y_test2)*1.0/len(Y_test2)\n",
    "    results_dict[\"Model\"] = gs_clf\n",
    "\n",
    "    for param_name in sorted(params.keys()):\n",
    "        results_dict[param_name] = gs_clf.best_params_[param_name]\n",
    "\n",
    "    return results_dict\n",
    "\n",
    "def get_best_local_model(clf_dict, df, X_train_counts, Y, count_vect):\n",
    "    import copy\n",
    "    \n",
    "    models_list = []\n",
    "    \n",
    "    ## create models and add to list\n",
    "    for clf_name, clf in clf_dict.items():\n",
    "        #model_trained = get_trained_clf(df, category, X_train_counts, Y, count_vect, copy.deepcopy(clf), clf_name)\n",
    "        model_trained = get_trained_clf_2(df, X_train_counts, Y, count_vect, copy.deepcopy(clf), clf_name)\n",
    "        models_list.append(model_trained)\n",
    "    \n",
    "    ## Get model with highest validation score\n",
    "    best_model_score_tuple = max(models_list, key = lambda model_score:model_score['Best Score'])\n",
    "    \n",
    "    return best_model_score_tuple\n",
    "\n",
    "def compare_and_pick_best_model(best_local_model, best_global_model):    \n",
    "    if not best_local_model:\n",
    "        return best_global_model\n",
    "    elif not best_global_model:\n",
    "        return best_local_model\n",
    "    elif(best_global_model['Best Score'] < best_local_model['Best Score']):\n",
    "        return best_local_model\n",
    "    else:\n",
    "        return best_global_model\n",
    "\n",
    "def get_best_model_2(desc_items_df, target_df, clf_dict, ng_ranges = [(1,1), (1,2), (2,2)], verbous = False):\n",
    "    # Result holder for final classifier\n",
    "    best_final_clf = {}\n",
    "    \n",
    "    # Get (positives, negatives) for training set\n",
    "    #(positives, negatives) = get_positives_negatives(data_df, category[0])\n",
    "    \n",
    "    # Start running classifiers for different feature counts\n",
    "    #for num_features_val in range(50,550,50):  # [50,100 ... 450,500]\n",
    "    #    print('Num of features: ', num_features_val)\n",
    "    \n",
    "    for ng_range in ng_ranges:\n",
    "        print('='*100)\n",
    "        print('Trial for n-gram range:',ng_range)\n",
    "\n",
    "        # Create training set\n",
    "        print('   Vectorizing training data ...')\n",
    "        count_vect = CountVectorizer(min_df=1, ngram_range=ng_range, binary = True, stop_words=\"english\")\n",
    "        tfidf_vect = TfidfVectorizer(sublinear_tf=True, max_df=1.0, ngram_range=ng_range, stop_words='english')\n",
    "        #(X_train_counts, Y), count_vect_fitted = get_vectorized_data(positives, negatives, count_vect=count_vect, tfidf_vect=None)\n",
    "        (X_train_counts, Y), count_vect_fitted, le = get_vectorized_data_2(desc_items_df, target_df, count_vect=count_vect, tfidf_vect=None)\n",
    "\n",
    "        # Get best classifier for current iteration (i.e. features) as local\n",
    "        print('   Testing classifiers ...')\n",
    "        #best_local_clf = get_best_local_model(clf_dict, data_df, category[0], X_train_counts, Y, count_vect)\n",
    "        data_df = Y\n",
    "        best_local_clf = get_best_local_model(clf_dict, data_df, X_train_counts, Y, count_vect)\n",
    "        \n",
    "        # Add additional meta-data\n",
    "        #best_local_clf['Num of features']  = num_features_val\n",
    "        best_local_clf['Count vectorizer'] = count_vect_fitted\n",
    "        best_local_clf['Label encoder'] = le\n",
    "        #best_local_clf['Category name'] = category[0]\n",
    "        #best_local_clf['Category ID']   = category[1]\n",
    "        best_local_clf['ngram range']  = ng_range\n",
    "        \n",
    "        \n",
    "        if(verbous == True):\n",
    "            print('Selected model for range ', ng_range)\n",
    "            print_(best_local_clf, indent ='      ', print_all = True)            \n",
    "\n",
    "        # Compare local classifier with previous, and get the best as final\n",
    "        best_final_clf = compare_and_pick_best_model(best_final_clf, best_local_clf)\n",
    "        \n",
    "    \n",
    "    return best_final_clf\n",
    "  \n",
    "def export_model_file(best_final_clf_, model_file_name_ = 'new_multiclass_model'):\n",
    "    print('Exporting model files ...')\n",
    "    #for category in selected_models_by_category_.keys():\n",
    "        # Create pickle file name for classifier\n",
    "    #model_file_name = 'category_' + str(best_final_clf_['Category ID']) + '_model.pkl'\n",
    "    model_file_name = model_file_name_+'.pkl'\n",
    "    \n",
    "    # Save model as pickle file\n",
    "    with open(model_file_name, 'wb') as pickle_file:\n",
    "        pickle.dump(best_final_clf_, pickle_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "    print('Exported model file: ', model_file_name)\n",
    "\n",
    "def export_model_files(selected_models_by_category_):\n",
    "    print('Exporting model files ...')\n",
    "    for category in selected_models_by_category_.keys():\n",
    "        # Create pickle file name for classifier\n",
    "        model_file_name = 'category_' + str(selected_models_by_category_[category]['Category ID']) + '_model.pkl'\n",
    "        \n",
    "        # Save model as pickle file\n",
    "        with open(model_file_name, 'wb') as pickle_file:\n",
    "            pickle.dump(selected_models_by_category_[category], pickle_file, protocol=pickle.HIGHEST_PROTOCOL)\n",
    "\n",
    "        print('Exported model file: ', model_file_name)\n",
    "\n",
    "def print_2(best_local_model, print_all = False):\n",
    "    for k,v in best_local_model.items():\n",
    "        if(not isinstance(v,CountVectorizer) and not isinstance(v,RandomizedSearchCV)):\n",
    "            print(k,v)\n",
    "\n",
    "def print_(best_local_model, indent = '', print_all = False):\n",
    "    print(indent + 'Model name:       ', best_local_model['Model name'])\n",
    "    #print(indent + 'Validation Score: ', best_local_model['Cross Validation Score'])\n",
    "    print(indent + 'Best Score: ', best_local_model['Best Score'])\n",
    "    #print(indent + 'Num of features:  ', best_local_model['Num of features'])\n",
    "    print(indent + 'ngram range:  ', best_local_model['ngram range'])\n",
    "    \n",
    "    #if(print_all == True):\n",
    "        #print(indent + 'Precision:        ', best_local_model['Precision'])\n",
    "        #print(indent + 'Recall:           ', best_local_model['Recall'])\n",
    "        #print(indent + 'Category name:    ', best_local_model['Category name'])\n",
    "        #print(indent + 'Category ID:      ', best_local_model['Category ID'])\n",
    "    print()\n",
    "    \n",
    "def print_selected_models(selected_models_by_category, print_all = False):\n",
    "    for category_tuple, model in selected_models_by_category.items():\n",
    "        print(category_tuple)\n",
    "        print_2(model, print_all = print_all)\n",
    "        print('\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "print('Start date and time:\\n',datetime.now().strftime('%Y-%m-%d %H:%M'),'\\n')                # '%Y-%m-%d %H:%M:%S'\n",
    "#logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "\n",
    "\n",
    "\n",
    "start = time.time()\n",
    "## Get input arguments\n",
    "(options, args) = get_args()\n",
    "\n",
    "## Fetch input data to be classified\n",
    "# train_data_df = get_data_df_2(options, args)\n",
    "import pandas as pd\n",
    "train_data_df = pd.read_csv('1_data/sampled_descriptionary_sample_size_5000.csv')\n",
    "\n",
    "\n",
    "# Preprocess\n",
    "# Rename columns\n",
    "train_data_df.rename(columns={'description': 'description_mod1', \n",
    "                            'category_id': 'category_id_mod1',\n",
    "                            'category_path': 'category_full_path_mod1'}, inplace=True)\n",
    "\n",
    "train_data_df['description_mod1'] = train_data_df['description_mod1'].apply(lambda x: process_string(x))\n",
    "train_data_df.drop_duplicates(subset=['description_mod1','category_full_path_mod1'], inplace = True, keep='first')\n",
    "train_data_df.drop_duplicates(subset=['description_mod1'], inplace = True, keep=False)\n",
    "print('Deduplicated data shape:',train_data_df.shape,'\\n')\n",
    "\n",
    "\n",
    "from collections import Counter\n",
    "\n",
    "arr = train_data_df.loc[:,'category_full_path_mod1']\n",
    "d = dict(Counter(arr))\n",
    "for k,v in d.items():\n",
    "    if(v<5):\n",
    "        #print(k,v)\n",
    "        train_data_df.drop(train_data_df[train_data_df.category_full_path_mod1==k].index, inplace=True)\n",
    "\n",
    "#sys.exit()\n",
    "#print(train_data_df.shape)\n",
    "#print(train_data_df.columns)\n",
    "\n",
    "# Result holder for all categories\n",
    "selected_models_by_category = {}\n",
    "\n",
    "# Get unique list of categories with ids\n",
    "#categories = pandas.unique(train_data_df.loc[:,['category_full_path_mod1','category_id_mod1']].values)\n",
    "\n",
    "# Indicate classifiers to be tested\n",
    "clf_dict = {}\n",
    "#clf_dict['Decision Tree']       = tree.DecisionTreeClassifier()\n",
    "#clf_dict['Random Forest']       = RandomForestClassifier()\n",
    "#clf_dict['Logistic Regression'] = LogisticRegression()\n",
    "clf_dict['SVM']                 = svm.SVC(probability=True)\n",
    "clf_dict['MultinomialNB']       = MultinomialNB()\n",
    "#clf_dict['AdaBoost']            = AdaBoostClassifier()\n",
    "clf_dict['KNeighbors']          = KNeighborsClassifier()\n",
    "\n",
    "# Start iterating categories through classifiers\n",
    "#for category in categories:    \n",
    "print('='*100,'\\nRun models for following category:\\n', '='*100)\n",
    "\n",
    "# Get best model with meta-info\n",
    "X = np.array(list(train_data_df.loc[:,'description_mod1']))\n",
    "y = np.array(list(train_data_df.loc[:,'category_full_path_mod1']))\n",
    "best_final_clf = get_best_model_2(X, y, clf_dict, ng_ranges = [(1,1),(1,2),(2,2)], verbous = True)\n",
    "\n",
    "# Print best model scores for current category\n",
    "print('SELECTED MODEL FOR :')\n",
    "print_2(best_final_clf, print_all = True)\n",
    "print('\\n',best_final_clf,'\\n')\n",
    "\n",
    "# Persist final classifier for current category to dict\n",
    "#selected_models_by_category[category] = best_final_clf\n",
    "\n",
    "# Pickling classifier and CountVectorizers as one file with all meta data\n",
    "#export_model_file(best_final_clf)\n",
    "\n",
    "\n",
    "# Print final classifier scores for all categories\n",
    "print('='*100,'\\nFINAL RESULTS:')\n",
    "#print_selected_models(selected_models_by_category, print_all = True)\n",
    "\n",
    "# Pickling classifiers and CountVectorizers as one file with all meta data\n",
    "#export_model_file(best_final_clf, model_file_name_ = 'model_trained_on_dict_5000')\n",
    "\n",
    "print(\"Total process time: %g s\" % (time.time()- start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>description_mod1</th>\n",
       "      <th>category_id_mod1</th>\n",
       "      <th>category_full_path_mod1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1000</th>\n",
       "      <td>8964</td>\n",
       "      <td>the nightmare before christmas jack face contr...</td>\n",
       "      <td>137</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Sweaters, Sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1001</th>\n",
       "      <td>9296</td>\n",
       "      <td>under armour girls armour fleece novelty jumbo...</td>\n",
       "      <td>137</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Sweaters, Sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1002</th>\n",
       "      <td>9628</td>\n",
       "      <td>yoki  black cableknit toggle duster  plus size...</td>\n",
       "      <td>137</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Sweaters, Sw...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1003</th>\n",
       "      <td>0</td>\n",
       "      <td>as is denim  co quilted vest with faux sher pa...</td>\n",
       "      <td>1370</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Vests &gt; Women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1004</th>\n",
       "      <td>491</td>\n",
       "      <td>boys quilted frost free vest black m</td>\n",
       "      <td>1370</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Vests &gt; Women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1005</th>\n",
       "      <td>982</td>\n",
       "      <td>diamondquilted vest s aviator navy</td>\n",
       "      <td>1370</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Vests &gt; Women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1006</th>\n",
       "      <td>1473</td>\n",
       "      <td>factory printed quilted puffer vest  xlarge  s...</td>\n",
       "      <td>1370</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Vests &gt; Women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1007</th>\n",
       "      <td>1964</td>\n",
       "      <td>faux fur vest 0379170805223 78 years 504 inches</td>\n",
       "      <td>1370</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Vests &gt; Women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1008</th>\n",
       "      <td>2455</td>\n",
       "      <td>frostfree quilted vest for saturn l</td>\n",
       "      <td>1370</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Vests &gt; Women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1009</th>\n",
       "      <td>2946</td>\n",
       "      <td>longline faux suede vest chestnut</td>\n",
       "      <td>1370</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Vests &gt; Women</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1010</th>\n",
       "      <td>3437</td>\n",
       "      <td>port authority f219 value fleece vest  dark ch...</td>\n",
       "      <td>1370</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Vests &gt; Women</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      index                                   description_mod1  \\\n",
       "1000   8964  the nightmare before christmas jack face contr...   \n",
       "1001   9296  under armour girls armour fleece novelty jumbo...   \n",
       "1002   9628  yoki  black cableknit toggle duster  plus size...   \n",
       "1003      0  as is denim  co quilted vest with faux sher pa...   \n",
       "1004    491               boys quilted frost free vest black m   \n",
       "1005    982                 diamondquilted vest s aviator navy   \n",
       "1006   1473  factory printed quilted puffer vest  xlarge  s...   \n",
       "1007   1964    faux fur vest 0379170805223 78 years 504 inches   \n",
       "1008   2455                frostfree quilted vest for saturn l   \n",
       "1009   2946                  longline faux suede vest chestnut   \n",
       "1010   3437  port authority f219 value fleece vest  dark ch...   \n",
       "\n",
       "      category_id_mod1                            category_full_path_mod1  \n",
       "1000               137  Apparel & Accessories > Apparel > Sweaters, Sw...  \n",
       "1001               137  Apparel & Accessories > Apparel > Sweaters, Sw...  \n",
       "1002               137  Apparel & Accessories > Apparel > Sweaters, Sw...  \n",
       "1003              1370    Apparel & Accessories > Apparel > Vests > Women  \n",
       "1004              1370    Apparel & Accessories > Apparel > Vests > Women  \n",
       "1005              1370    Apparel & Accessories > Apparel > Vests > Women  \n",
       "1006              1370    Apparel & Accessories > Apparel > Vests > Women  \n",
       "1007              1370    Apparel & Accessories > Apparel > Vests > Women  \n",
       "1008              1370    Apparel & Accessories > Apparel > Vests > Women  \n",
       "1009              1370    Apparel & Accessories > Apparel > Vests > Women  \n",
       "1010              1370    Apparel & Accessories > Apparel > Vests > Women  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Just random train data part\n",
    "train_data_df.loc[1000:1010,:]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "the nightmare before christmas jack face contrast zip hoodie\n",
      "['Apparel & Accessories > Apparel > Sweaters, Sweatshirts, Hoodies > Girls']\n"
     ]
    }
   ],
   "source": [
    "# Manual random prediction check\n",
    "count_vect = best_final_clf['Count vectorizer']\n",
    "clf = best_final_clf['Model']\n",
    "le = best_final_clf['Label encoder']\n",
    "\n",
    "i = 1000\n",
    "description_str = train_data_df.loc[i,'description_mod1']\n",
    "row_vectorized = count_vect.transform([str(description_str)])\n",
    "clf_prediction = clf.predict(row_vectorized)\n",
    "\n",
    "print(train_data_df.loc[i,'description_mod1'])\n",
    "print(le.inverse_transform(clf_prediction))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Prediction\n",
    "def predict(description_str_, clf_):\n",
    "    row_vectorized_ = count_vect.transform([str(description_str_)])\n",
    "    clf_prediction_ = clf_.predict(row_vectorized_)\n",
    "    #le_.inverse_transform(clf_prediction_)\n",
    "    return clf_prediction_[0]\n",
    "\n",
    "train_data_df['Prediction_le'] = train_data_df['description_mod1'].apply(lambda x: predict(x, clf))\n",
    "train_data_df['Prediction'] = train_data_df['Prediction_le'].apply(lambda x: le.inverse_transform(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>index</th>\n",
       "      <th>description_mod1</th>\n",
       "      <th>category_id_mod1</th>\n",
       "      <th>category_full_path_mod1</th>\n",
       "      <th>Prediction</th>\n",
       "      <th>Prediction_le</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>it jeans maternity skinny jeans dark wash m</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>117</td>\n",
       "      <td>citizens of humanity avedon skinny maternity a...</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>234</td>\n",
       "      <td>dl1961 maternity angel jeans  riker30</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>351</td>\n",
       "      <td>james jeans  twiggy maternity legging in dark ...</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>468</td>\n",
       "      <td>james jeans twiggy maternity under belly pull ...</td>\n",
       "      <td>100</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>Apparel &amp; Accessories &gt; Apparel &gt; Maternity</td>\n",
       "      <td>25</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   index                                   description_mod1  category_id_mod1  \\\n",
       "0      0        it jeans maternity skinny jeans dark wash m               100   \n",
       "1    117  citizens of humanity avedon skinny maternity a...               100   \n",
       "2    234              dl1961 maternity angel jeans  riker30               100   \n",
       "3    351  james jeans  twiggy maternity legging in dark ...               100   \n",
       "4    468  james jeans twiggy maternity under belly pull ...               100   \n",
       "\n",
       "                       category_full_path_mod1  \\\n",
       "0  Apparel & Accessories > Apparel > Maternity   \n",
       "1  Apparel & Accessories > Apparel > Maternity   \n",
       "2  Apparel & Accessories > Apparel > Maternity   \n",
       "3  Apparel & Accessories > Apparel > Maternity   \n",
       "4  Apparel & Accessories > Apparel > Maternity   \n",
       "\n",
       "                                    Prediction  Prediction_le  \n",
       "0  Apparel & Accessories > Apparel > Maternity             25  \n",
       "1  Apparel & Accessories > Apparel > Maternity             25  \n",
       "2  Apparel & Accessories > Apparel > Maternity             25  \n",
       "3  Apparel & Accessories > Apparel > Maternity             25  \n",
       "4  Apparel & Accessories > Apparel > Maternity             25  "
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Export predictions and model\n",
    "train_data_df.to_csv('Predictions_descriptionary_5000.csv')\n",
    "export_model_file(best_final_clf, model_file_name_ = 'model_trained_on_dict_5000')"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [conda root]",
   "language": "python",
   "name": "conda-root-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
